{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain matplotlib scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('/workspaces/ml-learning/.env', override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def pickleSave(object, name, folder=\".\", silent=False):\n",
    "    filename = folder + \"/\" + name + \".pkl\"\n",
    "    if silent == False:\n",
    "        print(\"Saving object {} to pickle file {}\".format(name, filename))\n",
    "    with open(filename, mode=\"wb\") as fipkl:\n",
    "        pickle.dump(object, fipkl)\n",
    "\n",
    "\n",
    "def pickleLoad(name, folder: str, silent=False):\n",
    "    filename = folder + \"/\" + name + \".pkl\"\n",
    "    if silent == False:\n",
    "        print(\"Loading object {} from pickle file {}\".format(name, filename))\n",
    "\n",
    "    try:\n",
    "        with open(filename, mode=\"rb\") as fipkl:\n",
    "            myObject = pickle.load(fipkl)\n",
    "        return myObject\n",
    "    except IOError:\n",
    "        print(\"Pickle file {} not found, returning None object\".format(filename))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading documents\n",
    "\n",
    "### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "from llama_index.node_parser import TokenTextSplitter\n",
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "# documents = loader.load_data(pages=['Louvain Method', 'Modularity (networks)', 'Community structure'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### directory loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ml-learning/src/phages/data/test2/1-s2.0-S0092867422008509-main.pdf\n",
      "/workspaces/ml-learning/src/phages/data/test2/1-s2.0-S2213716519302516-main.pdf\n",
      "/workspaces/ml-learning/src/phages/data/test2/ciac453.pdf\n",
      "/workspaces/ml-learning/src/phages/data/test2/nihms-1861394.pdf\n"
     ]
    }
   ],
   "source": [
    "# With llamaindex\n",
    "\n",
    "import openai\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "from llama_index import (\n",
    "  SimpleDirectoryReader,\n",
    "  ListIndex,\n",
    "  ServiceContext,\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=1024)\n",
    "\n",
    "documents = SimpleDirectoryReader('/workspaces/ml-learning/src/phages/data/test2').load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/run-llama/llama-hub/tree/main/llama_hub/semanticscholar\n",
    "\n",
    "https://github.com/run-llama/llama-hub/tree/main/llama_hub/smart_pdf_loader\n",
    "\n",
    "https://github.com/run-llama/llama-hub/tree/afc8b8e0bdeb07c88adc31e94e26e666901b0677/llama_hub/papers/pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=10388293&db=pmc\n",
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=10236208&db=pmc\n",
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=10458410&db=pmc\n",
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=10186849&db=pmc\n",
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=9816530&db=pmc\n",
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=9697571&db=pmc\n",
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=9673753&db=pmc\n",
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=9645218&db=pmc\n",
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=9495427&db=pmc\n",
      "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id=8788396&db=pmc\n"
     ]
    }
   ],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "PubmedReader = download_loader(\"PubmedReader\")\n",
    "\n",
    "loader = PubmedReader()\n",
    "documents = loader.load_data(search_query='Phi X 174 Phage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving object test2_documents to pickle file ./test2_documents.pkl\n"
     ]
    }
   ],
   "source": [
    "pickleSave(documents, 'test2_documents', folder=\".\", silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading object test2_documents from pickle file ./test2_documents.pkl\n"
     ]
    }
   ],
   "source": [
    "documents = pickleLoad('test2_documents', folder=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2879', 'file_name': '1-s2.0-S0092867422008509-main.pdf', 'file_path': '/workspaces/ml-learning/src/phages/data/test2/1-s2.0-S0092867422008509-main.pdf', 'file_type': 'application/pdf', 'file_size': 13522904, 'creation_date': '2024-01-16', 'last_modified_date': '2024-01-16', 'last_accessed_date': '2024-01-16'}\n",
      "{'page_label': '2880', 'file_name': '1-s2.0-S0092867422008509-main.pdf', 'file_path': '/workspaces/ml-learning/src/phages/data/test2/1-s2.0-S0092867422008509-main.pdf', 'file_type': 'application/pdf', 'file_size': 13522904, 'creation_date': '2024-01-16', 'last_modified_date': '2024-01-16', 'last_accessed_date': '2024-01-16'}\n",
      "{'page_label': '2881', 'file_name': '1-s2.0-S0092867422008509-main.pdf', 'file_path': '/workspaces/ml-learning/src/phages/data/test2/1-s2.0-S0092867422008509-main.pdf', 'file_type': 'application/pdf', 'file_size': 13522904, 'creation_date': '2024-01-16', 'last_modified_date': '2024-01-16', 'last_accessed_date': '2024-01-16'}\n",
      "{'page_label': '2882', 'file_name': '1-s2.0-S0092867422008509-main.pdf', 'file_path': '/workspaces/ml-learning/src/phages/data/test2/1-s2.0-S0092867422008509-main.pdf', 'file_type': 'application/pdf', 'file_size': 13522904, 'creation_date': '2024-01-16', 'last_modified_date': '2024-01-16', 'last_accessed_date': '2024-01-16'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].metadata)\n",
    "print(documents[1].metadata)\n",
    "print(documents[2].metadata)\n",
    "print(documents[3].metadata)\n",
    "# print(documents[4].metadata)\n",
    "# print(documents[5].metadata)\n",
    "# print(documents[6].metadata)\n",
    "# print(documents[7].metadata)\n",
    "# print(documents[8].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INGESTION PIPELINE WITH TRANSFORMATIONS: Getting the nodes and extracting metadata\n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/indexing/metadata_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/24 [00:03<01:22,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/24 [00:04<00:49,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 3/24 [00:05<00:26,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 4/24 [00:07<00:34,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 5/24 [00:12<00:55,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 6/24 [00:12<00:37,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 7/24 [00:14<00:30,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 8/24 [00:18<00:39,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 9/24 [00:21<00:39,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 10/24 [00:22<00:32,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 11/24 [00:22<00:21,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 12/24 [00:24<00:20,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 13/24 [00:27<00:24,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 14/24 [00:32<00:29,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 15/24 [00:33<00:20,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 16/24 [00:33<00:13,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 17/24 [00:36<00:13,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 18/24 [00:40<00:15,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 19/24 [00:41<00:11,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 20/24 [00:43<00:08,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 21/24 [00:44<00:05,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 22/24 [00:46<00:03,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 23/24 [00:51<00:02,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:51<00:00,  2.16s/it]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/24 [00:02<00:56,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 3/24 [00:02<00:16,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 4/24 [00:04<00:18,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 5/24 [00:04<00:16,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 6/24 [00:05<00:12,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 7/24 [00:05<00:09,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 8/24 [00:05<00:08,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 9/24 [00:08<00:16,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 10/24 [00:08<00:11,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 11/24 [00:08<00:07,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 12/24 [00:09<00:09,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 13/24 [00:10<00:08,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 14/24 [00:13<00:13,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 15/24 [00:13<00:09,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 16/24 [00:15<00:11,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 17/24 [00:15<00:07,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 18/24 [00:16<00:05,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 19/24 [00:17<00:05,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 20/24 [00:19<00:04,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 21/24 [00:19<00:02,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 22/24 [00:21<00:02,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 23/24 [00:22<00:01,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:26<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "from llama_index.schema import Document\n",
    "from llama_index.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    EntityExtractor,\n",
    ")\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "import tqdm as notebook_tqdm\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "transformations = [\n",
    "    SentenceSplitter(chunk_size=256, chunk_overlap=64), #SentenceSplitter(chunk_size=512, chunk_overlap=128),\n",
    "    # TitleExtractor(nodes=5),\n",
    "    # QuestionsAnsweredExtractor(questions=3),\n",
    "    SummaryExtractor(summaries=[\"self\"]),\n",
    "    KeywordExtractor(keywords=10),\n",
    "    # EntityExtractor(prediction_threshold=0.5),\n",
    "]\n",
    "\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=transformations)\n",
    "\n",
    "nodes = pipeline.run(documents=[documents[0], documents[1], documents[2]])\n",
    "\n",
    "# test_nodes = pipeline.run(documents=[Document(text=\"\"\"The story tells of a prince who wants to marry a princess but is having difficulty finding a suitable wife. He meets many princesses, but is never sure that they are real (Danish: rigtig, lit. 'rightful') princesses. One stormy night, a young woman drenched with rain seeks shelter in the prince's castle. She claims to be a princess, but the queen has doubts. She decides to test their unexpected guest by placing a pea in the bed she is offered for the night, covered by twenty mattresses and twenty eider-down beds on top of the mattresses.\n",
    "\n",
    "# In the morning, the princess tells her hosts that she endured a sleepless night, kept awake by something hard in the bed that she is certain has bruised her. The prince's family realizes that she is a princess after all, since no one but a real princess could be so delicate. The two are happily married, and the story ends with the pea being placed in a museum, where it might still remain.\"\"\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving object test_nodes to pickle file ./test_nodes.pkl\n"
     ]
    }
   ],
   "source": [
    "pickleSave(test_nodes, 'test_nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving object leafTest2Nodes to pickle file ./leafTest2Nodes.pkl\n"
     ]
    }
   ],
   "source": [
    "pickleSave(nodes, 'leafTest2Nodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading ingested nodes from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading object leafTest2Nodes from pickle file ./leafTest2Nodes.pkl\n"
     ]
    }
   ],
   "source": [
    "nodes = pickleLoad('leafTest2Nodes', folder=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set node ids to be a constant\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"leaf_node-{idx}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display the nodes ingested by the transformations pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import Node\n",
    "from llama_index.schema import ImageNode, MetadataMode, NodeWithScore\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def truncate_text(text: str, max_length: int) -> str:\n",
    "    \"\"\"Truncate text to a maximum length.\"\"\"\n",
    "    if len(text) <= max_length:\n",
    "        return text\n",
    "    return text[: max_length - 3] + \"...\"\n",
    "\n",
    "def display_node(\n",
    "    node: Node,\n",
    "    source_length: int = 300,\n",
    "    show_source_metadata: bool = True,\n",
    "    metadata_mode: MetadataMode = MetadataMode.NONE,\n",
    ") -> None:\n",
    "    \"\"\"Display source node for jupyter notebook.\"\"\"\n",
    "    source_text_fmt = truncate_text(\n",
    "        node.get_content(metadata_mode=metadata_mode).strip(), source_length\n",
    "    )\n",
    "    text_md = (\n",
    "        f\"**Node ID:** {node.node_id}<br>\"\n",
    "        f\"**Text:** {source_text_fmt}<br>\"\n",
    "    )\n",
    "    if show_source_metadata:\n",
    "        text_md += f\"**Metadata:** {node.metadata}<br>\"\n",
    "    if isinstance(node, ImageNode):\n",
    "        text_md += \"**Image:**\"\n",
    "\n",
    "    display(Markdown(text_md))\n",
    "    if isinstance(node, ImageNode) and node.image is not None:\n",
    "        display_image(node.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** leaf_node-0<br>**Text:** Article\n",
       "Targeted suppression of human IBD-associated gut\n",
       "microbiota commensals by phage consortia fortreatment of intestinal inﬂammation\n",
       "Graphical abstract\n",
       "Highlights\n",
       "dKlebsiella pneumoniae (Kp) strains are associated with IBD\n",
       "severity across geography\n",
       "dIsolated Kp strains induce gut inﬂammation upon\n",
       "colonization in animal IBD models\n",
       "dA Kp-targeting ﬁve-phage combination suppressesintestinal inﬂammation in IBD models\n",
       "dPhages consumed by healthy humans are safe and viableand accumulate in the lower gutAuthors\n",
       "Sara Federici, Sharon Kredo-Russo,Rafael Valde ´s-Mas, .\n",
       "Ryan Balfour Sartor, Rotem Sorek,Eran Elinav\n",
       "Correspondence\n",
       "eran.elinav@weizmann.ac.<br>**Metadata:** {'page_label': '2879', 'file_name': '1-s2.0-S0092867422008509-main.pdf', 'file_path': '/workspaces/ml-learning/src/phages/data/test2/1-s2.0-S0092867422008509-main.pdf', 'file_type': 'application/pdf', 'file_size': 13522904, 'creation_date': '2024-01-16', 'last_modified_date': '2024-01-16', 'last_accessed_date': '2024-01-16', 'section_summary': 'The key topics of this section are:\\n\\n1. Targeted suppression of human IBD-associated gut microbiota commensals by phage consortia for the treatment of intestinal inflammation.\\n2. Klebsiella pneumoniae (Kp) strains and their association with IBD severity across geography.\\n3. The induction of gut inflammation by isolated Kp strains upon colonization in animal IBD models.\\n4. The effectiveness of a Kp-targeting five-phage combination in suppressing intestinal inflammation in IBD models.\\n5. The safety and viability of phages consumed by healthy humans, which accumulate in the lower gut.\\n\\nThe key entities mentioned in this section are:\\n\\n1. Sara Federici\\n2. Sharon Kredo-Russo\\n3. Rafael Valdés-Mas\\n4. Ryan Balfour Sartor\\n5. Rotem Sorek\\n6. Eran Elinav', 'excerpt_keywords': 'IBD, gut microbiota, phage therapy, Klebsiella pneumoniae, intestinal inflammation, phage consortia, gut health, human health, animal models, lower gut.'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** leaf_node-1<br>**Text:** Sharon Kredo-Russo,Rafael Valde ´s-Mas, .\n",
       "Ryan Balfour Sartor, Rotem Sorek,Eran Elinav\n",
       "Correspondence\n",
       "eran.elinav@weizmann.ac.il\n",
       "In brief\n",
       "An orally delivered combination of ﬁvephages successfully targets the bacterialpathogen Klebsiella pneumoniae to treat\n",
       "the symptoms of human inﬂammatorybowel disease.\n",
       "Federici et al., 2022, Cell 185, 2879–2898\n",
       "August 4, 2022 ª2022 Elsevier Inc.\n",
       "https://doi.org/10.1016/j.cell.2022.07.003 ll<br>**Metadata:** {'page_label': '2879', 'file_name': '1-s2.0-S0092867422008509-main.pdf', 'file_path': '/workspaces/ml-learning/src/phages/data/test2/1-s2.0-S0092867422008509-main.pdf', 'file_type': 'application/pdf', 'file_size': 13522904, 'creation_date': '2024-01-16', 'last_modified_date': '2024-01-16', 'last_accessed_date': '2024-01-16', 'section_summary': 'The section discusses a study conducted by Federici et al. in 2022, published in Cell, which focuses on the treatment of human inflammatory bowel disease (IBD) using a combination of five phages that target the bacterial pathogen Klebsiella pneumoniae. The study found that orally delivering these phages successfully alleviated the symptoms of IBD. The key entities mentioned in the section include the authors of the study (Sharon Kredo-Russo, Rafael Valde ´s-Mas, Ryan Balfour Sartor, Rotem Sorek, Eran Elinav), the bacterial pathogen (Klebsiella pneumoniae), and the disease being treated (inflammatory bowel disease).', 'excerpt_keywords': 'phages, bacterial pathogen, Klebsiella pneumoniae, human inflammatory bowel disease, orally delivered, combination, symptoms, treatment, Cell, Elsevier Inc.'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** leaf_node-2<br>**Text:** Article\n",
       "Targeted suppression of human IBD-associated gut\n",
       "microbiota commensals by phage consortiafor treatment of intestinal inﬂammation\n",
       "Sara Federici,1,23Sharon Kredo-Russo,2,3,23Rafael Valde ´s-Mas,1,23Denise Kviatcovsky,1,23Eyal Weinstock,2,3,4\n",
       "Yulia Matiuhin,2,3Yael Silberberg,2,3Koji Atarashi,5,6Munehiro Furuichi,5,6Akihiko Oka,7,8Bo Liu,8Morine Fibelman,9,10\n",
       "Iddo Nadav Weiner,2,3Efrat Khabra,2,3Nyssa Cullin,11Noa Ben-Yishai,2,3Dana Inbar,2,3Hava Ben-David,2,3\n",
       "Julian Nicenboim,2,<br>**Metadata:** {'page_label': '2880', 'file_name': '1-s2.0-S0092867422008509-main.pdf', 'file_path': '/workspaces/ml-learning/src/phages/data/test2/1-s2.0-S0092867422008509-main.pdf', 'file_type': 'application/pdf', 'file_size': 13522904, 'creation_date': '2024-01-16', 'last_modified_date': '2024-01-16', 'last_accessed_date': '2024-01-16', 'section_summary': 'The section discusses the targeted suppression of human IBD-associated gut microbiota commensals using phage consortia for the treatment of intestinal inflammation. The key topics include the use of phage consortia as a potential therapy for inflammatory bowel disease (IBD), the identification and targeting of specific gut microbiota commensals associated with IBD, and the potential benefits of this approach in reducing intestinal inflammation. The key entities mentioned in the section include Sara Federici, Sharon Kredo-Russo, Rafael Valde ´s-Mas, Denise Kviatcovsky, Eyal Weinstock, Yulia Matiuhin, Yael Silberberg, Koji Atarashi, Munehiro Furuichi, Akihiko Oka, Bo Liu, Morine Fibelman, Iddo Nadav Weiner, Efrat Khabra, Nyssa Cullin, Noa Ben-Yishai, Dana Inbar, Hava Ben-David, and Julian Nicenboim.', 'excerpt_keywords': 'IBD, gut microbiota, phage consortia, intestinal inflammation, commensals, treatment, suppression, targeted, human, article'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** leaf_node-3<br>**Text:** 2,3Efrat Khabra,2,3Nyssa Cullin,11Noa Ben-Yishai,2,3Dana Inbar,2,3Hava Ben-David,2,3\n",
       "Julian Nicenboim,2,3Noga Kowalsman,2,3Wolfgang Lieb,12Edith Kario,2,3Tal Cohen,2,3Yael Friedman Geffen,2,3\n",
       "Lior Zelcbuch,2,3Ariel Cohen,2,3Urania Rappo,2,3Inbar Gahali-Sass,2,3Myriam Golembo,2,3Vered Lev,2,3\n",
       "Mally Dori-Bachash,1Hagit Shapiro,1Claudia Moresi,1Amanda Cuevas-Sierra,1Gayatree Mohapatra,1Lara Kern,1\n",
       "Danping Zheng,1Samuel Philip Nobs,1Jotham Suez,<br>**Metadata:** {'page_label': '2880', 'file_name': '1-s2.0-S0092867422008509-main.pdf', 'file_path': '/workspaces/ml-learning/src/phages/data/test2/1-s2.0-S0092867422008509-main.pdf', 'file_type': 'application/pdf', 'file_size': 13522904, 'creation_date': '2024-01-16', 'last_modified_date': '2024-01-16', 'last_accessed_date': '2024-01-16', 'section_summary': 'The key topics of the section include the names of various individuals involved in the research or study mentioned. These individuals include Efrat Khabra, Nyssa Cullin, Noa Ben-Yishai, Dana Inbar, Hava Ben-David, Julian Nicenboim, Noga Kowalsman, Wolfgang Lieb, Edith Kario, Tal Cohen, Yael Friedman Geffen, Lior Zelcbuch, Ariel Cohen, Urania Rappo, Inbar Gahali-Sass, Myriam Golembo, Vered Lev, Mally Dori-Bachash, Hagit Shapiro, Claudia Moresi, Amanda Cuevas-Sierra, Gayatree Mohapatra, Lara Kern, Danping Zheng, Samuel Philip Nobs, and Jotham Suez.\\n\\nThe entities mentioned in the section are individuals involved in the research or study.', 'excerpt_keywords': 'Efrat Khabra, Nyssa Cullin, Noa Ben-Yishai, Dana Inbar, Hava Ben-David, Julian Nicenboim, Noga Kowalsman, Wolfgang Lieb, Edith Kario, Tal Cohen'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** leaf_node-4<br>**Text:** 1Hagit Shapiro,1Claudia Moresi,1Amanda Cuevas-Sierra,1Gayatree Mohapatra,1Lara Kern,1\n",
       "Danping Zheng,1Samuel Philip Nobs,1Jotham Suez,1Noa Stettner,13Alon Harmelin,13Naomi Zak,2,3\n",
       "Sailaja Puttagunta,2,3Merav Bassan,2,3Kenya Honda,5,6Harry Sokol,14,15,16Corinna Bang,17Andre Franke,17,18\n",
       "Christoph Schramm,19,20,21Nitsan Maharshak,9,22Ryan Balfour Sartor,8Rotem Sorek,4and Eran Elinav1,11,24, *\n",
       "1Systems Immunology Department, Weizmann Institute of Science, Rehovot, Israel\n",
       "2BiomX Ltd. 22 Einstein St. Ness Ziona, 7414001,<br>**Metadata:** {'page_label': '2880', 'file_name': '1-s2.0-S0092867422008509-main.pdf', 'file_path': '/workspaces/ml-learning/src/phages/data/test2/1-s2.0-S0092867422008509-main.pdf', 'file_type': 'application/pdf', 'file_size': 13522904, 'creation_date': '2024-01-16', 'last_modified_date': '2024-01-16', 'last_accessed_date': '2024-01-16', 'section_summary': 'The key topics of this section are the authors and their affiliations. The section lists the names of the authors and their respective institutions. The entities mentioned include the Weizmann Institute of Science, BiomX Ltd., and Einstein St. Ness Ziona.', 'excerpt_keywords': 'Hagit Shapiro, Claudia Moresi, Amanda Cuevas-Sierra, Gayatree Mohapatra, Lara Kern, Danping Zheng, Samuel Philip Nobs, Jotham Suez, Noa Stettner, Alon Harmelin, Naomi Zak, Sailaja Puttagunta, Merav Bassan, Kenya Honda, Harry Sokol, Corinna Bang, Andre Franke, Christoph Schramm, Nitsan Maharshak, Ryan Balfour Sartor, Rotem Sorek, Eran Elinav, Systems Immunology, Weizmann Institute of Science, BiomX Ltd.'}<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for node in nodes[:5]:\n",
    "  display_node(node, source_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMPLE SUMMARY USING THOSE NODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response_synthesizers import TreeSummarize, Refine\n",
    "from llama_index.types import BaseModel\n",
    "from typing import List\n",
    "\n",
    "from llama_index import SummaryIndex\n",
    "from llama_index.prompts.base import PromptTemplate\n",
    "from llama_index.prompts.prompt_type import PromptType\n",
    "from llama_index.response_synthesizers import (\n",
    "    BaseSynthesizer,\n",
    "    ResponseMode,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "\n",
    "list_index = SummaryIndex(nodes, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt_template = \"\"\"You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\n",
    "  {context_str}\n",
    "\"\"\"\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "   summary_prompt_template, prompt_type=PromptType.SUMMARY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: ['[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" that was published in the journal PeerJ Computer Science. The paper introduces a method called multi-scale color local binary patterns for recognizing visual object classes. The key topics covered in the section include the methodology used in the paper, the application of multi-scale color local binary patterns for object recognition, and the journal in which the paper was published. The section also provides the URL to access the full paper.\\nexcerpt_keywords: pmc, PeerJ Comput Sci, peerj-cs, PeerJ Computer Science, 2376-5992, PeerJ Inc., document, keywords, unique, format\\nExcerpt:\\n-----\\npmc PeerJ Comput Sci PeerJ Comput Sci peerj-cs PeerJ Computer Science 2376-5992 PeerJ Inc.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are multi-scale color local binary patterns for visual object classes recognition and the application of this technique in the field of gastrointestinal (GI) endoscopy. The section also mentions the authors of the paper, their affiliations, and the journal in which the paper was published.\\nexcerpt_keywords: gastrointestinal endoscopy, cancer, diseases, real-time, anatomical landmarks, abnormalities detection, bioinformatics, artificial intelligence, computer vision, gastrointestinal tract\\nExcerpt:\\n-----\\npmc PeerJ Comput Sci PeerJ Comput Sci peerj-cs PeerJ Computer Science 2376-5992 PeerJ Inc. San Diego, USA 38192480 10773696 cs-1685 10.7717/peerj-cs.1685 Bioinformatics Artificial Intelligence Computer Vision Real time anatomical landmarks and abnormalities detection in gastrointestinal tract Khan Zeshan zeshan.khan@nu.edu.pk Tahir Muhammad Atif  FAST School of Computing, National University of Computer and Emerging Sciences, Islamabad Karachi Sindh Pakistan Chaki Jyotismita 19 12 2023 2023 9 e1685 3 3 2023 16 10 2023 ©2023 Khan and Tahir 2023 Khan and Tahir https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License Gastrointestinal (GI) endoscopy is an active research field due to the lethal cancer diseases in the GI tract.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the importance of early cancer diagnosis, the challenges in detecting abnormalities in the GI tract during endoscopy or colonoscopy, and the need for automated detection to reduce risks. The section also discusses the use of a real-time endoscopic abnormalities detection system that combines handcrafted and deep features, with deep features extracted from a lightweight MobileNet convolutional neural network (CNN) architecture.\\nexcerpt_keywords: cancer treatments, early diagnosis, survival chances, miss rate, GI tract, endoscopy, colonoscopy, attentiveness, tiring procedures, lack of training, automated detection, reduction of risks, suspicious frames, abnormalities, anatomical landmarks, real-time endoscopic abnormalities detection system, handcrafted features, deep features, MobileNet convolutional neural network (CNN) architecture\\nExcerpt:\\n-----\\nCancer treatments result better if diagnosed early and it increases the survival chances. There is a high miss rate in the detection of the abnormalities in the GI tract during endoscopy or colonoscopy due to the lack of attentiveness, tiring procedures, or the lack of required training. The procedure of the detection can be automated to the reduction of the risks by identifying and flagging the suspicious frames. A suspicious frame may have some of the abnormality or the information about anatomical landmark in the frame. The frame then can be analysed for the anatomical landmarks and the abnormalities for the detection of disease. In this research, a real-time endoscopic abnormalities detection system is presented that detects the abnormalities and the landmarks. The proposed system is based on a combination of handcrafted and deep features. Deep features are extracted from lightweight MobileNet convolutional neural network (CNN) architecture.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the development of a real-time endoscopic abnormalities detection system, the use of handcrafted and deep features for detection, the use of a lightweight MobileNet convolutional neural network architecture, the use of a genetic algorithm to learn detection thresholds for classes with small inter-class differences, and the evaluation of the system on benchmark datasets. The key entities mentioned are the abnormalities, landmarks, handcrafted and deep features, MobileNet convolutional neural network, genetic algorithm, Kvasir datasets, DowPK dataset, and the detection speed of 41 frames per second.\\nexcerpt_keywords: real-time, endoscopic abnormalities detection system, landmarks, handcrafted features, deep features, MobileNet, convolutional neural network, inter-class difference, intra-class difference, detection threshold, genetic algorithm, benchmark datasets, accuracy, F1-score, Matthews correlation coefficient, Kvasir datasets, DowPK dataset, detection speed.\\nExcerpt:\\n-----\\nIn this research, a real-time endoscopic abnormalities detection system is presented that detects the abnormalities and the landmarks. The proposed system is based on a combination of handcrafted and deep features. Deep features are extracted from lightweight MobileNet convolutional neural network (CNN) architecture. There are some of the classes with a small inter-class difference and a higher intra-class differences, for such classes the same detection threshold is unable to distinguish. The threshold of such classes is learned from the training data using genetic algorithm. The system is evaluated on various benchmark datasets and resulted in an accuracy of 0.99 with the F1-score of 0.91 and Matthews correlation coefficient (MCC) of 0.91 on Kvasir datasets and F1-score of 0.93 on the dataset of DowPK. The system detects abnormalities in real-time with the detection speed of 41 frames per second.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the detection of abnormalities in real-time, medical image analysis, GI tract diagnostics, genetic algorithm, threshold selection, endoscopic disease detection, computer vision, and the funding of the research by the Higher Education Commission (HEC) Pakistan. The key entity mentioned is the HEC Pakistan, which funded the research project.\\nexcerpt_keywords: real-time, abnormalities, detection speed, medical image analysis, GI tract diagnostics, genetic algorithm, threshold selection, endoscopic disease detection, computer vision, Higher Education Commission (HEC) Pakistan\\nExcerpt:\\n-----\\nThe system detects abnormalities in real-time with the detection speed of 41 frames per second. Medical image analysis GI tract diagnostics Genetic algorithm Threshold selection Endoscopic disease detection Computer vision Higher Education Commission (HEC) Pakistan 10225/2017 This research work was funded by the Higher Education Commission (HEC) Pakistan under NRPU Project 10225/2017. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.   Introduction Gastrointestinal (GI) cancer significantly contributes to mortality among various cancer types. Colon cancer is fifth in the most dangerous cancer types concerning the number of affected patients and deaths due to cancer.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are gastrointestinal (GI) cancer, specifically colon cancer and rectum cancer, and the use of computer-aided diagnostic (CAD) systems for analyzing GI tract images or videos. The section mentions that GI cancer is a significant contributor to mortality and provides rankings for colon and rectum cancer in terms of their danger and prevalence. It also discusses the different techniques used in CAD systems, such as machine learning and image processing, and highlights the advantages and limitations of these systems.\\nexcerpt_keywords: GI cancer, colon cancer, rectum cancer, mortality, cancer types, affected patients, deaths, GI tract, computer-aided diagnostic, CAD systems, machine learning, image processing, texture features, color features, detection score, computer vision-based systems.\\nExcerpt:\\n-----\\nIntroduction Gastrointestinal (GI) cancer significantly contributes to mortality among various cancer types. Colon cancer is fifth in the most dangerous cancer types concerning the number of affected patients and deaths due to cancer. Rectum cancer is another type of GI tract cancer that is the 8th most dangerous cancer type in terms of number of cancer patients in the last five years\\xa0( Sung et al., 2021 Sung et al., 2021 Riegler et al., 2017 Pogorelov et al., 2018 Suzuki, 2012 Doi, 2007 There are many techniques to analyze GI tract images or videos using computer-aided diagnostic (CAD) systems. Most CAD systems primarily rely on machine learning and image processing techniques. The systems working on the machine learning algorithms on the texture and color features of the images are faster in detection. However, the detection score of such systems is lower than computer vision-based systems.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the limitations of existing CAD systems in terms of detection speed and accuracy, the need for a system that offers real-time detection with high accuracy, and the three-step model proposed in this research for detecting abnormalities in GI tract images. The entities mentioned in this section include machine learning algorithms, image processing techniques, computer vision-based systems, deep learning-based systems, GI tract images, data preprocessing, light reflection, endoscopic procedures, Open-CV Telea method, high-class imbalance, and image generation.\\nexcerpt_keywords: CAD systems, machine learning, image processing, texture features, color features, detection score, computer vision, deep learning, detection speed, detection accuracy, GI tract images, data preprocessing, light reflection, endoscopic procedures, Open-CV Telea method, class imbalance, image generation, balanced dataset.\\nExcerpt:\\n-----\\nMost CAD systems primarily rely on machine learning and image processing techniques. The systems working on the machine learning algorithms on the texture and color features of the images are faster in detection. However, the detection score of such systems is lower than computer vision-based systems. The deep learning-based systems are sound in detection with a slow detection speed. There is a need for a system that offers a high detection speed in real-time and a justifiable detection accuracy. This research presents a three-step model for detecting abnormalities in the GI tract images. The first step relies on data preprocessing to address two critical challenges in the GI tract datasets. The first challenge is of light reflection on the images due to endoscopic procedures which is mitigated by applying the Open-CV Telea method. The second challenge in the endoscopic datasets is the high-class imbalance, which is mitigated with the generation of new images for all the classes so that the argument and original images combined make the dataset balanced.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the challenges of high-class imbalance in endoscopic datasets, the generation of new images to balance the dataset, the extraction of suitable features for image representation, and the computation of classes for an instance using a neural network and genetic algorithm. The entities mentioned include endoscopic datasets, images, classes, features, neural network, thresholds, and genetic algorithm.\\nexcerpt_keywords: endoscopic datasets, high-class imbalance, image generation, feature extraction, feature combinations, misleading features, feature accuracies, feature diversities, neural network, class computation, genetic algorithm\\nExcerpt:\\n-----\\nThe second challenge in the endoscopic datasets is the high-class imbalance, which is mitigated with the generation of new images for all the classes so that the argument and original images combined make the dataset balanced. The following research step is to extract the most suitable features to represent images better. For this purpose, various features used in the literature have been explored, and a set of best features is selected after applying various feature combinations. The misleading features are excluded using feature accuracies and diversities. The third step of the research is to compute classes for an instance, which has further been divided in two steps. In the first part, a neural network is used to compute the probability of each class for an image and then to select the class based on different thresholds for each class instead of a 0.5 threshold or max value. The threshold for each class is computed by applying a genetic algorithm with the random initial threshold for all the classes.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the methodology named LiRE-CNN, the application of the detection algorithm to benchmark datasets, the use of preprocessing techniques, and the combination of texture features and local binary patterns. The key entities mentioned are the genetic algorithm, LiRE-CNN, benchmark datasets, reflection removal, augmentation, neural network, texture features, local binary patterns, and deep features. The section also highlights the accuracy and detection speed achieved by combining these features.\\nexcerpt_keywords: genetic algorithm, LiRE-CNN, neural network, benchmark datasets, detection algorithm, preprocessing, reflection removal, augmentation, texture features, local binary patterns, deep features, accuracy, optimal time, detection speed\\nExcerpt:\\n-----\\nThe threshold for each class is computed by applying a genetic algorithm with the random initial threshold for all the classes. The methodology is named LiRE-CNN, as it uses some of the Lire features in combination with a neural network architecture. The detection algorithm of the LiRE-CNN is applied to different benchmark datasets with several modifications in the detection approach. The algorithm is also applied with and without preprocessing. The final results show that applying the preprocessing of reflection removal and augmentation and the neural network can achieve the best accuracy with optimal time. The texture features and local binary patterns are faster commutable, and the deep features are good deciders. Combining both types of features resulted in an accuracy of 0.99 with a detection speed of 41 frames per second.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the combination of texture features and local binary patterns for object recognition, the accuracy and speed of the algorithm, and the primary objective of endoscopy or colonoscopy. The key entities mentioned are the Kvasir versions V1, Pogorelov et al., Borgli et al., and the Gastro-Intestinal tract.\\nexcerpt_keywords: texture features, local binary patterns, deep features, accuracy, detection speed, F1-score, Kvasir versions, endoscopy, colonoscopy, Gastro-Intestinal tract\\nExcerpt:\\n-----\\nThe texture features and local binary patterns are faster commutable, and the deep features are good deciders. Combining both types of features resulted in an accuracy of 0.99 with a detection speed of 41 frames per second. The same algorithm resulted in the F1-score of 0.91, 0.90, and 0.91 on the Kvasir versions V1\\xa0( Pogorelov et al., 2017c Pogorelov et al., 2018 Borgli et al., 2020 This article is organized as follows. ‘Related Work’ discusses the related work. The proposed methodology is presented in ‘Proposed Approach’. ‘Experimental Setup’ describes the experimental set-up followed by a discussion about results in ‘Results and Discussion’. ‘Conclusion and Future Work’ concludes the article.  Related Work The primary objective of the endoscopy or colonoscopy is to detect the functionality of the Gastro-Intestinal tract.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the objective of endoscopy or colonoscopy, the abnormalities that can be detected in the GI tract, the disagreement among medical specialists regarding the detection accuracy of these abnormalities, and the existing work on the detection, classification, localization, and segmentation of diseases in endoscopy. The key entities mentioned in this section are polyps, lesions, esophagitis,']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: ['texture features, local binary patterns, deep features, accuracy, detection speed, F1-score, Kvasir versions, endoscopy, colonoscopy, Gastro-Intestinal tract\\nExcerpt:\\n-----\\nThe texture features and local binary patterns are faster commutable, and the deep features are good deciders. Combining both types of features resulted in an accuracy of 0.99 with a detection speed of 41 frames per second. The same algorithm resulted in the F1-score of 0.91, 0.90, and 0.91 on the Kvasir versions V1\\xa0( Pogorelov et al., 2017c Pogorelov et al., 2018 Borgli et al., 2020 This article is organized as follows. ‘Related Work’ discusses the related work. The proposed methodology is presented in ‘Proposed Approach’. ‘Experimental Setup’ describes the experimental set-up followed by a discussion about results in ‘Results and Discussion’. ‘Conclusion and Future Work’ concludes the article.  Related Work The primary objective of the endoscopy or colonoscopy is to detect the functionality of the Gastro-Intestinal tract.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the objective of endoscopy or colonoscopy, the abnormalities that can be detected in the GI tract, the disagreement among medical specialists regarding the detection accuracy of these abnormalities, and the existing work on the detection, classification, localization, and segmentation of diseases in endoscopy. The key entities mentioned in this section are polyps, lesions, esophagitis, and ulcerative colitis.\\nexcerpt_keywords: endoscopy, colonoscopy, Gastro-Intestinal tract, abnormalities, polyps, lesions, esophagitis, ulcerative colitis, detection accuracy, diseases\\nExcerpt:\\n-----\\n‘Experimental Setup’ describes the experimental set-up followed by a discussion about results in ‘Results and Discussion’. ‘Conclusion and Future Work’ concludes the article.  Related Work The primary objective of the endoscopy or colonoscopy is to detect the functionality of the Gastro-Intestinal tract. Several possible abnormalities in the GI tract include polyps, lesions, esophagitis, and ulcerative colitis. There is a strong disagreement among medical specialists about the decision of the various abnormalities that can cause low detection accuracy of the issues in GI tract. Intense work is available on the detection, classification, localization, and segmentation of various diseases in endoscopy.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the disagreement among medical specialists regarding the detection of abnormalities in the GI tract, the detection, classification, localization, and segmentation of diseases in endoscopy, and the limited availability of labeled datasets for applying machine learning algorithms in medical diagnostics. The key entities mentioned are polyps, lesions, and non-polyps images.\\nexcerpt_keywords: medical specialists, abnormalities, low detection accuracy, GI tract, detection, classification, localization, segmentation, diseases, endoscopy, polyps, lesions, elliptical shape, color, position, texture, local intensity variation patterns, global geometric constraints, labeled datasets, machine learning algorithms, class imbalance, data augmentation\\nExcerpt:\\n-----\\nThere is a strong disagreement among medical specialists about the decision of the various abnormalities that can cause low detection accuracy of the issues in GI tract. Intense work is available on the detection, classification, localization, and segmentation of various diseases in endoscopy. The detection tasks are mainly done for the detection of polyps and lesions using elliptical shape, color, position, texture, local intensity variation patterns, and global geometric constraints as features\\xa0( Hwang et al., 2007 Alexandre, Nobre & Casteleiro, 2008 Tajbakhsh et al., 2014 The primary issue in medical diagnostics is the limited availability of labeled datasets for applying machine learning algorithms. The limited dataset availability also generates a problem of class imbalance in the case of detection. It may lead to a higher number of non-polyps images than polyps. The data augmentation is done by using three major techniques.   1.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the limited availability of datasets and the problem of class imbalance in object detection. The section discusses three major techniques used for data augmentation: generative adversarial networks (GANs), angular flips, and adding noise. The section also mentions that the detection of abnormalities is done using deep learning, machine learning, or a combination of both approaches. The entities mentioned in this section include GANs, Luo et al., Chang et al., Khan & Tahir, and Naqvi et al.\\nexcerpt_keywords: limited dataset availability, class imbalance, detection, data augmentation, generative adversarial network, GAN, angular flips, rotation, flipping, cropping, resizing, noise, deep learning, machine learning, abnormalities, combination, unique keywords\\nExcerpt:\\n-----\\nThe limited dataset availability also generates a problem of class imbalance in the case of detection. It may lead to a higher number of non-polyps images than polyps. The data augmentation is done by using three major techniques.   1. Generative adversarial network (GAN): The GANs are generative models used for the generation of images from some of the existing images of various classes.  2. Angular flips: The angular flips have been used for the data augmentation with the operations of the rotation and flipping image with various angles, cropping, and resizing of the image\\xa0( Luo et al., 2019 Chang et al., 2019  3. Noise: Adding noise is another approach used for the data augmentation by various authors\\xa0( Chang et al. 2019 Khan & Tahir, 2018 Chang et al. 2019 The detection of the abnormalities is done by several approaches including deep learning and machine learning and a combination of both the approaches\\xa0( Naqvi et al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the detection of abnormalities in visual object classes recognition, the use of deep learning and machine learning approaches, and the detection of polyps, cancer, or lesions in the GI tract using image features. The key entities mentioned are the authors Khan & Tahir, Chang et al., Naqvi et al., Tan & Triggs, Krizhevsky, Sutskever & Hinton, He et al., Zeiler & Fergus, Simonyan & Zisserman, Szegedy et al., Huang et al., and Zhang et al.\\nexcerpt_keywords: deep learning, machine learning, abnormalities detection, feature extraction, classification, polyps, cancer, lesions, GI tract, image features\\nExcerpt:\\n-----\\n2019 Khan & Tahir, 2018 Chang et al. 2019 The detection of the abnormalities is done by several approaches including deep learning and machine learning and a combination of both the approaches\\xa0( Naqvi et al. 2017 Tan & Triggs, 2010 Khan & Tahir, 2018 Krizhevsky, Sutskever & Hinton, 2012 He et al. 2016 Zeiler & Fergus, 2014 Simonyan & Zisserman, 2014 Szegedy et al. 2015 Huang et al. 2017 Some authors presented approaches of the deep neural network as a feature extractor and then the classification of the images on those deep features with or without state-of-the-art features\\xa0( Naqvi et al. 2017 Zhang et al. 2016 There is a sound contribution available on the detection of polyps, cancer, or lesions in the GI tract using various image features\\xa0( Ojala,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the detection of polyps, cancer, or lesions in the GI tract using various image features and the classification of support vector machine (SVM). The section mentions several studies and approaches that have been used in this field, including the work of Ojala, Pietikäinen & Harwood, Liu et al., Liao, Law & Chung, Zhu, Bichot & Chen, Hearst et al., Tajbakhsh et al., Pogorelov et al., Iakovidis, Maroulis & Karkanis, Alexandre, Nobre & Casteleiro, Karkanis et al., Zhao et al., Bernal, Sánchez & Vilarino, Esgiar et al., and Deeba et al. The section also mentions the publication year of Zhang et al. (2017) and provides a URL to the paper.\\nexcerpt_keywords: polyps, cancer, lesions, GI tract, image features, detection, support vector machine, SVM, classification, sound contribution\\nExcerpt:\\n-----\\n2017 Zhang et al. 2016 There is a sound contribution available on the detection of polyps, cancer, or lesions in the GI tract using various image features\\xa0( Ojala, Pietikäinen & Harwood, 1996 Liu et al. 2016 Liao, Law & Chung, 2009 Zhu, Bichot & Chen, 2010 The approaches using these set of features with the classification of support vector machine (SVM)\\xa0( Hearst et al. 1998 Tajbakhsh et al. 2014 Pogorelov et al. 2017a Iakovidis, Maroulis & Karkanis, 2006 Alexandre, Nobre & Casteleiro, 2008 Karkanis et al. 2001 Zhao et al. 2021 Bernal, Sánchez & Vilarino, 2013 Esgiar et al. 1998 Deeba et al. 2018 Hwang et al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The content includes a list of references cited in the paper, such as Nobre & Casteleiro, Karkanis et al., Zhao et al., Bernal, Sánchez & Vilarino, Esgiar et al., Deeba et al., Hwang et al., Pogorelov et al., Tajbakhsh et al., and Naqvi et al. It also mentions Table 1, which presents an analysis of different approaches for binary class classification on various polyps or lesion detection datasets. The evaluation measures and best scores are also mentioned.\\nexcerpt_keywords: Nobre & Casteleiro, 2008; Karkanis et al. 2001; Zhao et al. 2021; Bernal, Sánchez & Vilarino, 2013; Esgiar et al. 1998; Deeba et al. 2018; Hwang et al. 2007; Pogorelov et al. (2017a); Tajbakhsh et al., 2014; Naqvi et al., 2017\\nExcerpt:\\n-----\\nNobre & Casteleiro, 2008 Karkanis et al. 2001 Zhao et al. 2021 Bernal, Sánchez & Vilarino, 2013 Esgiar et al. 1998 Deeba et al. 2018 Hwang et al. 2007 Pogorelov et al. (2017a) Zhao et al. (2021) Hwang et al. (2007) Deeba et al. (2018) Tajbakhsh et al., 2014 Naqvi et al., 2017 Table 1  10.7717/peerjcs.1685/table-1 Table 1  Analysis of several approaches for binary class classification on various polyps or lesion detection datasets.      Sr. No. Evaluation measure Best scores Dataset   1 Sensitivity 0.94 ( Deeba et\\xa0al. 2018 CE Bleeding Dataset ( Deeba et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the evaluation measures and their best scores for visual object classes recognition using multi-scale color local binary patterns. The evaluation measures include sensitivity, specificity, precision, recall, F1-score, and accuracy. The best scores for these measures are provided for different datasets, such as the CE Bleeding Dataset and the WCE-279 Bleeding Dataset. The authors Deeba et al. (2018) and Zhao et al. (2021) are mentioned as the sources of the scores. The section highlights the high performance of the proposed method in recognizing visual object classes.\\nexcerpt_keywords: Sensitivity, Specificity, Precision, Recall, F1-score, Accuracy, Deeba et al. 2018, CE Bleeding Dataset, Faigel & Cave 2008, Zhao et al. 2021, WCE-279 Bleeding Dataset.\\nExcerpt:\\n-----\\nSr. No. Evaluation measure Best scores Dataset   1 Sensitivity 0.94 ( Deeba et\\xa0al. 2018 CE Bleeding Dataset ( Deeba et\\xa0al. 2018 Faigel & Cave, 2008  2 Specificity 0.95 ( Deeba et\\xa0al. 2018 CE Bleeding Dataset ( Deeba et\\xa0al. 2018 Faigel & Cave, 2008  3 Precision 0.98 ( Zhao et\\xa0al. 2021 WCE-279 Bleeding Dataset ( Zhao et\\xa0al. 2021  4 Recall 0.98 ( Zhao et\\xa0al. 2021 WCE-279 Bleeding Dataset ( Zhao et\\xa0al. 2021  5 F1-score 0.98 ( Zhao et\\xa0al. 2021 WCE-279 Bleeding Dataset ( Zhao et\\xa0al. 2021  6 Accuracy 0.98 ( Zhao et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the performance metrics (F1-score, accuracy, ROC, AUC) achieved by Zhao et al. in their study on visual object classes recognition using multi-scale color local binary patterns. The study utilized the WCE-279 Bleeding Dataset and Polyp2007 dataset. The key topics of the section include the evaluation of the proposed method\\'s performance on different datasets and the contributions made by the study in detecting and classifying abnormalities. The key entities mentioned in the section are Zhao et al., Alexandre, Nobre & Casteleiro, and the benchmark datasets used in the study.\\nexcerpt_keywords: F1-score, Accuracy, ROC, AUC, bleeding dataset, WCE-279, Polyp2007, Zhao et al., Alexandre, Nobre & Casteleiro, abnormalities, benchmark datasets\\nExcerpt:\\n-----\\n2021  5 F1-score 0.98 ( Zhao et\\xa0al. 2021 WCE-279 Bleeding Dataset ( Zhao et\\xa0al. 2021  6 Accuracy 0.98 ( Zhao et\\xa0al. 2021 WCE-279 Bleeding Dataset ( Zhao et\\xa0al. 2021  7 ROC 0.95 ( Alexandre, Nobre & Casteleiro, 2008 Polyp2007 ( Alexandre, Nobre & Casteleiro, 2008  8 AUC 0.83 ( Alexandre, Nobre & Casteleiro, 2008 Polyp2007 ( Alexandre, Nobre & Casteleiro, 2008 Several contributions are used to detect and classify abnormalities on some benchmark datasets.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the use of multi-scale color local binary patterns for visual object classes recognition and the detection and classification of abnormalities in benchmark datasets. The section also mentions the application of these approaches on the Kvasir dataset and the importance of accurately diagnosing and treating polyps, lesions, and other abnormalities within the GI tract. The entities mentioned include the authors Alexandre, Nobre, Casteleiro, Pogorelov, Agrawal, Szegedy, Jha, Hoang, Khan, Tahir,']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: ['F1-score, Accuracy, ROC, AUC, bleeding dataset, WCE-279, Polyp2007, Zhao et al., Alexandre, Nobre & Casteleiro, abnormalities, benchmark datasets\\nExcerpt:\\n-----\\n2021  5 F1-score 0.98 ( Zhao et\\xa0al. 2021 WCE-279 Bleeding Dataset ( Zhao et\\xa0al. 2021  6 Accuracy 0.98 ( Zhao et\\xa0al. 2021 WCE-279 Bleeding Dataset ( Zhao et\\xa0al. 2021  7 ROC 0.95 ( Alexandre, Nobre & Casteleiro, 2008 Polyp2007 ( Alexandre, Nobre & Casteleiro, 2008  8 AUC 0.83 ( Alexandre, Nobre & Casteleiro, 2008 Polyp2007 ( Alexandre, Nobre & Casteleiro, 2008 Several contributions are used to detect and classify abnormalities on some benchmark datasets.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the use of multi-scale color local binary patterns for visual object classes recognition and the detection and classification of abnormalities in benchmark datasets. The section also mentions the application of these approaches on the Kvasir dataset and the importance of accurately diagnosing and treating polyps, lesions, and other abnormalities within the GI tract. The entities mentioned include the authors Alexandre, Nobre, Casteleiro, Pogorelov, Agrawal, Szegedy, Jha, Hoang, Khan, Tahir, and Zhao.\\nexcerpt_keywords: 2008, AUC, Alexandre, Nobre, Casteleiro, Polyp2007, benchmark datasets, Kvasir dataset, abnormalities, detection, classification\\nExcerpt:\\n-----\\n2008  8 AUC 0.83 ( Alexandre, Nobre & Casteleiro, 2008 Polyp2007 ( Alexandre, Nobre & Casteleiro, 2008 Several contributions are used to detect and classify abnormalities on some benchmark datasets. Some of the approaches are applied on the Kvasir dataset ( Pogorelov et al., 2017c Agrawal et al., 2017 Szegedy et al., 2015 Szegedy et al., 2016 Jha et al., 2021a Pogorelov et al., 2017d Pogorelov et al., 2017c Hoang et al., 2018 Pogorelov et al., 2018 Khan & Tahir, 2018 Zhao et al. (2021) Classifying polyps, lesions, and other abnormalities within the GI tract is imperative for accurate diagnosis and subsequent treatment.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the classification of polyps, lesions, and other abnormalities within the GI tract, and the importance of precise information about these abnormalities for accurate diagnosis and treatment. The section mentions various types of abnormalities that can occur in different regions of the GI tract, and references several studies and authors that have contributed to the understanding of these abnormalities.\\nexcerpt_keywords: polyps, lesions, abnormalities, GI tract, diagnosis, treatment, classification, precise information, regions, esophagitis\\nExcerpt:\\n-----\\n(2021) Classifying polyps, lesions, and other abnormalities within the GI tract is imperative for accurate diagnosis and subsequent treatment. This classification requires precise information about the specific regions of these abnormalities. The abnormalities can be a polyp, ulcer, lesion, esophagitis, or cancer. These abnormalities can be in various regions of the GI tract, e.g. Krizhevsky, Sutskever & Hinton, 2012 He et al., 2016 Zeiler & Fergus, 2014 Simonyan & Zisserman, 2014 Szegedy et al., 2015 Huang et al., 2017 Chang et al. (2019) Hoang et al. (2019) Hoang et al. (2018) Luo et al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper presents an analysis of various approaches for the classification of abnormalities and landmarks detection. The authors compare the performance of different approaches, including Adaptive Ensembles, FAST RCNN, and Mobile Net, in terms of accuracy, F1-score, and frames per second (FPS). The datasets used for evaluation include Kvasir V2 dataset.\\nexcerpt_keywords: Adaptive Ensembles, FAST RCNN, Mobile Net, classification, abnormalities, landmarks detection, accuracy, F1-score, FPS, dataset description, Kvasir V2, Pogorelov et al., Hoang et al., Luo et al., Jha et al.\\nExcerpt:\\n-----\\n(2019) Hoang et al. (2019) Hoang et al. (2018) Luo et al. (2019) Table 2  10.7717/peerjcs.1685/table-2 Table 2  Analysis of several various approaches for classification of abnormalities and landmarks detection.      Sr. No. Approach Acc. F1-score FPS Dataset description   1 Adaptive Ensembles ( Luo et\\xa0al. 2019 Jha et\\xa0al. 2021a 0.99 0.95 10 Kvasir V2 dataset ( Pogorelov et\\xa0al. 2018  2 FAST RCNN ( Hoang et\\xa0al. 2018 Jha et\\xa0al. 2021a 0.99 0.94 23 Kvasir V2 ( Pogorelov et\\xa0al. 2018  3 Mobile Net ( Harzig, Einfalt & Lienhart,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses different methods and models used for visual object classes recognition. The key topics include multi-scale color local binary patterns, Kvasir V2 dataset, Mobile Net, Data Enhancement and ResNet50, DenseNet, and Alexnet. The section also mentions the authors and years of publication for each method or model.\\nexcerpt_keywords: Jha et al., 2021a, Kvasir V2, Pogorelov et al., 2018, Mobile Net, Harzig, Einfalt & Lienhart, 2019, Kvasir V1, Data Enhancement, ResNet50, Meng et al., 2019, DenseNet, Alexnet, Hicks et al., 2018\\nExcerpt:\\n-----\\n2018 Jha et\\xa0al. 2021a 0.99 0.94 23 Kvasir V2 ( Pogorelov et\\xa0al. 2018  3 Mobile Net ( Harzig, Einfalt & Lienhart, 2019 0.99 0.88 3,226 A combination of Kvasir V1 ( Pogorelov et\\xa0al. 2017c Pogorelov et\\xa0al. 2018  4 Data Enhancement and ResNet50 ( Meng et\\xa0al. 2019 0.98 0.87 98 A combination of Kvasir V2 ( Pogorelov et\\xa0al. 2018 Pogorelov et\\xa0al. 2017b  5 DenseNet and Alexnet ( Hicks et\\xa0al. 2018 0.99 0.89 1,015 Kvasir V2 dataset ( Pogorelov et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses various methods and models used for visual object classes recognition. The key topics include DenseNet, Alexnet, Multi-Class Classifier Neural Network, Majority Voting, and Inception Res-Net. The section also mentions the Kvasir V2 dataset and the authors Pogorelov et al. as references for the dataset.\\nexcerpt_keywords: Pogorelov et al., 2017b, DenseNet, Alexnet, Hicks et al., 2018, Kvasir V2 dataset, Multi-Class Classifier Neural Network, Hoang et al., 2019, Majority Voting, Khan & Tahir, 2018, Jha et al., 2021a, Inception Res-Net, Kirkerød et al., 2018\\nExcerpt:\\n-----\\n2018 Pogorelov et\\xa0al. 2017b  5 DenseNet and Alexnet ( Hicks et\\xa0al. 2018 0.99 0.89 1,015 Kvasir V2 dataset ( Pogorelov et\\xa0al. 2018  6 Multi-Class Classifier Neural Network ( Hoang et\\xa0al. 2019 0.99 0.88 3.6 Kvasir V2 dataset ( Pogorelov et\\xa0al. 2018  7 Majority Voting ( Khan & Tahir, 2018 Jha et\\xa0al. 2021a 0.98 0.76 43,328 Kvasir V2 dataset ( Pogorelov et\\xa0al. 2018  8 Inception Res-Net ( Kirkerød et\\xa0al. 2018 0.99 0.92 20 Kvasir V2 dataset ( Pogorelov et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the different approaches and models used for visual object classes recognition. The entities mentioned include the Kvasir V2 dataset, Inception Res-Net, Weighted Discriminant Embedding, Hyper Parameter optimised DenseNet 169, and the Kvasir V1 dataset. The section also mentions the presence of abnormalities and landmarks in the dataset, with a total of 23 classes and varying severity levels.\\nexcerpt_keywords: Kvasir V2 dataset, Inception Res-Net, Weighted Discriminant Embedding, Hyper Parameter optimised DenseNet 169, abnormalities, landmarks, severity levels, Pogorelov et al., Kirkerød et al., Ko, Gu & Liu, Jha et al., García-Aguirre et al., detection, classes, dataset, optimised, approach\\nExcerpt:\\n-----\\n328 Kvasir V2 dataset ( Pogorelov et\\xa0al. 2018  8 Inception Res-Net ( Kirkerød et\\xa0al. 2018 0.99 0.92 20 Kvasir V2 dataset ( Pogorelov et\\xa0al. 2018  9 Weighted Discriminant Embedding ( Ko, Gu & Liu, 2018 Jha et\\xa0al. 2021a 0.95 0.48 3,744 Kvasir V2 dataset ( Pogorelov et\\xa0al. 2018  10 Hyper Parameter optimised DenseNet 169 ( García-Aguirre et\\xa0al. 2022 0.98 0.94 10 Kvasir V1 dataset ( Pogorelov et\\xa0al. 2017c There are some approaches for detecting the abnormalities and the landmarks with 23 classes in total, including various severity levels of the abnormalities.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the approaches for detecting abnormalities and landmarks in the Kvasir V1 dataset, the proposed methodology of the research, and the seven main components of the system. The entities mentioned include the DDANet, Tomar et al., Jha et al., Khan et al., Dutta, Bhattacharjee & Barbhuiya, and the Genetic algorithm.\\nexcerpt_keywords: abnormalities, landmarks, severity levels, DDANet, Tomar, Jha, Khan, Dutta, Bhattacharjee, Barbhuiya, data preprocessing, data augmentation, handcrafted feature extraction, deep feature extraction, feature combination, feature selection, network-based classification, genetic algorithm, threshold detection\\nExcerpt:\\n-----\\n2022 0.98 0.94 10 Kvasir V1 dataset ( Pogorelov et\\xa0al. 2017c There are some approaches for detecting the abnormalities and the landmarks with 23 classes in total, including various severity levels of the abnormalities. The approaches of the DDANet ( Tomar et al., 2021 Jha et al., 2021b Khan et al., 2021 Dutta, Bhattacharjee & Barbhuiya, 2021  Proposed Approach In this section, the primary methodology of the research is discussed. The system consists of seven main components. Each component is explored via   1. Data preprocessing.  2. Data augmentation.  3. Handcrafted feature extraction.  4. Deep feature extraction.  5. Feature combination and selection.  6. Network-based classification.  7. Genetic algorithm for threshold detection.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are data preprocessing, data augmentation, handcrafted feature extraction, deep feature extraction, feature combination and selection, network-based classification, and genetic algorithm for threshold detection. The section discusses the importance of removing reflections from endoscopic images to ensure accurate image analysis. The section also mentions the use of a sample set of polyps picture for illustration.\\nexcerpt_keywords: data preprocessing, data augmentation, handcrafted feature extraction, deep feature extraction, feature combination, feature selection, network-based classification, genetic algorithm, threshold detection, endoscopic images\\nExcerpt:\\n-----\\nData preprocessing.  2. Data augmentation.  3. Handcrafted feature extraction.  4. Deep feature extraction.  5. Feature combination and selection.  6. Network-based classification.  7. Genetic algorithm for threshold detection.  Data preprocessing Reflections on the endoscopic images may arise during video capturing. It is necessary to remove reflections since the reflection of light may cause an erroneous image analysis, which can subsequently affect the abnormality and anatomical landmark detection. This reflection can be seen in the sample set of polyps picture in Fig. 1 Pogorelov et al., 2018 Fig.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the importance of removing reflections in image analysis for object recognition, the potential impact of reflections on abnormality and anatomical landmark detection, and the various procedures for removing reflections. The key entities mentioned in this section are the authors Pogorelov et al., the sample set of polyps picture in Fig. 1, the Kvasir V2 dataset, and the references to Criminisi, Pérez & Toyama (2004) and Eq.\\nexcerpt_keywords: reflections, light, image analysis, erroneous, abnormality detection, anatomical landmark detection, polyps, picture, Fig. 1, Pogorelov et al., 2018, Kvasir V2, procedures, human intervention, Criminisi, Pérez, Toyama, image crop, supervised detection, unsupervised detection\\nExcerpt:\\n-----\\nIt is necessary to remove reflections since the reflection of light may cause an erroneous image analysis, which can subsequently affect the abnormality and anatomical landmark detection. This reflection can be seen in the sample set of polyps picture in Fig. 1 Pogorelov et al., 2018 Fig. 1  10.7717/peerjcs.1685/fig-1 Figure 1  Reflections on the images of the Kvasir V2 ( Pogorelov et al., 2018 There are various procedures for removing these reflections with or without human intervention ( Criminisi, Pérez & Toyama, 2004 Image crop: Pogorelov et al., 2018 Supervised detection: Unsupervised detection: Eq.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL:']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: ['The key topics of this section are the importance of removing reflections in image analysis for object recognition, the potential impact of reflections on abnormality and anatomical landmark detection, and the various procedures for removing reflections. The key entities mentioned in this section are the authors Pogorelov et al., the sample set of polyps picture in Fig. 1, the Kvasir V2 dataset, and the references to Criminisi, Pérez & Toyama (2004) and Eq.\\nexcerpt_keywords: reflections, light, image analysis, erroneous, abnormality detection, anatomical landmark detection, polyps, picture, Fig. 1, Pogorelov et al., 2018, Kvasir V2, procedures, human intervention, Criminisi, Pérez, Toyama, image crop, supervised detection, unsupervised detection\\nExcerpt:\\n-----\\nIt is necessary to remove reflections since the reflection of light may cause an erroneous image analysis, which can subsequently affect the abnormality and anatomical landmark detection. This reflection can be seen in the sample set of polyps picture in Fig. 1 Pogorelov et al., 2018 Fig. 1  10.7717/peerjcs.1685/fig-1 Figure 1  Reflections on the images of the Kvasir V2 ( Pogorelov et al., 2018 There are various procedures for removing these reflections with or without human intervention ( Criminisi, Pérez & Toyama, 2004 Image crop: Pogorelov et al., 2018 Supervised detection: Unsupervised detection: Eq.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. It provides a mathematical equation for converting pixel values in an image to binary values based on a threshold. The key topics of the section include image processing, object recognition, and feature extraction. The key entities mentioned are the mathematical equation and the image pixels.\\nexcerpt_keywords: documentclass, amsmath, wasysym, amsfonts, amssymb, amsbsy, upgreek, mathrsfs, eqnarray, unique\\nExcerpt:\\n-----\\n(1) (1) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}{I}_{x,y}\\\\leftarrow \\\\left\\\\{ \\\\begin{array}{@{}ll@{}} \\\\displaystyle 255, &\\\\displaystyle \\\\text{if}{I}_{x,y}\\\\gt 180\\\\\\\\ \\\\displaystyle 0, &\\\\displaystyle \\\\text{otherwise}\\\\\\\\ \\\\displaystyle   \\\\end{array} \\\\right. \\\\end{eqnarray*}\\\\end{document} I x , y ← 255 , if I x , y > 180 0 , otherwise Eqs.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" that was published in the journal PeerJ Computer Science. The paper introduces a method for recognizing visual object classes using multi-scale color local binary patterns. The content of the section also includes equations and a figure.\\nexcerpt_keywords: document, equations, x, y, if statement, otherwise, Fig, unique, keywords\\nExcerpt:\\n-----\\n\\\\end{eqnarray*}\\\\end{document} I x , y ← 255 , if I x , y > 180 0 , otherwise Eqs. (2) (3) Fig.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. The paper was published in the PeerJ Computer Science journal. The section includes equations (2) and (3) and Figure 2. The key topics covered in the section are the calculation of local binary patterns and the thresholding process. The key entity mentioned in the section is the image intensity value (I).\\nexcerpt_keywords: document, Eq., I, x, y, 255, 180, otherwise, Fig., 2\\nExcerpt:\\n-----\\n\\\\end{eqnarray*}\\\\end{document} I x , y ← 255 , if I x , y > 180 0 , otherwise Eqs. (2) (3) Fig. 2 (2) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}{I}_{x,y}\\\\leftarrow \\\\left\\\\{ \\\\begin{array}{@{}ll@{}} \\\\displaystyle 255, &\\\\displaystyle \\\\text{if}{I}_{x,y}\\\\gt 130\\\\wedge \\\\exists Adj({I}_{x,y})\\\\gt 180\\\\\\\\ \\\\displaystyle 0, &\\\\displaystyle \\\\text{otherwise}\\\\\\\\ \\\\displaystyle   \\\\end{array} \\\\right.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. The paper was published in the PeerJ Computer Science journal. The section also includes a URL to access the full article. The key topics covered in the section include the definition and calculation of multi-scale color local binary patterns, as well as their application in visual object classes recognition. The key entity mentioned in the section is the \"I x , y\" variable, which represents the intensity of a pixel in an image.\\nexcerpt_keywords: image processing, thresholding, adjacency, pixel intensity, algorithm\\nExcerpt:\\n-----\\n\\\\end{eqnarray*}\\\\end{document} I x , y ← 255 , if I x , y > 130 ∧ ∃ A d j I x , y > 180 0 , otherwise (3) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}Adj({I}_{x,y})\\\\leftarrow {I}_{x-i,y-j}\\\\forall (i\\\\in 1,0,-1)\\\\forall (j\\\\in 1,0,-1)\\\\end{eqnarray*}\\\\end{document} A d j I x , y ← I x − i ,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the use of multi-scale color local binary patterns for visual object classes recognition and data preprocessing techniques for reflection removal and data augmentation. The section also mentions the existence of a high-class imbalance in some benchmark datasets for endoscopic procedures. The entities mentioned in this section include the authors Telea, Bertalmio, Bertozzi, and Sapiro, as well as the journal PeerJ Computer Science.\\nexcerpt_keywords: image inpainting, reflection removal, data augmentation, class imbalance, endoscopic procedures, abnormalities, benchmark datasets, high-class imbalance, Telea algorithm, Bertalmio algorithm\\nExcerpt:\\n-----\\ny})\\\\leftarrow {I}_{x-i,y-j}\\\\forall (i\\\\in 1,0,-1)\\\\forall (j\\\\in 1,0,-1)\\\\end{eqnarray*}\\\\end{document} A d j I x , y ← I x − i , y − j ∀ i ∈ 1 , 0 , − 1 ∀ j ∈ 1 , 0 , − 1 Telea (2004) Bertalmio, Bertozzi & Sapiro, 2001  10.7717/peerjcs.1685/fig-2 Figure 2  Data preprocessing for the reflection removal and data augmentation for the class imbalance problem.  Data augmentation A few datasets are available for endoscopic procedures, especially for some of the abnormalities. Several abnormalities are found in most patients, while some are rare in very few patients. There exists a high-class imbalance in some of the benchmark datasets.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the issue of high-class imbalance in benchmark datasets for endoscopic procedures, specifically focusing on the Kvasir V2 dataset. The dataset has a minor class with only four samples and a major class with 613 image samples. The section mentions that data augmentation is a common approach to address this issue, and it specifically mentions augmentation using generative models (GANs). The section also references several studies and authors who have worked on this topic.\\nexcerpt_keywords: data augmentation, endoscopic procedures, abnormalities, high-class imbalance, benchmark datasets, Kvasir V2, minor class, major class, image samples, generative models, GANs\\nExcerpt:\\n-----\\nData augmentation A few datasets are available for endoscopic procedures, especially for some of the abnormalities. Several abnormalities are found in most patients, while some are rare in very few patients. There exists a high-class imbalance in some of the benchmark datasets. The dataset of Kvasir V2 has a minor class with just four samples and a major class with 613 image samples ( Pogorelov et al. 2018 Borgli et al. 2020 Hoang et\\xa0al. 2018 Dias & Dias, 2018 Khan & Tahir, 2018 The common approaches for data augmentation are augmentation using generative models (GANs) ( Goodfellow et al. 2014 Shijie et al. 2017 Shorten & Khoshgoftaar, 2019 Ethiraj & Bolla, 2022 Kamruzzaman et al. 2022 Pogorelov et al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are image manipulation techniques used for visual object classes recognition. The section discusses various techniques such as rotation, flipping, cropping, resizing, and noise injection. These techniques are applied to images as a post-step of reflection removal and a pre-step to feature extraction and fine-tuning of the neural network. The section also mentions the authors and years of publication of related studies.\\nexcerpt_keywords: Shijie et al., Shorten & Khoshgoftaar, Ethiraj & Bolla, Kamruzzaman et al., Pogorelov et al., Rotate, Flip, Crop, Resize, Noise injection, Image manipulation, Reflection removal, Feature extraction, Fine-tuning, Neural network, Classes Eq.\\nExcerpt:\\n-----\\n2014 Shijie et al. 2017 Shorten & Khoshgoftaar, 2019 Ethiraj & Bolla, 2022 Kamruzzaman et al. 2022 Pogorelov et al. 2018   • Rotate (Random Angles)  • Flip (Random Angles)  • Crop (Random size from original to 256\\xa0×\\xa0256)  • Resize (Random size from original to 256\\xa0×\\xa0256)  • Noise injection ( Tammina, 2022 The image manipulation is done as a post-step of the reflection removal and a pre-step to the feature extraction and fine-tuning of the neural network as shown in Fig. 2 C Classes Eq.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. It mentions that image manipulation is performed as a post-step of reflection removal and a pre-step to feature extraction and fine-tuning of the neural network. The section also includes an equation (Eq. 4) related to generating I, C, and Classes. The key topics of the section are multi-scale color local binary patterns, visual object classes recognition, image manipulation, reflection removal, feature extraction, fine-tuning of neural network, and the equation for generating I, C, and Classes.\\nexcerpt_keywords: image manipulation, reflection removal, feature extraction, fine-tuning, neural network, post-step, pre-step, Fig. 2 C Classes, Eq. (4), Generate, I, C, Classes\\nExcerpt:\\n-----\\n2022 The image manipulation is done as a post-step of the reflection removal and a pre-step to the feature extraction and fine-tuning of the neural network as shown in Fig. 2 C Classes Eq. (4) (4) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}Generate\\\\{ I,C,Classes\\\\} \\\\leftarrow \\\\lfloor \\\\{ ma{x}_{Ci\\\\in Classes}(count(Ci))\\\\ast 1.1-count(C)\\\\} \\\\rfloor \\\\end{eqnarray*}\\\\end{document} G e n e r a t e I , C , C l a s s e s ← ⌊ m a x C i ∈ C l a s s e s c o u n t C i ∗ 1 .\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are image manipulation methods, texture feature extraction, and the use of decision tree classifiers. The section also mentions the Lucene Image Retrieval (LIRE) tool and references some specific texture features such as the Local Binary Patterns (LBP) and Haralick features.\\nexcerpt_keywords: image manipulation, decision tree classifier, texture feature extraction, Lucene Image Retrieval, LIRE, Lux & Chatzichristofis, Ojala, Pietikäinen & Harwood, Haralick & Shanmugam, detection F1-score, class C, count(C), count(Ci)\\nExcerpt:\\n-----\\n1 − c o u n t C ⌋ where count(C) is the number of images available in the class C from which class the current image belongs to and count (Ci) is the number of images available in class Ci. The average detection F1-score of the image manipulation method is above 0.84 by applying the decision tree classifier on various sets of features.  Texture feature extraction There are several state-of-the-art features shown as good deciders. Some of the texture features are available in Lucene Image Retrieval (LIRE) ( Lux & Chatzichristofis, 2008 Ojala, Pietikäinen & Harwood, 1996 Haralick & Shanmugam, 1973 Fig.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of Multi-scale color local binary patterns for visual object classes recognition. It mentions the availability of texture features in Lucene Image Retrieval (LIRE) and references several research papers on the topic. The key topics of the section include texture features, Multi-scale color local binary patterns, and visual object classes recognition. The entities mentioned are Lucene Image Retrieval (LIRE), Lux & Chatzichristofis (authors of a research paper), Ojala, Pietikäinen & Harwood (authors of another research paper), and Haralick & Shanmugam (authors of yet']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: ['Ojala, Pietikäinen & Harwood, Haralick & Shanmugam, detection F1-score, class C, count(C), count(Ci)\\nExcerpt:\\n-----\\n1 − c o u n t C ⌋ where count(C) is the number of images available in the class C from which class the current image belongs to and count (Ci) is the number of images available in class Ci. The average detection F1-score of the image manipulation method is above 0.84 by applying the decision tree classifier on various sets of features.  Texture feature extraction There are several state-of-the-art features shown as good deciders. Some of the texture features are available in Lucene Image Retrieval (LIRE) ( Lux & Chatzichristofis, 2008 Ojala, Pietikäinen & Harwood, 1996 Haralick & Shanmugam, 1973 Fig.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of Multi-scale color local binary patterns for visual object classes recognition. It mentions the availability of texture features in Lucene Image Retrieval (LIRE) and references several research papers on the topic. The key topics of the section include texture features, Multi-scale color local binary patterns, and visual object classes recognition. The entities mentioned are Lucene Image Retrieval (LIRE), Lux & Chatzichristofis (authors of a research paper), Ojala, Pietikäinen & Harwood (authors of another research paper), and Haralick & Shanmugam (authors of yet another research paper).\\nexcerpt_keywords: Lucene Image Retrieval, LIRE, texture features, Lux & Chatzichristofis, Ojala, Pietikäinen & Harwood, Haralick & Shanmugam, methodology, research, image retrieval, keyword extraction\\nExcerpt:\\n-----\\nSome of the texture features are available in Lucene Image Retrieval (LIRE) ( Lux & Chatzichristofis, 2008 Ojala, Pietikäinen & Harwood, 1996 Haralick & Shanmugam, 1973 Fig. 3 Lux & Chatzichristofis, 2008 Haralick & Shanmugam, 1973 Ojala, Pietikäinen & Harwood, 1996  10.7717/peerjcs.1685/fig-3 Figure 3  Methodology of the research.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the image features commonly used for detecting abnormalities and their localization and segmentation. These features include Color Layout, Edge Histogram, Tamura, Color and Edge Directivity Descriptor (CEDD), Fuzzy color and texture histogram (FCTH), and Color histograms (HSV and RGB). The section also mentions the researchers who have contributed to the development of these features, such as Sikora, Tamura, Chatzichristofis, Boutalis, Huang, Lux, and Mori & Yamawaki. The section provides a list of references for further reading on these image features.\\nexcerpt_keywords: Color Layout, Edge Histogram, Tamura, Color and Edge Directivity Descriptor, Fuzzy color and texture histogram, Color histograms, HSV, RGB, Lux, Chatzichristofis\\nExcerpt:\\n-----\\nThe most common image features used for the detection of abnormalities and their localization and segmentation are as follows: Color Layout: Sikora, 2001 Edge Histogram: Sikora, 2001 Tamura: Tamura, Mori & Yamawaki, 1978 Color and Edge Directivity Descriptor (CEDD): Chatzichristofis & Boutalis, 2008a Fuzzy color and texture histogram (FCTH): Chatzichristofis & Boutalis, 2008b Color histograms (HSV and RGB): Huang et al., 1997 Lux & Chatzichristofis, 2008 Lux, 2013 Eq.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. It mentions the journal in which the paper was published, \"PeerJ Computer Science,\" and provides a URL to access the full article. The section also briefly mentions Gabor features and auto color correlation as related topics.\\nexcerpt_keywords: Gabor features, Mehrotra, Namuduri & Ranganathan, Lux & Chatzichristofis, Auto Color Correlation\\nExcerpt:\\n-----\\n(5) (5) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}H(I,C)={\\\\forall }_{p\\\\in I,c\\\\in C}P({p}_{x}=={c}_{i})\\\\end{eqnarray*}\\\\end{document} H I , C = ∀ p ∈ I , c ∈ C P p x = = c i Gabor Features: Mehrotra, Namuduri & Ranganathan, 1992 Lux & Chatzichristofis, 2008 Auto Color Correlation: Lux & Chatzichristofis,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are multi-scale color local binary patterns and their application in visual object classes recognition. The section also mentions various techniques and features used in this context, such as Gabor features, auto color correlation, and auto color correlogram. The entities mentioned include the authors of the paper, Mehrotra, Namuduri & Ranganathan, Lux & Chatzichristofis, and Huang et al.\\nexcerpt_keywords: Gabor Features, Mehrotra, Namuduri & Ranganathan, Lux & Chatzichristofis, Auto Color Correlation, Lux, Auto Color Correlogram, Huang et al., I, C, Eq\\nExcerpt:\\n-----\\nC = ∀ p ∈ I , c ∈ C P p x = = c i Gabor Features: Mehrotra, Namuduri & Ranganathan, 1992 Lux & Chatzichristofis, 2008 Auto Color Correlation: Lux & Chatzichristofis, 2008 Lux, 2013 Auto Color Correlogram: Huang et al. (1997) I p 1,2,3,.., n C C 1,2,3,…, m k Eq.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. It references a paper by Lux in 2008 and Auto Color Correlogram by Huang et al. in 2013. The section also mentions equations (6) and (6) without providing further details. The key topics of the section are the use of color local binary patterns and their application in recognizing visual object classes. The entities mentioned include Lux, Huang et al., and the equations (6) and (6).\\nexcerpt_keywords: Lux, Auto Color Correlogram, Huang et al., 2008, 2013, image processing, color analysis, image recognition, pattern recognition, algorithm, equation\\nExcerpt:\\n-----\\n2008 Lux, 2013 Auto Color Correlogram: Huang et al. (1997) I p 1,2,3,.., n C C 1,2,3,…, m k Eq. (6) (6) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}ACC(I,C,k)=P({p}_{x}=={c}_{i}\\\\wedge {p}_{y}=={c}_{j}\\\\wedge d({p}_{x},\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the concept of Multi-scale color local binary patterns (MC-LBP) for visual object classes recognition. It introduces the formula for calculating the MC-LBP, which involves the probability of two pixels having specific color values and a specific distance between them. The section also mentions the journal in which the paper was published (PeerJ Computer Science) and provides a URL to access the full article.\\nexcerpt_keywords: probability, coordinates, distance, event, probability distribution, random variables, joint probability, conditional probability, probability function, probability mass function\\nExcerpt:\\n-----\\nC,k)=P({p}_{x}=={c}_{i}\\\\wedge {p}_{y}=={c}_{j}\\\\wedge d({p}_{x},{p}_{y})=k)\\\\end{eqnarray*}\\\\end{document} A C C I , C , k = P p x = = c i ∧ p y = = c j ∧ d p x , p y = k (7) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}d(P,Q)=max({|}{P}_{x}-{Q}_{x}{|},\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are different methods used for visual object classes recognition, including Speeded up robust features (SURF), Pyramid Histogram of Oriented Gradients (PHOG), and Local Binary Patterns (LBP). The section also mentions the authors of these methods and the equations used in their algorithms. The entities mentioned in this section include the authors Bay, Tuytelaars, Gool, Dalal, Triggs, Ojala, Pietikäinen, Harwood, Liu, Liao, Law, Chung, Zhu, Bichot, and Chen.\\nexcerpt_keywords: SURF, PHOG, LBP, feature extraction, image processing, computer vision, object recognition, image matching, keypoint detection, descriptor\\nExcerpt:\\n-----\\nQ)=max({|}{P}_{x}-{Q}_{x}{|},{|}{P}_{y}-{Q}_{y}{|})\\\\end{eqnarray*}\\\\end{document} d P , Q = m a x | P x − Q x | , | P y − Q y | Eq.\\xa0(7) Speeded up robust features (SURF): Bay, Tuytelaars & Gool, 2006 Pyramid Histogram of Oriented Gradients (PHOG): Dalal & Triggs, 2005 Local Binary Patterns (LBP): Ojala, Pietikäinen & Harwood, 1996 Liu et al., 2016 Liao, Law & Chung, 2009 Zhu, Bichot & Chen, 2010 Eqs.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. The key topics covered include the concept of local binary patterns, the use of color information in the patterns, and the multi-scale approach for recognizing object classes. The section also mentions the algorithm used for calculating the patterns and the thresholding process involved. The key entities mentioned in the section are the local binary patterns, color information, object classes, and the algorithm used for recognition.\\nexcerpt_keywords: P(i), X(x,y), I(i,j), if, otherwise, unique, keywords, document, comma, separated\\nExcerpt:\\n-----\\n(9) (8) (8) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}P(i)\\\\leftarrow \\\\left\\\\{ \\\\begin{array}{@{}ll@{}} \\\\displaystyle 1, &\\\\displaystyle \\\\text{if}X(x,y)\\\\geq I(i,j)\\\\\\\\ \\\\displaystyle 0, &\\\\displaystyle \\\\text{otherwise} \\\\end{array} \\\\right.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. It introduces the concept of Local Binary Patterns (LBP) and Local Ternary Patterns (LTP) and their application in recognizing visual object classes. The section also mentions the paper\\'s title, the journal it was published in (PeerJ Computer Science), and provides a URL to access the full article.\\nexcerpt_keywords: LBP, local binary patterns, LTP, local ternary patterns, image processing, computer vision, texture analysis, feature extraction, pattern recognition, image classification\\nExcerpt:\\n-----\\n\\\\end{eqnarray*}\\\\end{document} P i ← 1 , if X x , y ≥ I i , j 0 , otherwise (9) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}LBP(i,j)\\\\leftarrow \\\\sum _{\\\\forall p\\\\in P}{2}^{p}\\\\end{eqnarray*}\\\\end{document} L B P i , j ← ∑ ∀ p ∈ P 2 p Local Ternary Patterns (LTP): Tan & Triggs, 2010 Eq.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. The key topics covered include the concept of local binary patterns, the incorporation of color information, and the use of multiple scales for improved recognition. The section also mentions the algorithm used for assigning values to the patterns, denoted as P(i), based on the comparison of pixel values between the input image X(x,y) and the reference image I(i,j).\\nexcerpt_keywords: P, i, X, x, y, I, j, +1, -1, 0\\nExcerpt:\\n-----\\n(10) (10) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}P \\\\left( i \\\\right) \\\\leftarrow \\\\left\\\\{ \\\\begin{array}{@{}ll@{}} \\\\displaystyle +1, &\\\\displaystyle \\\\text{if}X \\\\left( x,y \\\\right) \\\\gt I \\\\left( i,j \\\\right) \\\\\\\\ \\\\displaystyle']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: ['L B P i , j ← ∑ ∀ p ∈ P 2 p Local Ternary Patterns (LTP): Tan & Triggs, 2010 Eq.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. The key topics covered include the concept of local binary patterns, the incorporation of color information, and the use of multiple scales for improved recognition. The section also mentions the algorithm used for assigning values to the patterns, denoted as P(i), based on the comparison of pixel values between the input image X(x,y) and the reference image I(i,j).\\nexcerpt_keywords: P, i, X, x, y, I, j, +1, -1, 0\\nExcerpt:\\n-----\\n(10) (10) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}P \\\\left( i \\\\right) \\\\leftarrow \\\\left\\\\{ \\\\begin{array}{@{}ll@{}} \\\\displaystyle +1, &\\\\displaystyle \\\\text{if}X \\\\left( x,y \\\\right) \\\\gt I \\\\left( i,j \\\\right) \\\\\\\\ \\\\displaystyle -1, &\\\\displaystyle \\\\text{if}X \\\\left( x,y \\\\right) \\\\lt I \\\\left( i,j \\\\right) \\\\\\\\ \\\\displaystyle 0,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the concept of Multi-scale color local binary patterns for visual object classes recognition. It mentions the journal in which the paper was published, which is PeerJ Computer Science. The section also includes a URL to access the full article. Additionally, it briefly mentions equations related to the Gray-Level Co-Occurrence Matrix (GLCM) and references Haralick & Shanmugam\\'s work from 1973.\\nexcerpt_keywords: image processing, texture analysis, gray-level co-occurrence matrix, Haralick, Shanmugam, pixel intensity, spatial relationship, statistical measures, feature extraction, image classification\\nExcerpt:\\n-----\\ny \\\\right) \\\\gt I \\\\left( i,j \\\\right) \\\\\\\\ \\\\displaystyle -1, &\\\\displaystyle \\\\text{if}X \\\\left( x,y \\\\right) \\\\lt I \\\\left( i,j \\\\right) \\\\\\\\ \\\\displaystyle 0, &\\\\displaystyle \\\\text{otherwise}\\\\\\\\ \\\\displaystyle   \\\\end{array} \\\\right. \\\\end{eqnarray*}\\\\end{document} P i ← + 1 , if X x , y > I i , j − 1 , if X x , y < I i , j 0 , otherwise Gray-Level Co-Occurrence Matrix (GLCM): Haralick & Shanmugam, 1973 P d P Eq.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. It introduces the concept of GLCM (Gray-Level Co-occurrence Matrix) and describes how it is used to count occurrences of specific pixel values in an image. The section also mentions the use of different parameters such as x, y, d, and hr in the GLCM equation. The key topics of the section include GLCM, multi-scale color local binary patterns, and visual object classes recognition.\\nexcerpt_keywords: GLCM, count, i, j, k, I, x, y, d, hr\\nExcerpt:\\n-----\\n(11) (11) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}GLCM(x,y,d,hr)\\\\leftarrow count({\\\\forall }_{i,j,k}({I}_{i,j}=x\\\\wedge {I}_{i,k}=y\\\\wedge {|}j-k{|}=d))\\\\end{eqnarray*}\\\\end{document} G L C M x , y , d , h r ← c o u n t ∀ i , j , k I i , j = x ∧ I i , k = y ∧ | j − k | = d hr Eq.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. It introduces the concept of GLCM (Gray-Level Co-occurrence Matrix) and explains how it is used to count occurrences of specific pixel values in an image. The section also mentions the journal in which the paper was published and provides a URL to access the full article.\\nexcerpt_keywords: GLCM, count, i, j, k, I, x, y, d, vr\\nExcerpt:\\n-----\\n(12) (12) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}GLCM(x,y,d,vr)\\\\leftarrow count({\\\\forall }_{i,j,k}({I}_{i,j}=x\\\\wedge {I}_{k,j}=y\\\\wedge {|}i-k{|}=d))\\\\end{eqnarray*}\\\\end{document} G L C M x , y , d , v r ← c o u n t ∀ i , j , k I i , j = x ∧ I k ,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. It mentions the publication title \"Multi-scale color local binary patterns for visual object classes recognition\" and the journal it was published in, which is PeerJ Computer Science. The section also includes a URL to the article. Additionally, it briefly mentions the Haralick Features and their statistics of GLCM (Gray-Level Co-occurrence Matrix).\\nexcerpt_keywords: GLCM, Haralick Features, Statistics, Count, x, y, d, v, i, j\\nExcerpt:\\n-----\\nj}=y\\\\wedge {|}i-k{|}=d))\\\\end{eqnarray*}\\\\end{document} G L C M x , y , d , v r ← c o u n t ∀ i , j , k I i , j = x ∧ I k , j = y ∧ | i − k | = d Haralick Features (Statistics of GLCM): Haralick & Shanmugam, 1973 Eq.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. It mentions the publication title \"Multi-scale color local binary patterns for visual object classes recognition\" and the journal it was published in, \"PeerJ Computer Science\". The section also provides a URL to access the full article. Additionally, it mentions the use of Haralick features for statistics of GLCM (Gray-Level Co-occurrence Matrix) and deep feature extraction using CNN-based approaches for detecting abnormalities in endoscopy and colonoscopy images.\\nexcerpt_keywords: count, Haralick Features, GLCM, statistics, CNN-based approaches, abnormality, endoscopy, colonoscopy, images, deep feature extraction\\nExcerpt:\\n-----\\ny , d , v r ← c o u n t ∀ i , j , k I i , j = x ∧ I k , j = y ∧ | i − k | = d Haralick Features (Statistics of GLCM): Haralick & Shanmugam, 1973 Eq. (13) (13) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}Haralick(I)=\\\\sum _{i=1}^{n}\\\\sum _{j=1}^{n}(P(i,j)/R)\\\\end{eqnarray*}\\\\end{document} H a r a l i c k I = ∑ i = 1 n ∑ j = 1 n P i , j / R P i j row i column j R  Deep feature extraction Various CNN-based approaches are used to detect abnormality in endoscopy and colonoscopy images.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the use of Dense Net for achieving high detection accuracies in visual object classes recognition, the approach of random combination of various features for classification using a voting classifier, and the importance of feature selection in achieving good detection with optimal classification time. The entities mentioned in this section include the authors of the papers that achieved the best detection accuracies using Dense Net, such as Huang et al., Hicks et al., Jha et al., He et al., Hoang et al., Kirkerød et al., Howard et al., Sandler et al., Harzig, Einfalt & Lienhart, and Deng et al.\\nexcerpt_keywords: Dense Net, detection accuracies, feature selection, deep feature extraction, classification, voting classifier, random combination, optimal classification time, document\\nExcerpt:\\n-----\\nThe best detection accuracies have been achieved using Dense Net ( Huang et al., 2017 Hicks et al., 2018 Jha et al., 2021a He et al., 2016 Hoang et al., 2018 Kirkerød et al., 2018 Jha et al., 2021a Howard et al., 2017 Sandler et al., 2018 Harzig, Einfalt & Lienhart, 2019 Jha et al., 2021a Howard et al., 2017 Sandler et al., 2018 Fig. 4 Deng et al., 2009  10.7717/peerjcs.1685/fig-4 Figure 4  Deep feature extraction methodology.  Feature selection The approach of the random combination of various features for the classification using a voting classifier resulted in good detection with optimal classification time.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the feature selection methodology and the specific features that resulted in the best accuracies for visual object classes recognition. The entities mentioned include Auto Color Correlogram, Color Layout, Edge Histogram, Gabor Features, and JCD (Joint Collection Distances).\\nexcerpt_keywords: Deep feature extraction, feature selection, random combination, voting classifier, classification time, accuracies, Auto Color Correlogram, Color Layout, Edge Histogram, Gabor Features, JCD, Lux & Chatzichristofis, Sikora, Huang et al., Deng et al., optimal classification.\\nExcerpt:\\n-----\\n4 Deng et al., 2009  10.7717/peerjcs.1685/fig-4 Figure 4  Deep feature extraction methodology.  Feature selection The approach of the random combination of various features for the classification using a voting classifier resulted in good detection with optimal classification time. The final set of features resulted in the best accuracies are as follows:   • Auto Color Correlogram ( Lux & Chatzichristofis, 2008  • Color Layout ( Lux & Chatzichristofis, 2008 Sikora, 2001 Huang et al. 1997  • Edge Histogram ( Lux & Chatzichristofis, 2008 Sikora, 2001  • Gabor Features ( Lux & Chatzichristofis, 2008  • JCD (Joint Collection Distances) ( Lux & Chatzichristofis,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses various methods and features used for visual object classes recognition. These include Edge Histogram, Gabor Features, JCD (Joint Collection Distances), Color and Edge Directivity Descriptor, Fuzzy Color and Texture Histogram, Tamura Features, and Local Binary Patterns. The authors provide references to the studies where these methods were introduced and discuss their relevance in object recognition.\\nexcerpt_keywords: Edge Histogram, Gabor Features, JCD, Color and Edge Directivity Descriptor, Fuzzy Color and Texture Histogram, Tamura Features, Local Binary Patterns, Lux & Chatzichristofis, Sikora, Boutalis\\nExcerpt:\\n-----\\n1997  • Edge Histogram ( Lux & Chatzichristofis, 2008 Sikora, 2001  • Gabor Features ( Lux & Chatzichristofis, 2008  • JCD (Joint Collection Distances) ( Lux & Chatzichristofis, 2008  • Color and Edge Directivity Descriptor ( Lux & Chatzichristofis, 2008 Chatzichristofis & Boutalis, 2008a  • Fuzzy Color and Texture Histogram ( Lux & Chatzichristofis, 2008 Chatzichristofis & Boutalis, 2008b  • Tamura Features (Contrast, Direction etc Lux & Chatzichristofis, 2008 Tamura, Mori & Yamawaki, 1978  • Local Binary Patterns (Radius from 1 to 5) ( Haralick & Shanmugam, 1973 Ojala,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the use of multi-scale color local binary patterns for visual object classes recognition and the classification network neural networks. The section also mentions various references and authors related to these topics.\\nexcerpt_keywords: Direction, Lux, Chatzichristofis, Tamura, Mori, Yamawaki, Local Binary Patterns, Haralick, Shanmugam, Ojala, Pietikäinen, Harwood, Deep Features, Howard, Sandler, Harzig, Einfalt, Lienhart, Classification network, Neural Networks, data availability, Deng, Simonyan, Zisserman, Agarap\\nExcerpt:\\n-----\\nDirection etc Lux & Chatzichristofis, 2008 Tamura, Mori & Yamawaki, 1978  • Local Binary Patterns (Radius from 1 to 5) ( Haralick & Shanmugam, 1973 Ojala, Pietikäinen & Harwood, 1996  • Deep Features ( Howard et al. 2017 Sandler et al. 2018 Harzig, Einfalt & Lienhart, 2019  Classification network Neural Networks are good deciders on the availability of sufficient data ( Deng et al. 2009 Simonyan & Zisserman, 2014 Agarap, 2018 Eq.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL:']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: ['Ojala,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the use of multi-scale color local binary patterns for visual object classes recognition and the classification network neural networks. The section also mentions various references and authors related to these topics.\\nexcerpt_keywords: Direction, Lux, Chatzichristofis, Tamura, Mori, Yamawaki, Local Binary Patterns, Haralick, Shanmugam, Ojala, Pietikäinen, Harwood, Deep Features, Howard, Sandler, Harzig, Einfalt, Lienhart, Classification network, Neural Networks, data availability, Deng, Simonyan, Zisserman, Agarap\\nExcerpt:\\n-----\\nDirection etc Lux & Chatzichristofis, 2008 Tamura, Mori & Yamawaki, 1978  • Local Binary Patterns (Radius from 1 to 5) ( Haralick & Shanmugam, 1973 Ojala, Pietikäinen & Harwood, 1996  • Deep Features ( Howard et al. 2017 Sandler et al. 2018 Harzig, Einfalt & Lienhart, 2019  Classification network Neural Networks are good deciders on the availability of sufficient data ( Deng et al. 2009 Simonyan & Zisserman, 2014 Agarap, 2018 Eq.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the use of multi-scale color local binary patterns for visual object classes recognition and the classification network neural networks. The section also mentions the availability of sufficient data for neural networks to make accurate decisions. The key entities mentioned in this section are the authors of the paper (Harzig, Einfalt & Lienhart), the journal it was published in (PeerJ Computer Science), and the references to other studies (Deng et al. 2009, Simonyan & Zisserman 2014, Agarap 2018).\\nexcerpt_keywords: Harzig, Einfalt & Lienhart, 2019, Classification network, Neural Networks, availability of sufficient data, Deng et al. 2009, Simonyan & Zisserman, 2014, Agarap, 2018, Eq. (14), Relu, probability, classes, input feature vectors.\\nExcerpt:\\n-----\\n2018 Harzig, Einfalt & Lienhart, 2019  Classification network Neural Networks are good deciders on the availability of sufficient data ( Deng et al. 2009 Simonyan & Zisserman, 2014 Agarap, 2018 Eq. (14) (14) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}Relu(x)=max(0,x)\\\\end{eqnarray*}\\\\end{document} R e l u x = m a x 0 , x The final output from the third layer was needed to be the probability of the various classes for each of the input feature vectors.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of a sigmoid activation function in the context of multi-scale color local binary patterns for visual object classes recognition. The function is defined by Eq. (15) and (16) and is used to transform input values into a range between 0 and 1. The section does not mention any specific entities.\\nexcerpt_keywords: sigmoid activation function, Eq. (15), Eq. (16), activation function, sigmoid, function, equation, unique, keywords, document\\nExcerpt:\\n-----\\nA sigmoid activation function, shown in Eq.\\xa0(15) (15) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}f(x)=\\\\sigma (x)= \\\\frac{1}{1+{e}^{-x}} \\\\end{eqnarray*}\\\\end{document} f x = σ x = 1 1 + e − x Eq. (16) (16)\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. The key topics covered include the formulation of the recognition model using a neural network, the evaluation of various optimizers for the model, and the use of different layers and parameters in the model. The key entities mentioned in the section are the neural network weights (W), biases (B), and the input data (X). The section also mentions the use of the ReLU activation function and the sigmoid function (σ) in the model.\\nexcerpt_keywords: optimizers, evaluated, unique, keywords, document, format, comma separated, various, sigma, Relu\\nExcerpt:\\n-----\\n(16) (16) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}Y=\\\\sigma ({W}_{3}^{T}\\\\ast Relu({W}_{2}^{T}\\\\ast Relu({W}_{1}^{T}\\\\ast X+{B}_{1})+{B}_{2})+{B}_{3})\\\\end{eqnarray*}\\\\end{document} Y = σ W 3 T ∗ R e l u W 2 T ∗ R e l u W 1 T ∗ X + B 1 + B 2 + B 3 Various optimizers were evaluated for\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the evaluation of various optimizers for a neural network used in visual object classes recognition. The Nadam optimizer is highlighted as it showed good weights adjustment and fast convergence. The equation (17) is provided to represent the Nadam optimizer. The key topics of the section include neural network optimization and the evaluation of different optimizers. The key entities mentioned are the Nadam optimizer and the equation representing it.\\nexcerpt_keywords: neural network, optimizers, weights adjustment, convergence, Nadam, fast convergence, Dozat, equation, learning rate, momentum, adaptive learning rate, gradient descent\\nExcerpt:\\n-----\\nY = σ W 3 T ∗ R e l u W 2 T ∗ R e l u W 1 T ∗ X + B 1 + B 2 + B 3 Various optimizers were evaluated for the neural network. Based on the good weights adjustment and fast convergence, Nadam (shown in Eq. (17) Dozat, 2016 (17) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}{Y}_{i+1}={Y}_{i}- \\\\frac{\\\\eta }{\\\\sqrt{{\\\\hat {v}}_{t}}+\\\\epsilon } ({\\\\beta\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a mathematical equation related to the optimization algorithm used in the paper. The equation involves variables such as η, β1, β2, Y, m, v, and g. The key topics of the section include the optimization algorithm and its components, as well as the variables involved in the equation.\\nexcerpt_keywords: eta, v_t, epsilon, beta_1, m_t, g_t, Y_i, Y_i+1, t, beta_2\\nExcerpt:\\n-----\\n\\\\frac{\\\\eta }{\\\\sqrt{{\\\\hat {v}}_{t}}+\\\\epsilon } ({\\\\beta }_{1}{\\\\hat {m}}_{t}+ \\\\frac{(1-{\\\\beta }_{1}){g}_{t}}{1-{\\\\beta }_{1}^{t}} + \\\\frac{{\\\\beta }_{1}{\\\\beta }_{2}}{1-{\\\\beta }_{1}^{t}} {m}_{t})\\\\end{eqnarray*}\\\\end{document} Y i + 1 = Y i − η v ˆ t + ϵ β 1 m ˆ t + 1 − β 1 g t 1 − β 1 t + β 1 β 2 1 − β 1 t m t where:   • i  • i  • m t  • v t  • g t  •\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. It provides a formula for calculating the value of ${\\\\hat {m}}_{t}$, which is related to the recognition process. The section does not provide any specific key topics or entities.\\nexcerpt_keywords: β 1, m t, v t, g t, i, 1 − β 1, β 2, t, 1 − β 1 t, m t\\nExcerpt:\\n-----\\nβ 1 m ˆ t + 1 − β 1 g t 1 − β 1 t + β 1 β 2 1 − β 1 t m t where:   • i  • i  • m t  • v t  • g t  • \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n${\\\\hat {m}}_{t}$\\\\end{document} m ˆ t\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper proposes an approach for recognizing visual object classes using multi-scale color local binary patterns. The accuracy of the approach is reported to be 0.99 with an F1-score of 0.89 and Matthews Correlation Coefficient (MCC). The section does not provide specific details about the key topics and entities discussed in the paper.\\nexcerpt_keywords: accuracy, F1-score, Matthews Correlation Coefficient, network configurations, unique keywords, document format, machine learning, neural network, Guilford, Matthews\\nExcerpt:\\n-----\\n{m}}_{t}$\\\\end{document} m ˆ t \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n${\\\\hat {v}}_{t}$\\\\end{document} v ˆ t m t v t  • i  • η  • β 1 β 2  • ϵ The accuracy of this approach is 0.99 with the F1-score of 0.89 and Matthews Correlation Coefficient (MCC) ( Guilford, 1954 Matthews, 1975 The configurations of the network used are as follows:   1.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the accuracy and performance metrics of a multi-scale color local binary patterns approach for visual object classes recognition. The section also discusses the configuration of the neural network used in the approach and the use of a genetic algorithm for threshold detection. The entities mentioned include the F1-score, Matthews Correlation Coefficient, fully connected neurons, activation functions, and different classes such as polyp, z-line, and esophagitis.\\nexcerpt_keywords: accuracy, approach, F1-score, Matthews Correlation Coefficient, network, fully connected neurons, Relu Activation Function, Sigmoid Activation Function, NN-based approach, boost, Genetic algorithm, threshold detection, intra-class differences, inter-class differences, polyp, z-line, esophagitis\\nExcerpt:\\n-----\\nβ 2  • ϵ The accuracy of this approach is 0.99 with the F1-score of 0.89 and Matthews Correlation Coefficient (MCC) ( Guilford, 1954 Matthews, 1975 The configurations of the network used are as follows:   1. Layer 1: 64 Fully Connected Neurons with Relu Activation Function  2. Layer 2: 64 Fully Connected Neurons with Relu Activation Function  3. Layer 3: 64 Fully Connected Neurons with Sigmoid Activation Function The NN-based approach resulted in a boost in F1-score from 0.89 to 0.90 and the Matthews Correlation Coefficient (MCC) ( Guilford, 1954 Matthews, 1975  Genetic algorithm for threshold detection The intra-class and inter-class differences are different in different classes. For example, the difference between polyp and z-line class is much higher than between z-line and esophagitis.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the differences between different visual object classes in terms of detection probabilities, the use of a neural network architecture for detection, and the application of a genetic algorithm (GA-Boost) to learn thresholds for each detected class. The key entities mentioned are polyp, z-line, esophagitis, neural network architecture, GA-Boost algorithm, and genetic algorithm.\\nexcerpt_keywords: polyp, z-line, esophagitis, neural network architecture, detection probabilities, threshold detection, genetic algorithm, GA-Boost, crossover operator, modulus operation\\nExcerpt:\\n-----\\nFor example, the difference between polyp and z-line class is much higher than between z-line and esophagitis. The inter-class difference resulted in the low detection probabilities, detected by a neural network architecture for some classes, and higher for others. The detection is boosted with threshold detection using a genetic algorithm (GA-Boost). The GA-Boost algorithm learned the thresholds for each detected class by applying the genetic algorithm starting with a random threshold from these ten (0.0,\\xa00.1,\\xa00.2,\\xa00.3,\\xa00.4,\\xa00.5,\\xa00.6,\\xa00.7,\\xa00.8,\\xa00.9). The crossover operator employed in GA-Boost is an addition with modulus operation. It involves removing the integral part of the floating-point number to ensure that the resulting number lies within the range of 0 to 1. Mathematically, the operator can be represented as Eq.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper:']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: [\"of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the differences between different visual object classes in terms of detection probabilities, the use of a neural network architecture for detection, and the application of a genetic algorithm (GA-Boost) to learn thresholds for each detected class. The key entities mentioned are polyp, z-line, esophagitis, neural network architecture, GA-Boost algorithm, and genetic algorithm.\\nexcerpt_keywords: polyp, z-line, esophagitis, neural network architecture, detection probabilities, threshold detection, genetic algorithm, GA-Boost, crossover operator, modulus operation\\nExcerpt:\\n-----\\nFor example, the difference between polyp and z-line class is much higher than between z-line and esophagitis. The inter-class difference resulted in the low detection probabilities, detected by a neural network architecture for some classes, and higher for others. The detection is boosted with threshold detection using a genetic algorithm (GA-Boost). The GA-Boost algorithm learned the thresholds for each detected class by applying the genetic algorithm starting with a random threshold from these ten (0.0,\\xa00.1,\\xa00.2,\\xa00.3,\\xa00.4,\\xa00.5,\\xa00.6,\\xa00.7,\\xa00.8,\\xa00.9). The crossover operator employed in GA-Boost is an addition with modulus operation. It involves removing the integral part of the floating-point number to ensure that the resulting number lies within the range of 0 to 1. Mathematically, the operator can be represented as Eq.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the crossover operator employed in GA-Boost and the experimental setup used for the empirical analysis of the proposed methodology. The crossover operator is described as an addition with modulus operation, which involves removing the integral part of a floating-point number to ensure it falls within the range of 0 to 1. The experimental setup is not described in detail in this section, but it is mentioned that it is used for the empirical analysis of the proposed methodology.\\nexcerpt_keywords: crossover operator, GA-Boost, addition with modulus operation, floating-point number, range, mathematical representation, empirical analysis, experimental setup, unique keywords\\nExcerpt:\\n-----\\nThe crossover operator employed in GA-Boost is an addition with modulus operation. It involves removing the integral part of the floating-point number to ensure that the resulting number lies within the range of 0 to 1. Mathematically, the operator can be represented as Eq. (18) x y X n Y n Pogorelov et al., 2018 (18) \\\\documentclass[12pt]{minimal}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{wasysym}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{amsbsy}\\n\\\\usepackage{upgreek}\\n\\\\usepackage{mathrsfs}\\n\\\\setlength{\\\\oddsidemargin}{-69pt}\\n\\\\begin{document}\\n\\\\begin{eqnarray*}({X}_{n},{Y}_{n})\\\\leftarrow ((x+2y) \\\\mathrm{}mod 1,(2x+y) \\\\mathrm{}mod 1)\\\\end{eqnarray*}\\\\end{document} X n , Y n ← x + 2 y m o d 1 , 2 x + y m o d 1  Experimental Setup This section discusses the experimental setting used for the empirical analysis of the proposed methodology.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the datasets used in the research on multi-scale color local binary patterns for visual object classes recognition. The datasets focus on the detection of abnormalities in endoscopic or colonoscopic images. The section includes a figure showing images from the Kvasir V2 dataset and a table showing the class distribution in various datasets. The key topics of the section are the datasets used for the analysis of endoscopic abnormalities and the class distribution in these datasets. The key entities mentioned are the Kvasir V1, Kvasir V2, and Hyper Kvasir datasets.\\nexcerpt_keywords: datasets, endoscopic abnormalities, colonoscopic abnormalities, detection, polyps, lesions, Kvasir V2, class distribution, evaluation measures, comparison methods, annotation, images, Kvasir V1, Hyper Kvasir, DowPK Khan\\nExcerpt:\\n-----\\nIt includes a description of the datasets, details of the evaluation measures, and the comparison methods used in the research.  Datasets There are several datasets available for the analysis of endoscopic or colonoscopic abnormalities. Some datasets are annotated for detecting polyps or lesions in the endoscopy. This research focuses on the detection of the abnormalities, so the datasets for the detection are listed below:  10.7717/peerjcs.1685/fig-5 Figure 5  Some images of various classes from Kvasir V2 ( Pogorelov et al., 2018  10.7717/peerjcs.1685/table-3 Table 3  Class distribution in various datasets.      Sr. No. Class name Kvasir V1 Pogorelov et\\xa0al. 2017c Kvasir V2 Pogorelov et\\xa0al. 2018 Hyper Kvasir Borgli et\\xa0al. 2020 DowPK Khan,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the results of various visual object classes recognition experiments conducted by different researchers. The experiments involve different datasets and classes such as barretts, BBPS, blurry nothing, colon clear, dyed lifted polyps, dyed resection margins, and esophagitis A. The section provides the number of instances or samples for each class in different datasets.\\nexcerpt_keywords: barretts, short segment, BBPS, blurry, colon clear, dyed lifted polyps, dyed resection margins, esophagitis A\\nExcerpt:\\n-----\\nSr. No. Class name Kvasir V1 Pogorelov et\\xa0al. 2017c Kvasir V2 Pogorelov et\\xa0al. 2018 Hyper Kvasir Borgli et\\xa0al. 2020 DowPK Khan, 2023   1 barretts – – 41 –  2 barretts short segment – – 53 –  3 BBPS 0–1 – – 646 –  4 BBPS 2–3 – – 1,148 –  5 blurry nothing – 213 – 114  6 colon clear – 1,332 – 38  7 dyed lifted polyps 500 1,013 1,002 7  8 dyed resection margins 500 980 989 85  9 esophagitis A 500 1,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses various visual object classes and their recognition using multi-scale color local binary patterns. The key topics include dyed lifted polyps, dyed resection margins, esophagitis A and B-D, hemorrhoids, impacted stools, instruments, normal cecum, normal pylorus, normal z-line, and polyps. The section also mentions specific numerical values related to these entities, such as their sizes and pixel values.\\nexcerpt_keywords: dyed lifted polyps, dyed resection margins, esophagitis A, esophagitis B-D, hemorrhoids, impacted stools, instruments, normal cecum, normal pylorus, normal z-line\\nExcerpt:\\n-----\\n332 – 38  7 dyed lifted polyps 500 1,013 1,002 7  8 dyed resection margins 500 980 989 85  9 esophagitis A 500 1,000 403 28  10 esophagitis B-D – – 260 –  11 hemmorrhoids – – 6 –  12 impacted stools – 636 131 6  13 instruments – 309 – 20  14 normal cecum 500 1,000 1,009 16  15 normal pylorus 500 1,000 999 35  16 normal z-line 500 1,000 932 135  17 out of patient – 9 – 23  18 polyps 500 987 1,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section contains a list of numerical values and descriptions related to various medical conditions and procedures. Some of the key topics and entities mentioned include normal pylorus, normal z-line, polyps, retroflex rectum, retroflex stomach, and stool.\\nexcerpt_keywords: normal, pylorus, z-line, polyps, retroflex rectum, retroflex stomach, stool, patient, keywords\\nExcerpt:\\n-----\\n000 1,009 16  15 normal pylorus 500 1,000 999 35  16 normal z-line 500 1,000 932 135  17 out of patient – 9 – 23  18 polyps 500 987 1,028 104  19 retroflex rectum – 429 391 82  20 retroflex stomach – 795 764 42  21 stool plenty – 2,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses various medical conditions and their corresponding grades, such as retroflex rectum, retroflex stomach, stool plenty, Terminal Illunium, and ulcerative colitis. It also mentions the Kvasir V1 dataset and the authors of the paper, Pogorelov et al.\\nexcerpt_keywords: retroflex rectum, retroflex stomach, stool plenty, Terminal Illunium, ulcerative colitis grade 0-1, ulcerative colitis grade 1, ulcerative colitis grade 1-2, ulcerative colitis grade 2, ulcerative colitis grade 2-3, ulcerative colitis grade 3\\nExcerpt:\\n-----\\n028 104  19 retroflex rectum – 429 391 82  20 retroflex stomach – 795 764 42  21 stool plenty – 2,331 – 36  22 Terminal Illunium – – 9 –  23 ulcerative colitis grade 0–1 500 999 35 70  24 ulcerative colitis grade 1 – – 201 –  25 ulcerative colitis grade 1–2 – – 11 –  26 ulcerative colitis grade 2 – – 443 –  27 ulcerative colitis grade 2–3 – – 28 –  28 ulcerative colitis grade 3 – – 133 – Kvasir V1 Pogorelov et al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the various classes of images from different datasets, such as dyed-lifted-polyps, dyed-resection-margins, esophagitis, normal-cecum, normal-pylorus, normal-z-line, polyps, ulcerative-colitis, and DowPK dataset. The section also mentions the Kvasir V2 dataset and the Hyper Kvasir Classification method. Additionally, it refers to the evaluation measures used for the segmentation evaluation.\\nexcerpt_keywords: dyed-lifted-polyps, dyed-resection-margins, esophagitis, normal-cecum, normal-pylorus, normal-z-line, polyps, ulcerative-colitis, Kvasir V2, Hyper Kvasir Classification\\nExcerpt:\\n-----\\n2017c   • dyed-lifted-polyps  • dyed-resection-margins  • esophagitis  • normal-cecum  • normal-pylorus  • normal-z-line  • polyps  • ulcerative-colitis Kvasir V2: Pogorelov et al. 2018 Fig. 5 Table 3 Hyper Kvasir Classification: Borgli et al., 2020 Table 3 DowPK: Fig. 6 Table 3  10.7717/peerjcs.1685/fig-6 Figure 6  Some images of various classes from DowPK dataset ( Khan, 2023  Evaluation measures Various evaluation measures have been used for the evaluation of the segmentation.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the evaluation measures used for the research on visual object classes recognition, specifically in the context of the DowPK dataset. The evaluation measures mentioned include accuracy, F1-score, Matthews Correlation Coefficient (MCC), and sensitivity. The DowPK dataset is highlighted as having the highest sensitivity value among the evaluated datasets, indicating the model's strong performance in correctly identifying positive instances within this dataset.\\nexcerpt_keywords: DowPK dataset, segmentation evaluation, accuracy, F1-score, Matthews Correlation Coefficient, Guilford, Matthews, sensitivity, true positive rate, recall, positive instances, model performance, desired outcomes, conditions.\\nExcerpt:\\n-----\\n6 Table 3  10.7717/peerjcs.1685/fig-6 Figure 6  Some images of various classes from DowPK dataset ( Khan, 2023  Evaluation measures Various evaluation measures have been used for the evaluation of the segmentation. Some of the evaluation measures used for the research are as follows: Accuracy: F1-score: Matthews Correlation Coefficient (MCC): Guilford, 1954 Matthews, 1975 Sensitivity: The DowPK dataset demonstrated the highest sensitivity value among the evaluated datasets, reaching an impressive value of 0.93. Sensitivity, also known as the true positive rate or recall, measures the proportion of correctly identified positive instances out of all actual positive instances. A sensitivity value of 0.93 indicates that the model achieved a high level of accuracy in correctly identifying the positive instances within the DowPK dataset. This highlights the model’s strong performance in detecting the desired outcomes or conditions specific to the DowPK dataset.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the sensitivity, specificity, and area under the receiver operating characteristic curve (AUC-ROC) values of a model for visual object classes recognition. The section discusses the model's high accuracy in identifying positive instances within the DowPK dataset, as indicated by a sensitivity value of 0.93. It also highlights the model's proficiency in recognizing negative instances within the DowPK dataset, with a specificity value of 0.93. Additionally, the section mentions that the Kvasir V2 and Hyper Kvasir datasets achieved the highest AUC-ROC values of 0.99, indicating excellent performance in distinguishing between positive and negative instances.\\nexcerpt_keywords: sensitivity, accuracy, positive instances, DowPK dataset, specificity, negative instances, AUC-ROC, Kvasir V2, Hyper Kvasir, performance\\nExcerpt:\\n-----\\nA sensitivity value of 0.93 indicates that the model achieved a high level of accuracy in correctly identifying the positive instances within the DowPK dataset. This highlights the model’s strong performance in detecting the desired outcomes or conditions specific to the DowPK dataset. Specificity: The DowPK dataset showcased the highest specificity value of 0.93 among the assessed datasets. Specificity gauges the ratio of accurately identified negative instances among all the negatives. A specificity score of 0.93 underscores the model’s proficiency in recognizing negative instances within the DowPK dataset. Area under the receiver operating characteristic curve (AUC-ROC): McClish, 1989 The Kvasir V2 and Hyper Kvasir datasets demonstrated the highest AUC-ROC values among the evaluated datasets, both achieving an impressive value of 0.99. A value of 0.99 indicates that the model achieved excellent performance in distinguishing between positive and negative instances within these datasets.\\n-----\\n\\n[Excerpt from document]\\nTitle of this\"]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: [\"key topics of this section are the sensitivity, specificity, and area under the receiver operating characteristic curve (AUC-ROC) values of a model for visual object classes recognition. The section discusses the model's high accuracy in identifying positive instances within the DowPK dataset, as indicated by a sensitivity value of 0.93. It also highlights the model's proficiency in recognizing negative instances within the DowPK dataset, with a specificity value of 0.93. Additionally, the section mentions that the Kvasir V2 and Hyper Kvasir datasets achieved the highest AUC-ROC values of 0.99, indicating excellent performance in distinguishing between positive and negative instances.\\nexcerpt_keywords: sensitivity, accuracy, positive instances, DowPK dataset, specificity, negative instances, AUC-ROC, Kvasir V2, Hyper Kvasir, performance\\nExcerpt:\\n-----\\nA sensitivity value of 0.93 indicates that the model achieved a high level of accuracy in correctly identifying the positive instances within the DowPK dataset. This highlights the model’s strong performance in detecting the desired outcomes or conditions specific to the DowPK dataset. Specificity: The DowPK dataset showcased the highest specificity value of 0.93 among the assessed datasets. Specificity gauges the ratio of accurately identified negative instances among all the negatives. A specificity score of 0.93 underscores the model’s proficiency in recognizing negative instances within the DowPK dataset. Area under the receiver operating characteristic curve (AUC-ROC): McClish, 1989 The Kvasir V2 and Hyper Kvasir datasets demonstrated the highest AUC-ROC values among the evaluated datasets, both achieving an impressive value of 0.99. A value of 0.99 indicates that the model achieved excellent performance in distinguishing between positive and negative instances within these datasets.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the performance evaluation of the model in distinguishing between positive and negative instances, the AUC-ROC value as a measure of classification accuracy, the MCC value as a measure of correct classification, and the comparison of different methods on benchmark datasets. The entities mentioned in this section include the Kvasir V2 and Hyper Kvasir datasets.\\nexcerpt_keywords: value, AUC-ROC, model, performance, positive, negative, instances, datasets, accuracy, classification, Kvasir V2, Hyper Kvasir, MCC, ratio, TP, TN, FP, FN, methods, variants, benchmark\\nExcerpt:\\n-----\\nA value of 0.99 indicates that the model achieved excellent performance in distinguishing between positive and negative instances within these datasets. This high AUC-ROC value highlights the model’s ability to classify instances accurately and effectively separate the two classes in the Kvasir V2 and Hyper Kvasir datasets. When all the samples have been correctly classified, the value of the MCC will be one due to the ratio between the product of TP and TN over the same. The minimum possible value of the MCC can be -1 when all the results are false, and the values of FP and FN are maximum while the values of TP and TN are minimum.  Compared methods In this section, we will first discuss various variants of the proposed methods, followed by comparing those methods on benchmark datasets.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses various variants of the proposed methods for visual object classes recognition. It compares these methods on benchmark datasets and presents an analysis of different approaches for classification of abnormalities and landmarks detection on the Kvasir V1 dataset. The key topics include the proposed methods, benchmark datasets, and the analysis of different approaches. The key entities mentioned in the section are HKBU17, Liu, Gu & Cheung, Jha et al., Ensemble17, Naqvi et al., Inception17, and Petscharnig.\\nexcerpt_keywords: methods, variants, comparison, benchmark datasets, classification, abnormalities, landmarks detection, Kvasir V1 dataset, analysis, approach, accuracy, F1-score, MCC, FPS, HKBU17, Liu, Gu, Cheung, Jha et al., Ensemble17, Naqvi et al., Inception17, Petscharnig, Give\\nExcerpt:\\n-----\\nCompared methods In this section, we will first discuss various variants of the proposed methods, followed by comparing those methods on benchmark datasets. A comparison of the various approaches is shown in Tables 4 5 6  10.7717/peerjcs.1685/table-4 Table 4  Analysis of some of the approaches for classification of abnormalities and landmarks detection on the Kvasir V1 dataset ( Pogorelov et al., 2017c      Sr. No. Approach Acc. F1-score MCC FPS   1 HKBU17 ( Liu, Gu & Cheung, 2017 Jha et\\xa0al. 2021a 0.93 0.70 0.66 2  2 Ensemble17 ( Naqvi et\\xa0al. 2017 Jha et\\xa0al. 2021a 0.94 0.77 0.74 2  3 Inception17 ( Petscharnig,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the performance evaluation of various algorithms for visual object classes recognition. The key topics include the use of multi-scale color local binary patterns and the comparison of different algorithms such as Ensemble17, Inception17, SCL-UMD17, DLGF17, and Lire-CNN. The section also provides the corresponding accuracy scores for each algorithm. The entities mentioned in the section are the authors of the algorithms, such as Naqvi et al., Jha et al., Petscharnig, Schöffmann & Lux, and Agrawal et al. The section also mentions the URL of the paper published in the PeerJ Computer Science journal.\\nexcerpt_keywords: Ensemble17, Naqvi et al. 2017, Jha et al. 2021a, Inception17, Petscharnig, Schöffmann & Lux, 2017, SCL-UMD17, Agrawal et al. 2017, DLGF17, Pogorelov et al. 2017d, Lire-CNN\\nExcerpt:\\n-----\\n93 0.70 0.66 2  2 Ensemble17 ( Naqvi et\\xa0al. 2017 Jha et\\xa0al. 2021a 0.94 0.77 0.74 2  3 Inception17 ( Petscharnig, Schöffmann & Lux, 2017 0.94 0.76 0.72 1  4 SCL-UMD17 ( Agrawal et\\xa0al. 2017 0.96 0.85 0.83 1  5 DLGF17 ( Pogorelov et\\xa0al. 2017d Jha et\\xa0al. 2021a 0.96 0.83 0.79 46  6 Lire-CNN (Proposed) 0.99 0.90 0.90 41  10.7717/peerjcs.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the analysis of various approaches for classification of abnormalities and landmarks detection on the Kvasir V2 dataset. The key topics include the evaluation of different approaches such as EEIC19, EEIC19 Fast, TLCSS18, and Lire-CNN (Proposed) in terms of accuracy, F1-score, MCC, and FPS. The entities mentioned in the section include the authors Jha et al., Pogorelov et al., Hoang et al., and Dias & Dias.\\nexcerpt_keywords: Jha et al., 2021a, Lire-CNN, classification, abnormalities, landmarks detection, Kvasir V2 dataset, Pogorelov et al., 2018, EEIC19, Hoang et al., 2019, TLCSS18, Dias & Dias\\nExcerpt:\\n-----\\n2017d Jha et\\xa0al. 2021a 0.96 0.83 0.79 46  6 Lire-CNN (Proposed) 0.99 0.90 0.90 41  10.7717/peerjcs.1685/table-5 Table 5  Analysis of various approaches for classification of abnormalities and landmarks detection on the Kvasir V2 dataset ( Pogorelov et al. 2018      Sr. No. Approach Acc. F1-score MCC FPS   1 EEIC19 ( Hoang et\\xa0al. 2019 0.99 0.95 0.94 23  1 EEIC19 Fast ( Hoang et\\xa0al. 2019 0.99 0.88 0.89 3  2 TLCSS18 ( Dias & Dias, 2018 Jha et\\xa0al. 2021a 0.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section provides a list of different methods and their performance metrics for visual object classes recognition. The key topics discussed include the use of multi-scale color local binary patterns for recognition, the journals in which the methods were published, and the corresponding URLs for accessing the papers. The entities mentioned in the section include the authors of the methods (Hoang et al., Dias & Dias, Hicks et al., Ko, Gu & Liu) and the performance metrics (precision, recall, and F1-score) of the methods.\\nexcerpt_keywords: EEIC19, Fast, Hoang et al. 2019, TLCSS18, Dias & Dias, 2018, Jha et al. 2021a, FRCSS18, DSTL18, Hicks et al. 2018, WDE18, Ko, Gu & Liu\\nExcerpt:\\n-----\\n94 23  1 EEIC19 Fast ( Hoang et\\xa0al. 2019 0.99 0.88 0.89 3  2 TLCSS18 ( Dias & Dias, 2018 Jha et\\xa0al. 2021a 0.98 0.87 0.89 3  2 TLCSS18 Fast ( Dias & Dias, 2018 Jha et\\xa0al. 2021a 0.99 0.91 0.90 27  3 FRCSS18 ( Hoang et\\xa0al. 2018 Jha et\\xa0al. 2021a 0.99 0.94 0.94 23  4 DSTL18 ( Hicks et\\xa0al. 2018 Jha et\\xa0al. 2021a 0.99 0.89 0.89 1,015  5 WDE18 ( Ko, Gu & Liu,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the performance analysis of several approaches for classification of abnormalities and landmarks detection on the Kvasir V3 (Hyper Kvasir) dataset. The key topics include the evaluation of different methods such as DSTL18, WDE18, MVHC18, and Lire-CNN for visual object classes recognition. The section also mentions the accuracy scores of these approaches and provides a table with the corresponding results. The entities mentioned in the section include the authors of the approaches (Hicks et al., Jha et al., Ko, Gu & Liu, Khan & Tahir, Borgli et al.) and the dataset (Kvasir V3 or Hyper Kvasir).\\nexcerpt_keywords: classification, abnormalities, landmarks detection, Kvasir V3, Hyper Kvasir, dataset, DSTL18, Hicks et al., Jha et al., WDE18, Ko, Gu & Liu, MVHC18, Khan & Tahir, Lire-CNN, analysis\\nExcerpt:\\n-----\\n94 0.94 23  4 DSTL18 ( Hicks et\\xa0al. 2018 Jha et\\xa0al. 2021a 0.99 0.89 0.89 1,015  5 WDE18 ( Ko, Gu & Liu, 2018 0.95 0.48 0.54 3,744  6 MVHC18 ( Khan & Tahir, 2018 0.98 0.75 0.81 43,328  7 Lire-CNN (Proposed) 0.99 0.90 0.90 41  10.7717/peerjcs.1685/table-6 Table 6  Analysis of several approaches for classification of abnormalities and landmarks detection on the Kvasir V3 (Hyper Kvasir) dataset ( Borgli et al. 2020      Sr. No. Approach Acc.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the analysis of several approaches for classification of abnormalities and landmarks detection on the Kvasir V3 dataset. The key topics include the different approaches used for classification, such as HMTA, HLNN, EDTDE, and Lire-CNN. The section also presents the results of these approaches in terms of accuracy, F1-score, MCC, and FPS. The entities mentioned in the section include the authors of the approaches, such as Galdran, Carneiro & Ballester, He et al., Dutta, Bhattacharjee & Barbhuiya, and the proposed Lire-CNN approach.\\nexcerpt_keywords: classification, abnormalities, landmarks detection, Kvasir V3, Hyper Kvasir, dataset, HMTA, HLNN, EDTDE, Lire-CNN, accuracy, F1-score, MCC, FPS, improvements, detection accuracy, time\\nExcerpt:\\n-----\\n7717/peerjcs.1685/table-6 Table 6  Analysis of several approaches for classification of abnormalities and landmarks detection on the Kvasir V3 (Hyper Kvasir) dataset ( Borgli et al. 2020      Sr. No. Approach Acc. F1-score MCC FPS   1 HMTA ( Galdran, Carneiro & Ballester, 2021 0.99 0.87 0.86 20  2 HLNN ( He et\\xa0al., 2021 0.98 0.90 0.89 129  3 EDTDE ( Dutta, Bhattacharjee & Barbhuiya, 2021 0.93 0.76 0.76 49  4 Lire-CNN (Proposed) 0.99 0.91 0.91 41  Results and discussion This section presents the results of various steps in terms of improvements in detection accuracy and time.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the impact of different steps in the proposed approach for visual object classes recognition. It specifically focuses on the reflection removal step and evaluates two common approaches, Image Crop and Unsupervised Detection, on multiple datasets. The impact of reflection removal on the abnormalities detection F1-score is presented in Table 7. The key topics of this section are the proposed approach, reflection removal, Image Crop, Unsupervised Detection, datasets, and abnormalities detection F1-score. The key entities mentioned are Kvasir V1 dataset, Pogorelov et al. (2017c, 2018), and Borgli et al. (2020).\\nexcerpt_keywords: reflection removal, image crop, unsupervised detection, datasets, Kvasir V1, Pogorelov et al., 2017c, Pogorelov et al., 2018, Borgli et al., 2020, Table 7, impact, abnormalities detection, F1-score, original, Telea ref.\\nExcerpt:\\n-----\\nThe section also discusses the impact of each step on the final detection results. Several possible options were used in the literature in each step and evaluated theoretically in ‘Proposed Approach’. This section elaborates on the empirical impact of each possible option in all steps. The steps of the approach are listed below, with the impact on features, interim results, and the final detection results.  Reflection removal Both of the most common reflection removal approaches of the\"]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: ['41  Results and discussion This section presents the results of various steps in terms of improvements in detection accuracy and time.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the impact of different steps in the proposed approach for visual object classes recognition. It specifically focuses on the reflection removal step and evaluates two common approaches, Image Crop and Unsupervised Detection, on multiple datasets. The impact of reflection removal on the abnormalities detection F1-score is presented in Table 7. The key topics of this section are the proposed approach, reflection removal, Image Crop, Unsupervised Detection, datasets, and abnormalities detection F1-score. The key entities mentioned are Kvasir V1 dataset, Pogorelov et al. (2017c, 2018), and Borgli et al. (2020).\\nexcerpt_keywords: reflection removal, image crop, unsupervised detection, datasets, Kvasir V1, Pogorelov et al., 2017c, Pogorelov et al., 2018, Borgli et al., 2020, Table 7, impact, abnormalities detection, F1-score, original, Telea ref.\\nExcerpt:\\n-----\\nThe section also discusses the impact of each step on the final detection results. Several possible options were used in the literature in each step and evaluated theoretically in ‘Proposed Approach’. This section elaborates on the empirical impact of each possible option in all steps. The steps of the approach are listed below, with the impact on features, interim results, and the final detection results.  Reflection removal Both of the most common reflection removal approaches of the Image Crop and the Unsupervised Detection were applied to multiple datasets including Kvasir V1 ( Pogorelov et al., 2017c Pogorelov et al., 2018 Borgli et al., 2020 i.e., Table 7  10.7717/peerjcs.1685/table-7 Table 7  Impact of reflection removal on the abnormalities detection F1-score.      Dataset Original Telea ref.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the performance of different datasets in the context of visual object classes recognition. The datasets mentioned are Kvasir V1, Kvasir V2, Kvasir V3, and DowPK. The section also mentions the use of Telea reflection removal technique and its impact on the F1-score of the datasets. The key topics of the section include dataset performance, reflection removal techniques, and F1-score improvement. The entities mentioned are the dataset names (Kvasir V1, Kvasir V2, Kvasir V3, DowPK) and the Telea reflection removal technique.\\nexcerpt_keywords: Dataset, Original, Telea ref, Image crop, Kvasir V1, Pogorelov et al., 2017c, Kvasir V2, Pogorelov et al., 2018, Kvasir V3, Borgli et al., 2020, DowPK, Reflection removal, F1-score, Unsupervised reflection detection, Improvements.\\nExcerpt:\\n-----\\nDataset Original Telea ref. Image crop   Kvasir V1 ( Pogorelov et\\xa0al., 2017c 0.70 0.71 0.52  Kvasir V2 ( Pogorelov et\\xa0al., 2018 0.81 0.82 0.47  Kvasir V3 ( Borgli et\\xa0al., 2020 0.82 0.82 0.61  DowPK 0.62 0.68 0.63 Similarly, Telea reflection removal boosted the F1-score when applied to the Kvasir V1, Kvasir V2, and the dowPK datasets. The dataset of DowPK resulted in no improvements by applying Image Crop reflection removal and improved with the application of unsupervised reflection detection and removal using Telea.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the impact of reflection removal techniques on the DowPK dataset. It compares the F1-score of different datasets with and without reflection removal using Telea-based reflection removal and Image Crop. The results show that the impact of reflection removal varied across datasets, depending on the number of images within a class containing reflections and the prevalence of reflections in the dataset. The dowPK and Kvasir V1 datasets, which had fewer images and a higher prevalence of reflections, experienced a more significant impact from reflection removal techniques compared to the Kvasir V2 and Hyper Kvasir datasets.\\nexcerpt_keywords: DowPK, dataset, image crop, reflection removal, unsupervised reflection detection, Telea, F1-score, improvements, data characteristics, number of images, class, prevalence of reflections, Kvasir V1, Kvasir V2, Hyper Kvasir.\\nExcerpt:\\n-----\\nThe dataset of DowPK resulted in no improvements by applying Image Crop reflection removal and improved with the application of unsupervised reflection detection and removal using Telea. The comparison of the F1-score of the various datasets with Telea-based reflection removal, Image Crop, and without any reflection removal is shown in Table 7 The impact of reflection removal varied across different datasets, and the results can be attributed to two key data characteristics. Firstly, the number of images within a class containing reflections influenced the effect of reflection removal. When a class had more images, the impact of reflection removal was comparatively lower, resulting in fewer improvements. Secondly, the amount of reflections in the dataset played a role. Datasets with a higher prevalence of reflections experienced a more significant impact from reflection removal techniques. For instance, the dowPK and Kvasir V1 datasets contained a smaller number of images compared to the Kvasir V2 and Hyper Kvasir datasets.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the impact of reflection removal techniques on datasets with different prevalence of reflections. It highlights that datasets with a higher prevalence of reflections, such as dowPK and Kvasir V2, are more affected by reflection removal techniques. The section also mentions the use of data augmentation to address the class imbalance problem in versions 2 and 3 of the Kvasir dataset, which significantly improved detection.\\nexcerpt_keywords: reflection removal, datasets, prevalence, impact, dowPK, Kvasir V1, Kvasir V2, Hyper Kvasir, variations, number of images, class imbalance, data augmentation, detection, low Kvasir V1 datasets\\nExcerpt:\\n-----\\nDatasets with a higher prevalence of reflections experienced a more significant impact from reflection removal techniques. For instance, the dowPK and Kvasir V1 datasets contained a smaller number of images compared to the Kvasir V2 and Hyper Kvasir datasets. Consequently, as illustrated in Table\\xa07 These insights provide an understanding of the variations in the impact of reflection removal across datasets and highlight the influence of factors such as the number of images per class and the amount of reflections present.  Data augmentation The data augmentation technique is applied to the class imbalance problem. The class imbalance was higher in versions 2 and 3 of Kvasir. The augmentation is applied on all the datasets, and it improved the detection significantly in versions 2 and 3 of the Kvasir and affected very low Kvasir V1 datasets where there was no class imbalance.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the class imbalance in versions 2 and 3 of the Kvasir dataset, the application of data augmentation, and its impact on detection performance. The key entities mentioned are the Kvasir V1, V2, and V3 datasets, as well as the authors Pogorelov et al. (2017c, 2018) and Borgli et al. (2020).\\nexcerpt_keywords: class imbalance, versions 2 and 3, Kvasir, augmentation, detection, significantly, datasets, low, improved, Table 8, impact, F-1 score, Pogorelov et al., Borgli et al., Dow\\nExcerpt:\\n-----\\nThe class imbalance was higher in versions 2 and 3 of Kvasir. The augmentation is applied on all the datasets, and it improved the detection significantly in versions 2 and 3 of the Kvasir and affected very low Kvasir V1 datasets where there was no class imbalance. The detection results of the augmented data and un-augmented data are shown in Table 8  10.7717/peerjcs.1685/table-8 Table 8  Impact of data augmentation on the detection F-1 score.      Dataset Original Augmentation   Kvasir V1 ( Pogorelov et\\xa0al. 2017c 0.71 0.71  Kvasir V2 ( Pogorelov et\\xa0al. 2018 0.82 0.84  Kvasir V3 ( Borgli et\\xa0al. 2020 0.82 0.83  Dow 0.68 0.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are multi-scale color local binary patterns for visual object classes recognition and feature selection. The section mentions the Kvasir V2 and Kvasir V3 datasets, as well as the Dow dataset. It also discusses the use of deep features and texture-based features in achieving high accuracy and F1-Score. The section lists several feature selection techniques, including Auto Color Correlogram, Color Layout, and Edge Histogram.\\nexcerpt_keywords: Kvasir V2, Pogorelov et al. 2018, Kvasir V3, Borgli et al. 2020, Dow, Feature selection, deep features, texture-based features, accuracy, F1-Score\\nExcerpt:\\n-----\\n71 0.71  Kvasir V2 ( Pogorelov et\\xa0al. 2018 0.82 0.84  Kvasir V3 ( Borgli et\\xa0al. 2020 0.82 0.83  Dow 0.68 0.79  Feature selection The approach of deep features with texture-based features has been applied and it resulted in an accuracy of 0.99 with the F1-Score of 0.86 on the Kvasir V2 dataset ( Pogorelov et al. 2018 Feature selection Pogorelov et al. 2018   • Auto Color Correlogram ( Huang et al. 1997 Lux & Chatzichristofis, 2008  • Color Layout ( Sikora, 2001 Lux & Chatzichristofis, 2008  • Edge Histogram ( Sikora, 2001 Lux & Chatzichristofis,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses various methods and techniques used for visual object classes recognition. These include Color Layout, Edge Histogram, Gabor, JCD, PHOG, Tamura, and LBP. The section also mentions the authors and years of publication for each method.\\nexcerpt_keywords: 1997 Lux & Chatzichristofis, 2008, Color Layout, Edge Histogram, Gabor, JCD, PHOG, Tamura, LBP, radius values, Ojala, Pietikäinen, Harwood, Liu, Liao, Law, Chung, Zhu, Bichot, Chen\\nExcerpt:\\n-----\\n1997 Lux & Chatzichristofis, 2008  • Color Layout ( Sikora, 2001 Lux & Chatzichristofis, 2008  • Edge Histogram ( Sikora, 2001 Lux & Chatzichristofis, 2008  • Gabor ( Mehrotra, Namuduri & Ranganathan, 1992 Lux & Chatzichristofis, 2008  • JCD ( Lux & Chatzichristofis, 2008  • PHOG ( Dalal & Triggs, 2005  • Tamura ( Tamura, Mori & Yamawaki, 1978  • LBP (for radius values of 1, 2, 3, 4, 5) ( Ojala, Pietikäinen & Harwood, 1996 Liu et al. 2016 Liao, Law & Chung, 2009 Zhu, Bichot & Chen,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the use of multi-scale color local binary patterns for visual object classes recognition and the features that were used for selection but resulted in misleading results. The entities mentioned in the section include the authors of the papers cited (Ojala, Pietikäinen & Harwood, Liu et al., Liao, Law & Chung, Zhu, Bichot & Chen, Sandler et al., Lux & Chatzichristofis, Huang et al., Tan & Triggs, Haralick & Shanmugam) and the journal in which the paper was published (PeerJ Computer Science).\\nexcerpt_keywords: Ojala, Pietikäinen, Harwood, Liu, Liao, Law, Chung, Zhu, Bichot, Chen, Fine-tuned Light Weight Network, Mobile Net V2, Sandler, Color Histogram, Lux, Chatzichristofis, Huang, Auto Color Correlation, Speeded up robust features, SURF, Local Ternary Patterns, LTP, Gray-Level Co-Occurrence Matrix, GLCM, Haralick, Shanmugam\\nExcerpt:\\n-----\\n2, 3, 4, 5) ( Ojala, Pietikäinen & Harwood, 1996 Liu et al. 2016 Liao, Law & Chung, 2009 Zhu, Bichot & Chen, 2010  • Fine-tuned Light Weight Network (Mobile Net V2) ( Sandler et al. 2018 The features that were used for the selection but resulted in the missleading features are as follows:   • Color Histogram ( Lux & Chatzichristofis, 2008 Huang et al. 1997  • Auto Color Correlation ( Lux & Chatzichristofis, 2008  • Speeded up robust features (SURF) ( Lux & Chatzichristofis, 2008  • Local Ternary Patterns (LTP) ( Tan & Triggs, 2010  • Gray-Level Co-Occurrence Matrix (GLCM) ( Haralick & Shanmugam,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are different methods used for visual object classes recognition, including Local Ternary Patterns (LTP), Gray-Level Co-Occurrence Matrix (GLCM), and Haralick Features. The section also mentions the use of various neural classifiers for classification purposes, such as decision tree classifier, random forest classifier, and fully connected neural network. The section concludes by stating that these approaches improved the detection accuracy and achieved a detection time of 41 frames per second (FPS).\\nexcerpt_keywords: Local Ternary Patterns, Gray-Level Co-Occurrence Matrix, Haralick Features, neural classifiers, decision tree classifier, random forest classifier, fully connected neural network, detection accuracy, detection time, frames per second\\nExcerpt:\\n-----\\n2008  • Local Ternary Patterns (LTP) ( Tan & Triggs, 2010  • Gray-Level Co-Occurrence Matrix (GLCM) ( Haralick & Shanmugam, 1973  • Haralick Features (Statistics of GLCM) ( Haralick & Shanmugam, 1973  Classifications The final set of features was used for the classification using various neural classifiers. Some of the reasonable classifications resulted from the classification techniques listed below. Decision tree classifier Random forest classifier Fully connected neural network Eq. (16) The approaches improved the detection accuracy, and the detection time was also 41 frames per second (FPS).\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper:']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: ['Co-Occurrence Matrix (GLCM) ( Haralick & Shanmugam,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are different methods used for visual object classes recognition, including Local Ternary Patterns (LTP), Gray-Level Co-Occurrence Matrix (GLCM), and Haralick Features. The section also mentions the use of various neural classifiers for classification purposes, such as decision tree classifier, random forest classifier, and fully connected neural network. The section concludes by stating that these approaches improved the detection accuracy and achieved a detection time of 41 frames per second (FPS).\\nexcerpt_keywords: Local Ternary Patterns, Gray-Level Co-Occurrence Matrix, Haralick Features, neural classifiers, decision tree classifier, random forest classifier, fully connected neural network, detection accuracy, detection time, frames per second\\nExcerpt:\\n-----\\n2008  • Local Ternary Patterns (LTP) ( Tan & Triggs, 2010  • Gray-Level Co-Occurrence Matrix (GLCM) ( Haralick & Shanmugam, 1973  • Haralick Features (Statistics of GLCM) ( Haralick & Shanmugam, 1973  Classifications The final set of features was used for the classification using various neural classifiers. Some of the reasonable classifications resulted from the classification techniques listed below. Decision tree classifier Random forest classifier Fully connected neural network Eq. (16) The approaches improved the detection accuracy, and the detection time was also 41 frames per second (FPS).\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are classification techniques for visual object recognition, including decision tree classifier, random forest classifier, and fully connected neural network. The section also discusses the improvement in detection accuracy and the detection time of 41 frames per second (FPS). Additionally, it mentions the commendable F1-score of 0.89 for the Kvasir V1 dataset. The section further highlights the significant variations in probability ranges for different classes and the need to utilize different thresholds for each class. It introduces a computational approach called GA-Boost, which is a genetic algorithm used to address this requirement.\\nexcerpt_keywords: classification techniques, decision tree classifier, random forest classifier, fully connected neural network, detection accuracy, detection time, frames per second, F1-score, Kvasir V1 dataset, GA-Boost, tailored class thresholds, probability ranges, ulcerative colitis, polyps, probability disparities, thresholds, computational approach, genetic algorithm, GA-Boost.\\nExcerpt:\\n-----\\nSome of the reasonable classifications resulted from the classification techniques listed below. Decision tree classifier Random forest classifier Fully connected neural network Eq. (16) The approaches improved the detection accuracy, and the detection time was also 41 frames per second (FPS). The final results indicated a commendable F1-score of 0.89 for the Kvasir V1 dataset ( Pogorelov et al., 2017c Pogorelov et al., 2018 Borgli et al., 2020  GA-Boost: a solution to tailored class thresholds Significant variations were found in the probability ranges for different classes. The average probability of images belonging to one class differed from those from other classes. For instance, the images classified as ulcerative colitis exhibited lower probabilities than those classified as polyps. These probability disparities for correctly detected classes highlight the need to utilize different thresholds for each class. A computational approach based on a genetic algorithm named GA-Boost was employed to address this requirement.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the need for different thresholds for each class in visual object recognition, the use of a genetic algorithm named GA-Boost to address this need, and the execution and parameters of the GA-Boost algorithm. The key entities mentioned are ulcerative colitis, polyps, and the GA-Boost algorithm.\\nexcerpt_keywords: ulcerative colitis, polyps, probability disparities, thresholds, computational approach, genetic algorithm, GA-Boost, adaptation, classification process, characteristics\\nExcerpt:\\n-----\\nFor instance, the images classified as ulcerative colitis exhibited lower probabilities than those classified as polyps. These probability disparities for correctly detected classes highlight the need to utilize different thresholds for each class. A computational approach based on a genetic algorithm named GA-Boost was employed to address this requirement. GA-Boost algorithm facilitated the generation of optimal thresholds tailored to each class, accommodating the observed variations in probability distributions within the dataset. GA-Boost allowed for the precise adaptation of thresholds for different classes ensuring an effective classification process based on the specific characteristics of each class. The GA-Boost algorithm learned the thresholds for each detected class by applying the genetic algorithm starting with a random threshold ranging from 0.0 to 1.0 with one floating point value. The crossover operator being used in the GA-Boost is the addition with the modulus operator. The algorithm was executed on a population of 10 probability sets, each comprising 16 randomly generated elements.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the use of a crossover operator in the GA-Boost algorithm for visual object classes recognition. The algorithm is executed on a population of probability sets, and an additive crossover operator with a mutation rate is employed. The F1-score evaluation measure is used to select the best chromosomes. The impact of applying various additions in the procedure on different datasets is also analyzed. The key topics of the section include GA-Boost algorithm, crossover operator, mutation rate, F1-score evaluation measure, and dataset analysis. The key entities mentioned are the Kvasir V1 dataset, Kvasir V2 dataset, Hyper Kvasir dataset, and DowPK dataset.\\nexcerpt_keywords: crossover operator, GA-Boost, addition with modulus operator, probability sets, randomly generated elements, additive crossover operator, mutation rate, F1-score evaluation measure, chromosomes, datasets, Kvasir V1, Pogorelov et al. 2017c, Kvasir V2, Pogorelov et al. 2018, Hyper Kvasir, Borgli et al. 2020, DowPK Khan, 2023, No Preprocessing, No Augmentation, Reflection Removed\\nExcerpt:\\n-----\\nThe crossover operator being used in the GA-Boost is the addition with the modulus operator. The algorithm was executed on a population of 10 probability sets, each comprising 16 randomly generated elements. An additive crossover operator with a mutation rate of 20% was employed. The F1-score evaluation measure was calculated over 20 iterations to select the best chromosomes. As shown in Table 9  10.7717/peerjcs.1685/table-9 Table 9  Impact on F1-score by applying various additions in the procedure on various datasets.      Dataset Kvasir V1 Pogorelov et\\xa0al. 2017c Kvasir V2 Pogorelov et\\xa0al. 2018 Hyper Kvasir Borgli et\\xa0al. 2020 DowPK Khan, 2023   No Preprocessing 0.74 0.75 0.79 0.61  No Augmentation (Reflection Removed) 0.76 0.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section presents the results of an experiment on visual object classes recognition using multi-scale color local binary patterns. The experiment evaluates the performance of different individual features, such as AutoColorCorrelogram, ColorLayout, EdgeHistogram, Gabor, JCD, PHOG, and Tamura. The results show the accuracy scores for each feature. The section also mentions the year and authors of the paper, as well as the journal it was published in. The URL to access the full article is provided as well.\\nexcerpt_keywords: Hyper Kvasir, Borgli, DowPK Khan, Preprocessing, Augmentation, Reflection, AutoColorCorrelogram, ColorLayout, EdgeHistogram, Gabor, JCD, PHOG, Tamura\\nExcerpt:\\n-----\\n2018 Hyper Kvasir Borgli et\\xa0al. 2020 DowPK Khan, 2023   No Preprocessing 0.74 0.75 0.79 0.61  No Augmentation (Reflection Removed) 0.76 0.76 0.79 0.70  Individual Feature AutoColorCorrelogram 0.77 0.81 0.80 0.79  Individual Feature ColorLayout 0.67 0.75 0.74 0.75  Individual Feature EdgeHistogram 0.59 0.64 0.70 0.60  Individual Feature Gabor 0.38 0.30 0.32 0.31  Individual Feature JCD 0.75 0.76 0.78 0.72  Individual Feature PHOG 0.61 0.62 0.65 0.60  Individual Feature Tamura 0.49 0.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the results of a study on visual object classes recognition using multi-scale color local binary patterns. The key topics include the evaluation of different individual features such as JCD, PHOG, and Tamura, as well as the combination of all Lire features, all texture features, and deep features. The section also mentions the use of selected features with an RF-Classifier, a 3 layer neural network, and GA-Boost. The results were evaluated across ten runs and statistical analysis was performed.\\nexcerpt_keywords: Individual Feature JCD, Individual Feature PHOG, Individual Feature Tamura, All Lire Features, All Texture Features, Deep Features, Selected Features (RF-Classifier), 3 Layer Neural Network, GA-Boost, Statistical analysis\\nExcerpt:\\n-----\\n30 0.32 0.31  Individual Feature JCD 0.75 0.76 0.78 0.72  Individual Feature PHOG 0.61 0.62 0.65 0.60  Individual Feature Tamura 0.49 0.48 0.52 0.42  All Lire Features 0.71 0.82 0.82 0.79  All Texture Features 0.75 0.88 0.88 0.89  Deep Features 0.78 0.81 0.83 0.80  Selected Features (RF-Classifier) 0.79 0.89 0.88 0.91  3 Layer Neural Network 0.89 0.90 0.90 0.91  GA-Boost 0.90 0.91 0.91 0.93  Statistical analysis The results were evaluated across ten runs,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the results of a study on multi-scale color local binary patterns for visual object classes recognition. The study was published in the PeerJ Computer Science journal. The results show that the proposed approach outperforms real-time detection methods in terms of accuracy, F1-score, and MCC. The approach also demonstrates a high detection speed of 41 frames per second, making it efficient for real-time applications. The section also mentions the statistical analysis conducted across ten runs and provides a link to Table 10 for a more detailed comparison of the results across different datasets.\\nexcerpt_keywords: neural network, real-time detection, detection accuracy, F1-score, MCC, detection speed, FPS, efficiency, datasets, abnormalities, landmark detection\\nExcerpt:\\n-----\\n89 0.88 0.91  3 Layer Neural Network 0.89 0.90 0.90 0.91  GA-Boost 0.90 0.91 0.91 0.93  Statistical analysis The results were evaluated across ten runs, and a T p Table 9 Our proposed approach outperforms real-time detection methods regarding detection accuracy, F1-score, and MCC. The results showcase superior accuracy and an impressive detection speed of 41 frames per second (FPS) making it highly efficient for real-time applications. For a more in-depth comparison of the results across different datasets, please refer to Table 10  10.7717/peerjcs.1685/table-10 Table 10  Analysis of LiRE-CNN on various datasets for abnormalities and land mark detection with respect to accuracy and speed.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the use of multi-scale color local binary patterns for visual object classes recognition and the analysis of LiRE-CNN on various datasets for abnormalities and landmark detection. The entities mentioned include the Kvasir V1 dataset, Kvasir V2 dataset, Hyper Kvasir dataset, Pogorelov et al. (2017c), Pogorelov et al. (2018), and Borgli et al. (2020).\\nexcerpt_keywords: LiRE-CNN, comparison, datasets, abnormalities, landmark detection, accuracy, speed, Kvasir V1, Kvasir V2, Hyper Kvasir, F1-score, MCC, sensitivity, specificity, AUC-ROC, FPS\\nExcerpt:\\n-----\\nFor a more in-depth comparison of the results across different datasets, please refer to Table 10  10.7717/peerjcs.1685/table-10 Table 10  Analysis of LiRE-CNN on various datasets for abnormalities and land mark detection with respect to accuracy and speed.      Datasets Accuracy F1-score MCC Sensitivity Specificity AUC-ROC FPS   Kvasir V1 ( Pogorelov et\\xa0al. 2017c 0.94 0.90 0.86 0.90 0.90 0.94 41  Kvasir V2 ( Pogorelov et\\xa0al. 2018 0.99 0.91 0.90 0.91 0.91 0.99 41  Hyper Kvasir ( Borgli et\\xa0al. 2020 0.99 0.91 0.91 0.91 0.91 0.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the application of multi-scale color local binary patterns for visual object classes recognition and the analysis of various algorithmic steps. The section also mentions the datasets used and the improvements in the overall F1-score. The entities mentioned include the authors of the paper (Borgli et al.) and the datasets (Hyper Kvasir and DowPK).\\nexcerpt_keywords: composite analysis, algorithmic steps, F1-score, dataset, enhancements, tailored approach, procedure, additions, results analysis, Borgli et al. 2020\\nExcerpt:\\n-----\\n99 0.91 0.90 0.91 0.91 0.99 41  Hyper Kvasir ( Borgli et\\xa0al. 2020 0.99 0.91 0.91 0.91 0.91 0.99 41  DowPK 0.99 0.93 0.92 0.93 0.93 0.99 41 Furthermore, we conducted a composite analysis of various algorithmic steps, as detailed in Table 9 Borgli et al. 2020  Results analysis The application of various additions in the procedure significantly impacted the F1-score across different datasets. The enhancements introduced through these additions resulted in notable improvements in the overall F1-score. The specific effects on the F1-score varied depending on the dataset, indicating the effectiveness of the tailored approach for each dataset.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the enhancements introduced to improve the overall F1-score in visual object classes recognition. The section also discusses the impact of these enhancements on the F1-score, the effects on different datasets, and the rectification']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: ['various algorithmic steps. The section also mentions the datasets used and the improvements in the overall F1-score. The entities mentioned include the authors of the paper (Borgli et al.) and the datasets (Hyper Kvasir and DowPK).\\nexcerpt_keywords: composite analysis, algorithmic steps, F1-score, dataset, enhancements, tailored approach, procedure, additions, results analysis, Borgli et al. 2020\\nExcerpt:\\n-----\\n99 0.91 0.90 0.91 0.91 0.99 41  Hyper Kvasir ( Borgli et\\xa0al. 2020 0.99 0.91 0.91 0.91 0.91 0.99 41  DowPK 0.99 0.93 0.92 0.93 0.93 0.99 41 Furthermore, we conducted a composite analysis of various algorithmic steps, as detailed in Table 9 Borgli et al. 2020  Results analysis The application of various additions in the procedure significantly impacted the F1-score across different datasets. The enhancements introduced through these additions resulted in notable improvements in the overall F1-score. The specific effects on the F1-score varied depending on the dataset, indicating the effectiveness of the tailored approach for each dataset.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the enhancements introduced to improve the overall F1-score in visual object classes recognition. The section also discusses the impact of these enhancements on the F1-score, the effects on different datasets, and the rectification of images to their correct class through reflection removal and the use of different thresholds. The entities mentioned in this section include the F1-score, datasets, images, detectors, and classes.\\nexcerpt_keywords: enhancements, additions, improvements, F1-score, dataset, tailored approach, detailed analysis, impact, reflections, detectors, misleading class, thresholds, rectifying, correct detection, reflection removal, images\\nExcerpt:\\n-----\\nThe enhancements introduced through these additions resulted in notable improvements in the overall F1-score. The specific effects on the F1-score varied depending on the dataset, indicating the effectiveness of the tailored approach for each dataset. A detailed analysis of the impact on the F1-score, resulting from applying these additions, is presented in Table 9 The reflections on the images misguided the detectors for some images from the actual class to the misleading class. Removing the reflection and using the different thresholds for all the classes led to rectifying some of the images to their correct class. Some of the images resulted in correct detection by applying the reflection removal and different thresholds, shown in Figs.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the removal of reflections in images and the use of different thresholds for object class recognition. The section discusses how removing reflections and applying different thresholds helped rectify some images to their correct class. It also mentions that the reflection removal technique improved polyp detection and reduced misclassification of other classes as polyps. The entities mentioned in this section are the Kvasir V2 dataset and the authors Pogorelov et al.\\nexcerpt_keywords: reflection removal, thresholds, rectifying, images, correct class, detection, Kvasir V2, polyps, dyed classes, misclassification\\nExcerpt:\\n-----\\nRemoving the reflection and using the different thresholds for all the classes led to rectifying some of the images to their correct class. Some of the images resulted in correct detection by applying the reflection removal and different thresholds, shown in Figs. 7 8  10.7717/peerjcs.1685/fig-7 Figure 7  Some images of various classes from Kvasir V2 ( Pogorelov et al., 2018  10.7717/peerjcs.1685/fig-8 Figure 8  Some images of various classes from Kvasir V2 ( Pogorelov et al., 2018 There was a problem with the reflections on the image, which led various images to the polyps and dyed classes for some of the non-polyps images. The reflection removal technique improved the polyp detection. The misclassification of other classes to the polyp is reduced with the reflection removal.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics discussed in this section are the reflection removal technique for improving polyp detection, the issue of misclassification of other classes as polyps, the class imbalance problem in the datasets, the lower accuracy of minor classes compared to major classes, the presence of very few samples in some classes, and the difficulty in differentiating certain classes due to high similarity in their images. The key entities mentioned include polyps, ulcerative colitis, out-of-patient class, dyed lifted polyps, and dyed resection margins.\\nexcerpt_keywords: reflection removal, polyp detection, misclassification, class imbalance, data augmentation, accuracy, minor classes, major classes, ulcerative colitis, class imbalance, out-of-patient, Kvasir V2 dataset, class differentiation, image similarity, dyed lifted polyps, dyed resection margins\\nExcerpt:\\n-----\\nThe reflection removal technique improved the polyp detection. The misclassification of other classes to the polyp is reduced with the reflection removal. There was a class imbalance problem in the datasets that affected the results even after the data augmentation. The accuracy of the minor classes is relatively lower than that of the major classes in all the datasets. The polyp and ulcerative colitis have more images leading to higher accuracy with or without augmentation. There were some classes with very few samples. The class of out-of-patient had only four samples in the Kvasir V2 dataset ( Pogorelov et al., 2018 Some of the classes are difficult to differentiate due to the high similarity in the images of those classes. The images of the dyed lifted polyps are very similar to the dyed resection margins because of the same color in both these classes.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the similarity between dyed lifted polyps and dyed resection margins in terms of color, texture, and deep features. The section also mentions the detection accuracy for these classes in various benchmark datasets. Additionally, it highlights the presence of misleading images in both classes. The section briefly mentions another misclassification between esophagitis and normal z-line.\\nexcerpt_keywords: dyed lifted polyps, dyed resection margins, detection accuracy, benchmark datasets, misleading images, Fig. 9, Kvasir V2 dataset, misclassification, esophagitis, normal z-line\\nExcerpt:\\n-----\\nThe images of the dyed lifted polyps are very similar to the dyed resection margins because of the same color in both these classes. The images’ texture, color, and deep features show similar values for the dyed images of both types, the polyps or resection margins. The detection accuracy between both dyed classes is 0.80 to 0.85 for various benchmark datasets, which can be implemented with a higher focus on these classes. Some misleading images of both types are shown in Fig. 9  10.7717/peerjcs.1685/fig-9 Figure 9  Incorrectly detected images from the classes of dyed lifted polyps and dyed resection margins in the Kvasir V2 dataset ( Pogorelov et al., 2018 The other misclassification is found between esophagitis and normal z-line.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are esophagitis, the z-line, misclassification between esophagitis and the z-line, the Kvasir V2 dataset, and the LiRE-CNN approach for detecting GI tract abnormalities. The key entities mentioned are the esophagus, the z-line, the Kvasir V2 dataset, and the LiRE-CNN approach.\\nexcerpt_keywords: esophagitis, z-line, abnormality, esophagus, misclassification, landmark, images, Kvasir V2 dataset, detection, LiRE-CNN, state-of-the-art approaches, GI tract abnormalities\\nExcerpt:\\n-----\\nEsophagitis is an abnormality found in the esophagus, while the z-line is a landmark at the end of the esophagus. Most of the images of esophagitis have the z-line visible, leading to misclassification between these classes. A set of misclassified images from the Kvasir V2 dataset are shown in Fig. 10  10.7717/peerjcs.1685/fig-10 Figure 10  Incorrectly detected images from the classes of esophagitis and normal z-line in the Kvasir V2 dataset ( Pogorelov et al., 2018 The proposed best approach of the LiRE-CNN is compared with the state-of-the-art approaches for the detection of the GI tract abnormalities are shown in Table 11  10.7717/peerjcs.1685/table-11 Table 11  Comparison of the best approaches with LiRE-CNN.      Sr. No.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses various approaches and their performance in visual object classes recognition. The key topics include adaptive ensembles, combined neural networks, augmentation with neural networks, majority voting, hyper parameter optimized DenseNet 169, and LiRE-CNN. The section also mentions the accuracy, F1-score, and frames per second (FPS) achieved by each approach. The conclusion states that the study evaluated different methodologies for abnormality and landmark detection, and future work is mentioned as well.\\nexcerpt_keywords: Adaptive Ensembles, Combined Neural Networks, Augmentation with Neural Networks, Majority Voting, Hyper Parameter optimised DenseNet 169, LiRE-CNN, abnormality detection, landmark detection, methodology evaluation, future work\\nExcerpt:\\n-----\\nSr. No. Approach Accuracy F1-score FPS   1 Adaptive Ensembles ( Luo et\\xa0al., 2019 0.99 0.95 10  2 Combined Neural Networks ( Meng et\\xa0al., 2019 Jha et\\xa0al., 2021a 0.98 0.88 98  3 Augmentation with Neural Networks ( Hoang et\\xa0al., 2019 0.99 0.95 23  4 Majority Voting ( Khan & Tahir, 2018 Jha et\\xa0al., 2021a 0.98 0.76 43328  13 Hyper Parameter optimised DenseNet 169 ( García-Aguirre et\\xa0al., 2022 0.98 0.94 10  5 LiRE-CNN (Proposed) 0.99 0.93 41  Conclusion and Future Work In conclusion, this study comprehensively evaluated diverse methodologies for abnormality and landmark detection.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the preprocessing methods used in the research, specifically supervised and unsupervised methods for reflection elimination. The section also mentions the challenge of class imbalance and how it was effectively tackled. The section then discusses the importance of detecting landmarks and abnormalities for the treatment of diseases, such as polyps or cancer. The future task mentioned is to localize abnormalities and segment the affected region for treatment. The section also includes information about competing interests, author contributions, and data availability. The key entities mentioned in this section are the authors of the paper, Zeshan Khan and Muhammad Atif Tahir.\\nexcerpt_keywords: preprocessing, supervised, unsupervised, reflection elimination, dataset requirements, task objectives, class imbalance, abnormalities, landmarks, diseases, treatment, polyps, cancer, segmentation, localization, competing interests, author contributions, data availability\\nExcerpt:\\n-----\\nThe research delved into two key preprocessing categories: supervised and unsupervised, focusing on reflection elimination. Notably, the most favorable outcomes were attained through unsupervised methods, aligning with dataset requirements and task objectives. Additionally, the challenge of class imbalance was effectively tackled via The datasets in the study were only for detecting abnormalities and landmarks. Detecting landmarks and abnormalities can facilitate the treatment of diseases caused by such irregularities by pinpointing and localizing the specific ailment. Some diseases like polyps or cancer needs the segmentation of the affected region for treatment. The future task is to localize the abnormalities and then segment the abnormal region in case of polyps or cancer.   Additional Information and Declarations  Competing Interests  The authors declare there are no competing interests.  Author Contributions  Zeshan Khan  Muhammad Atif Tahir  Data Availability  The following information was supplied regarding data availability: The code is available at Zenodo: Khan, Zeshan. (2023).\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the title of the paper, the journal it was published in, the author contributions, and the availability of data. The title of the paper is \"Multi-scale color local binary patterns for visual object classes recognition.\" The paper was published in the journal PeerJ Computer Science. The author contributions are listed as Zeshan Khan and Muhammad Atif Tahir. The section also mentions the availability of data, with the code being available at Zenodo and the datasets used in the research being available at simula and KVASIR. The DowPK dataset is also mentioned as being available at Zenodo.\\nexcerpt_keywords: author contributions, data availability, code, datasets, real-time detection, endoscopic abnormalities, Zenodo, simula, KVASIR, DowPK\\nExcerpt:\\n-----\\nAuthor Contributions  Zeshan Khan  Muhammad Atif Tahir  Data Availability  The following information was supplied regarding data availability: The code is available at Zenodo: Khan, Zeshan. (2023). RealTime Detection of Endoscopic Abnormalities (V1.2). Zenodo. https://doi.org/10.5281/zenodo.7687070 The datasets used in the research are available at simula: https://datasets.simula.no/kvasir/ The dataset is described at KVASIR: A Multi-Class Image Dataset for Computer Aided Gastrointestinal Disease Detection: https://doi.org/10.1145/3193289 The DowPK dataset is also available at Zenodo: Zeshan Khan. (2023). DowPK: A dataset generated by the help of endoscopic department of DowPK.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The section also provides a URL to access the paper. Additionally, it mentions a dataset called DowPK, which was generated with the help of the endoscopic department of DowPK. The section includes references to other papers and studies related to deep learning, transfer learning, and medical image classification.\\nexcerpt_keywords: DowPK, dataset, endoscopic department, deep learning, rectified linear units, relu, transfer learning, medical images, polyp detection, color, position, texture features\\nExcerpt:\\n-----\\n(2023). DowPK: A dataset generated by the help of endoscopic department of DowPK. https://doi.org/10.5281/zenodo.8343995  References  Agarap (2018)   Agarap AF 2018 Deep learning using rectified linear units (relu) 1803.08375  Agrawal et\\xa0al. (2017)   Agrawal T Gupta R Sahu S Espy-Wilson CY 2017 SCL-UMD at the medico task-MediaEval 2017: transfer learning based classification of medical images MediaEval’17, 13-15 September 2017, Dublin, Ireland  Alexandre, Nobre & Casteleiro (2008)   Alexandre LA Nobre N Casteleiro J 2008 Color and position versus texture features for endoscopic polyp detection 2008 international conference on biomedical engineering and informatics,']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: ['it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The section also provides a URL to access the paper. Additionally, it mentions a dataset called DowPK, which was generated with the help of the endoscopic department of DowPK. The section includes references to other papers and studies related to deep learning, transfer learning, and medical image classification.\\nexcerpt_keywords: DowPK, dataset, endoscopic department, deep learning, rectified linear units, relu, transfer learning, medical images, polyp detection, color, position, texture features\\nExcerpt:\\n-----\\n(2023). DowPK: A dataset generated by the help of endoscopic department of DowPK. https://doi.org/10.5281/zenodo.8343995  References  Agarap (2018)   Agarap AF 2018 Deep learning using rectified linear units (relu) 1803.08375  Agrawal et\\xa0al. (2017)   Agrawal T Gupta R Sahu S Espy-Wilson CY 2017 SCL-UMD at the medico task-MediaEval 2017: transfer learning based classification of medical images MediaEval’17, 13-15 September 2017, Dublin, Ireland  Alexandre, Nobre & Casteleiro (2008)   Alexandre LA Nobre N Casteleiro J 2008 Color and position versus texture features for endoscopic polyp detection 2008 international conference on biomedical engineering and informatics, vol.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of the section are multi-scale color local binary patterns and visual object classes recognition. The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" that was published in the journal PeerJ Computer Science. The section also mentions the authors of the paper, Bay, Tuytelaars, and Gool, and provides a citation to another paper by Bay, Tuytelaars, and Gool titled \"SURF: speeded up robust features\" that was published in the journal Computer Vision - ECCV 2006.\\nexcerpt_keywords: Piscataway, IEEE, Bay, Tuytelaars, Gool, Leonardis, Bischof, Pinz, SURF, computer vision\\nExcerpt:\\n-----\\n2 Piscataway IEEE 38 42  Bay, Tuytelaars & Gool (2006)   Bay H Tuytelaars T Gool LV 2006  Leonardis A Bischof H Pinz A SURF: speeded up robust features Computer vision – ECCV 2006. ECCV 2006.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of the section are multi-scale color local binary patterns and visual object classes recognition. The section discusses the use of these patterns for recognizing different object classes in visual data. The section also mentions the impact of image preprocessing methods on polyp localization in colonoscopy frames. The entities mentioned in the section include the authors of the paper, the journal it was published in, and references to other related works.\\nexcerpt_keywords: ECCV 2006, Lecture notes in computer science, image preprocessing methods, polyp localization, colonoscopy frames, IEEE engineering in medicine and biology society, Navier-stokes, fluid dynamics, image inpainting, video inpainting, computer vision, pattern recognition\\nExcerpt:\\n-----\\nECCV 2006. Lecture notes in computer science, vol 3951 Berlin, Heidelberg Springer 404 417 10.1007/11744023_32  Bernal, Sánchez & Vilarino (2013)   Bernal J Sánchez J Vilarino F 2013 Impact of image preprocessing methods on polyp localization in colonoscopy frames 2013 35th annual international conference of the IEEE engineering in medicine and biology society (EMBC) Piscataway IEEE 7350 7354  Bertalmio, Bertozzi & Sapiro (2001)   Bertalmio M Bertozzi AL Sapiro G 2001 Navier-stokes, fluid dynamics, and image and video inpainting Proceedings of the 2001 IEEE computer society conference on computer vision and pattern recognition. CVPR 2001, vol. 1 Piscataway IEEE I I  Borgli et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" that was published in the journal PeerJ Computer Science. The paper proposes a method for recognizing visual object classes using multi-scale color local binary patterns. The authors of the paper are Borgli et al. The section also mentions another paper titled \"HyperKvasir, a comprehensive multi-class image and video dataset for gastrointestinal endoscopy\" published in Scientific Data, authored by Borgli H, Thambawita V, Smedsrud PH, Hicks S, Jha D, Eskeland SL, Randel KR, Pogorelov K, Lux M, Nguyen DTD, Johansen D, Griwodz C, Stensland HK, Garcia-Ceja E, Schmidt PT, Hammer HL, Riegler MA, Halvorsen P, and de Lange T. The section provides the URL to access the paper.\\nexcerpt_keywords: CVPR 2001, vol. 1, Piscataway, IEEE, Borgli, et al., 2020, Borgli, Thambawita, Smedsrud, Hicks, Jha, Eskeland, Randel, Pogorelov, Lux, Nguyen, Johansen, Griwodz, Stensland, Garcia-Ceja, Schmidt, Hammer, Riegler, Halvorsen, de Lange, HyperKvasir, comprehensive, multi-class, image, video dataset, gastrointestinal endoscopy, Scientific Data, 10.1038/s41597-019-0340-y, 31896794, Chang\\nExcerpt:\\n-----\\nCVPR 2001, vol. 1 Piscataway IEEE I I  Borgli et\\xa0al. (2020)  Borgli H Thambawita V Smedsrud PH Hicks S Jha D Eskeland SL Randel KR Pogorelov K Lux M Nguyen DTD Johansen D Griwodz C Stensland HK Garcia-Ceja E Schmidt PT Hammer HL Riegler MA Halvorsen P de\\xa0Lange T 2020 HyperKvasir, a comprehensive multi-class image and video dataset for gastrointestinal endoscopy Scientific Data 7 1 10.1038/s41597-019-0340-y 31896794  Chang et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on a method called multi-scale color local binary patterns for recognizing visual object classes. The section also mentions another paper titled \"Gastrointestinal tract diseases detection with deep attention neural network\" published in the Proceedings of the 27th ACM international conference on multimedia. Additionally, it references a paper titled \"CEDD: color and edge directivity descriptor: a compact descriptor for image indexing and retrieval\" published in the Lecture notes in computer science.\\nexcerpt_keywords: Gastrointestinal tract diseases, deep attention neural network, image indexing, image retrieval, compact descriptor, color and edge directivity descriptor, computer vision systems, ACM international conference on multimedia, Proceedings, detection\\nExcerpt:\\n-----\\n(2019)   Chang Y Huang Z Chen W Shen Q 2019 Gastrointestinal tract diseases detection with deep attention neural network Proceedings of the 27th ACM international conference on multimedia New York ACM 2568 2572  Chatzichristofis & Boutalis (2008a)   Chatzichristofis SA Boutalis YS 2008a  Gasteratos A Vincze M Tsotsos JK CEDD: color and edge directivity descriptor: a compact descriptor for image indexing and retrieval Computer vision systems. ICVS 2008. Lecture notes in computer science, vol 5008 Berlin, Heidelberg Springer 312 322 10.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of the section are multi-scale color local binary patterns and visual object classes recognition. The section discusses the use of multi-scale color local binary patterns as a method for recognizing visual object classes. The section also mentions the publication of the paper in the PeerJ Computer Science journal and provides a URL to access the full article. Additionally, the section includes references to other related papers, such as FCTH: fuzzy color and texture histogram and Region filling and object removal by exemplar-based image inpainting.\\nexcerpt_keywords: ICVS, Lecture notes in computer science, vol 5008, Berlin, Heidelberg, Springer, Chatzichristofis, Boutalis, FCTH, fuzzy color, texture histogram, low level feature, accurate image retrieval, ninth international workshop, image analysis, multimedia interactive services, Criminisi, Pérez, Toyama, region filling, object removal, exemplar-based image inpainting, IEEE Transactions on Image Processing\\nExcerpt:\\n-----\\nICVS 2008. Lecture notes in computer science, vol 5008 Berlin, Heidelberg Springer 312 322 10.1007/978-3-540-79547-6_30  Chatzichristofis & Boutalis (2008b)   Chatzichristofis SA Boutalis YS 2008b FCTH: fuzzy color and texture histogram-a low level feature for accurate image retrieval 2008 ninth international workshop on image analysis for multimedia interactive services Piscataway IEEE 191 196  Criminisi, Pérez & Toyama (2004)  Criminisi A Pérez P Toyama K 2004 Region filling and object removal by exemplar-based image inpainting IEEE Transactions on Image Processing 13 9 1200 1212 10.1109/TIP.2004.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on a method called multi-scale color local binary patterns for recognizing visual object classes. The section also mentions other related papers such as \"Region filling and object removal by exemplar-based image inpainting\" by Pérez & Toyama (2004) and \"Histograms of oriented gradients for human detection\" by Dalal & Triggs (2005). Additionally, it mentions a bleeding detection algorithm for endoscopic video by Deeba et al. (2018).\\nexcerpt_keywords: image inpainting, region filling, object removal, exemplar-based, image processing, histograms of oriented gradients, human detection, bleeding detection algorithm, endoscopic video, classifier fusion method, exhaustive feature selection\\nExcerpt:\\n-----\\nPérez & Toyama (2004)  Criminisi A Pérez P Toyama K 2004 Region filling and object removal by exemplar-based image inpainting IEEE Transactions on Image Processing 13 9 1200 1212 10.1109/TIP.2004.833105 15449582  Dalal & Triggs (2005)   Dalal N Triggs B 2005 Histograms of oriented gradients for human detection 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR’05), vol. 1 Piscataway IEEE 886 893  Deeba et\\xa0al. (2018)   Deeba F Islam M Bui FM Wahid KA 2018 Performance assessment of a bleeding detection algorithm for endoscopic video based on classifier fusion method and exhaustive feature selection Biomedical Signal Processing and Control 40 415 424 10.1016/j.bspc.2017.10.011  Deng et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The section also provides references to other related papers and conferences, such as the ImageNet database, transfer learning with CNN architectures for classifying gastrointestinal diseases and anatomical landmarks, computer-aided diagnosis in medical imaging, and incorporating Nesterov momentum into Adam optimization algorithm.\\nexcerpt_keywords: Imagenet, hierarchical image database, computer vision, pattern recognition, transfer learning, CNN architectures, gastrointestinal diseases, anatomical landmarks, computer-aided diagnosis, medical imaging, historical review, current status, future potential, nesterov momentum, adam, ICLR 2016 workshop submission.\\nExcerpt:\\n-----\\n(2009)   Deng J Dong W Socher R Li L-J Li K Fei-Fei L 2009 Imagenet: a large-scale hierarchical image database 2009 IEEE conference on computer vision and pattern recognition Piscataway IEEE 248 255  Dias & Dias (2018)   Dias D Dias U 2018 Transfer learning with CNN architectures for classifying gastrointestinal diseases and anatomical landmarks MediaEval’18, 29-31 October 2018, Sophia Antipolis, France  Doi (2007)  Doi K 2007 Computer-aided diagnosis in medical imaging: historical review, current status and future potential Computerized Medical Imaging and Graphics 31 4–5 198 211 10.1016/j.compmedimag.2007.02.002 17349778  Dozat (2016)   Dozat T 2016 Incorporating nesterov momentum into adam ICLR 2016 workshop submission  Dutta,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" that was published in the journal PeerJ Computer Science. The paper focuses on a method called multi-scale color local binary patterns for recognizing visual object classes. The section also provides a URL to access the full paper. Additionally, it mentions several other papers and authors, including Dozat (2016), Dutta, Bhattacharjee & Barbhuiya (2021), and Esgiar et al.\\nexcerpt_keywords: compmedimag, 17349778, Dozat, nesterov momentum, adam, ICLR 2016 workshop submission, Dutta, Bhattacharjee, Barbhuiya, efficient detection, lesions, endoscopy, pattern recognition, Cham Springer\\nExcerpt:\\n-----\\n1016/j.compmedimag.2007.02.002 17349778  Dozat (2016)   Dozat T 2016 Incorporating nesterov momentum into adam ICLR 2016 workshop submission  Dutta, Bhattacharjee & Barbhuiya (2021)   Dutta A Bhattacharjee RK Barbhuiya FA 2021 Efficient detection of lesions during endoscopy International conference on pattern recognition Cham Springer 315 322  Esgiar et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are multi-scale color local binary patterns for visual object classes recognition and the publications and authors related to this topic. The entities mentioned include the authors Bhattacharjee, Barbhuiya, Dutta, Esgiar, Naguib, Sharif, Bennett, Murray, Ethiraj, Bolla, Faigel, Cave, Galdran, Carneiro, and Ballester.\\nexcerpt_keywords: lesions, endoscopy, detection, microscopic image analysis, colonic mucosa, convolution neural networks, augmentations, capsule']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: ['and authors, including Dozat (2016), Dutta, Bhattacharjee & Barbhuiya (2021), and Esgiar et al.\\nexcerpt_keywords: compmedimag, 17349778, Dozat, nesterov momentum, adam, ICLR 2016 workshop submission, Dutta, Bhattacharjee, Barbhuiya, efficient detection, lesions, endoscopy, pattern recognition, Cham Springer\\nExcerpt:\\n-----\\n1016/j.compmedimag.2007.02.002 17349778  Dozat (2016)   Dozat T 2016 Incorporating nesterov momentum into adam ICLR 2016 workshop submission  Dutta, Bhattacharjee & Barbhuiya (2021)   Dutta A Bhattacharjee RK Barbhuiya FA 2021 Efficient detection of lesions during endoscopy International conference on pattern recognition Cham Springer 315 322  Esgiar et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are multi-scale color local binary patterns for visual object classes recognition and the publications and authors related to this topic. The entities mentioned include the authors Bhattacharjee, Barbhuiya, Dutta, Esgiar, Naguib, Sharif, Bennett, Murray, Ethiraj, Bolla, Faigel, Cave, Galdran, Carneiro, and Ballester.\\nexcerpt_keywords: lesions, endoscopy, detection, microscopic image analysis, colonic mucosa, convolution neural networks, augmentations, capsule endoscopy, gastrointestinal image analysis\\nExcerpt:\\n-----\\nBhattacharjee & Barbhuiya (2021)   Dutta A Bhattacharjee RK Barbhuiya FA 2021 Efficient detection of lesions during endoscopy International conference on pattern recognition Cham Springer 315 322  Esgiar et\\xa0al. (1998)  Esgiar AN Naguib RN Sharif BS Bennett MK Murray A 1998 Microscopic image analysis for quantitative measurement and feature identification of normal and cancerous colonic mucosa IEEE Transactions on Information Technology in Biomedicine 2 3 197 203 10.1109/4233.735785 10719530  Ethiraj & Bolla (2022)   Ethiraj S Bolla BK 2022 Augmentations: an insight into their effectiveness on convolution neural networks 2205.04064  Faigel & Cave (2008)   Faigel DO Cave DR 2008 Capsule endoscopy Philadelphia Saunders Elsevier  Galdran, Carneiro & Ballester (2021)   Galdran A Carneiro G Ballester MAG 2021 A hierarchical multi-task approach to gastrointestinal image analysis International conference on pattern recognition Cham Springer 275 282  García-Aguirre et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" that was published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The section also provides a URL to access the full paper. Additionally, it mentions another paper titled \"Automatic generation of optimized convolutional neural networks for medical image classification using a genetic algorithm\" by García-Aguirre, Torres-Treviño, Navarro-López, and González-González, which is available on SSRN.\\nexcerpt_keywords: genetic algorithm, convolutional neural networks, medical image classification, automatic generation, optimized, SSRN, Goodfellow, García-Aguirre, Torres-Treviño, Navarro-López\\nExcerpt:\\n-----\\n(2022)   García-Aguirre R Torres-Treviño L Navarro-López EM González-González JA 2022 Automatic generation of optimized convolutional neural networks for medical image classification using a genetic algorithm Available at SSRN 10.2139/ssrn.4167905  Goodfellow et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of the section are multi-scale color local binary patterns and visual object classes recognition. The section also mentions the publication details of the paper, including the title, journal, and URL. The entities mentioned in the section include the authors of the paper (Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, Bengio), the journal (PeerJ Computer Science), and other references (Guilford, Haralick, Shanmugam, Harzig, Einfalt, Lienhart).\\nexcerpt_keywords: Generative adversarial nets, neural information processing, psychometric methods, textural features, image classification, automatic disease detection, report generation, gastrointestinal tract examination, multimedia, ACM conference\\nExcerpt:\\n-----\\n(2014)   Goodfellow I Pouget-Abadie J Mirza M Xu B Warde-Farley D Ozair S Courville A Bengio Y 2014 Generative adversarial nets Advances in neural information processing systems 27 (NIPS 2014)  Guilford (1954)   Guilford JP 1954 Psychometric methods New York McGraw-Hill  Haralick & Shanmugam (1973)   Haralick RM Shanmugam K 1973 Textural features for image classification IEEE Transactions on Systems, Man, and Cybernetics (6) 610 621  Harzig, Einfalt & Lienhart (2019)   Harzig P Einfalt M Lienhart R 2019 Automatic disease detection and report generation for gastrointestinal tract examination Proceedings of the 27th ACM international conference on multimedia New York ACM 2573 2577  He et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The authors propose a method that combines color and texture information at multiple scales to improve object recognition accuracy. The section also mentions other related papers and authors, such as He et al. (2016) and Hearst et al. (1998), which are referenced in the paper.\\nexcerpt_keywords: Deep residual learning, image recognition, computer vision, pattern recognition, hybrid loss, network trimming, disease recognition, gastrointestinal endoscopy, support vector machines, intelligent systems\\nExcerpt:\\n-----\\n(2016)   He K Zhang X Ren S Sun J 2016 Deep residual learning for image recognition Proceedings of the IEEE conference on computer vision and pattern recognition Piscataway 770 778  He et\\xa0al. (2021)   He Q Bano S Stoyanov D Zuo S 2021 Hybrid loss with network trimming for disease recognition in gastrointestinal endoscopy International conference on pattern recognition Cham Springer 299 306  Hearst et\\xa0al. (1998)   Hearst MA Dumais ST Osuna E Platt J Scholkopf B 1998 Support vector machines IEEE Intelligent Systems and their Applications 13 4 18 28  Hicks et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of the section are multi-scale color local binary patterns and visual object classes recognition. The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" that was published in the journal PeerJ Computer Science. The section also provides a URL to access the paper. Additionally, the section mentions other papers and their authors that are related to the topic of the section.\\nexcerpt_keywords: support vector machines, deep learning, disease detection, transfer learning, domain specific, residual network, faster-RCNN, medico, multimedia task, MediaEval\\nExcerpt:\\n-----\\n(1998)   Hearst MA Dumais ST Osuna E Platt J Scholkopf B 1998 Support vector machines IEEE Intelligent Systems and their Applications 13 4 18 28  Hicks et\\xa0al. (2018)   Hicks SA Smedsrud PH Halvorsen P Riegler M 2018 Deep learning based disease detection using domain specific transfer learning MediaEval’18, 29-31 October 2018, Sophia Antipolis, France  Hoang et\\xa0al. (2018)   Hoang T-H Nguyen H-D Nguyen T-A Nguyen V-T Tran M-T 2018 An application of residual network and faster-RCNN for medico: multimedia task at MediaEval 2018 MediaEval’18, 29-31 October 2018, Sophia Antipolis, France  Hoang et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The section also mentions other related papers such as \"Enhancing endoscopic image classification with symptom localization and data augmentation\" by Hoang et al. and \"Mobilenets: efficient convolutional neural networks for mobile vision applications\" by Howard et al. It also briefly mentions the paper \"Densely connected convolutional networks\" by Huang et al.\\nexcerpt_keywords: endoscopic image classification, symptom localization, data augmentation, convolutional neural networks, mobile vision applications, Mobilenets, Densely connected convolutional networks, computer vision, pattern recognition, ACM international conference on multimedia\\nExcerpt:\\n-----\\n(2019)   Hoang T-H Nguyen H-D Nguyen V-A Nguyen T-A Nguyen V-T Tran M-T 2019 Enhancing endoscopic image classification with symptom localization and data augmentation Proceedings of the 27th ACM international conference on multimedia New York ACM 2578 2582  Howard et\\xa0al. (2017)   Howard AG Zhu M Chen B Kalenichenko D Wang W Weyand T Andreetto M Adam H 2017 Mobilenets: efficient convolutional neural networks for mobile vision applications 1704.04861  Huang et\\xa0al. (2017)   Huang G Liu Z Van Der\\xa0Maaten L Weinberger KQ 2017 Densely connected convolutional networks Proceedings of the IEEE conference on computer vision and pattern recognition Piscataway IEEE 4700 4708  Huang et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are multi-scale color local binary patterns (LBP) and visual object classes recognition. The section also mentions the publication of a paper titled \"Densely connected convolutional networks\" in the IEEE conference on computer vision and pattern recognition in 2017. Additionally, it references a paper from 1997 on image indexing using color correlograms and another paper from 2007 on polyp detection in colonoscopy videos using elliptical shape features. The section provides a URL to access the full paper.\\nexcerpt_keywords: Densely connected convolutional networks, computer vision, pattern recognition, image indexing, color correlograms, polyp detection, colonoscopy video, elliptical shape feature, image processing, IEEE conference\\nExcerpt:\\n-----\\n(2017)   Huang G Liu Z Van Der\\xa0Maaten L Weinberger KQ 2017 Densely connected convolutional networks Proceedings of the IEEE conference on computer vision and pattern recognition Piscataway IEEE 4700 4708  Huang et\\xa0al. (1997)   Huang J Kumar S Mitra M Zhu W-J Zabih R 1997 Image indexing using color correlograms CVPR, vol. 97 762  Hwang et\\xa0al. (2007)   Hwang S Oh J Tavanapong W Wong J De\\xa0Groen PC 2007 Polyp detection in colonoscopy video using elliptical shape feature 2007 IEEE international conference on image processing, vol.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on a method for recognizing visual object classes using multi-scale color local binary patterns. The section also mentions other papers related to object recognition, such as \"Polyp detection in colonoscopy video using elliptical shape feature\" and \"An intelligent system for automatic detection of gastrointestinal adenomas in video endoscopy.\"\\nexcerpt_keywords: polyp detection, colonoscopy video, elliptical shape feature, gastrointestinal adenomas, video endoscopy, intelligent system, automatic detection, image processing, IEEE international conference, Computers in Biology and Medicine\\nExcerpt:\\n-----\\n97 762  Hwang et\\xa0al. (2007)   Hwang S Oh J Tavanapong W Wong J De\\xa0Groen PC 2007 Polyp detection in colonoscopy video using elliptical shape feature 2007 IEEE international conference on image processing, vol. 2 Piscataway IEEE II 465  Iakovidis, Maroulis & Karkanis (2006)  Iakovidis DK Maroulis DE Karkanis SA 2006 An intelligent system for automatic detection of gastrointestinal adenomas in video endoscopy Computers in Biology and Medicine 36 10 1084 1103 10.1016/j.compbiomed.2005.09.008 16293240  Jha et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The authors of the paper are Jha D, Ali S, Hicks S, Thambawita V, Borgli H, Smedsrud PH, de Lange T, Pogorelov K, Wang X, Harzig P, Tran M-T, Meng W, Hoang T-H, Dias D, Ko TH, Agrawal T, Ostroukhova O, Khan Z, Tahir MA, Liu Y, Chang Y, Kirkerød M, Johansen D, Lux M, Johansen HD, Riegler MA, and Halvorsen P. The paper was published in 2021 in the journal PeerJ Computer Science. The URL to access the paper is provided as well.\\nexcerpt_keywords: classification methods, gastrointestinal endoscopy imaging, medical image analysis, comprehensive analysis, Jha et al., endoscopy, imaging, machine learning, deep learning, computer-aided diagnosis\\nExcerpt:\\n-----\\n(2021a)  Jha D Ali S Hicks S Thambawita V Borgli H Smedsrud PH de\\xa0Lange T Pogorelov K Wang X Harzig P Tran M-T Meng W Hoang T-H Dias D Ko TH Agrawal T Ostroukhova O Khan Z Tahir MA Liu Y Chang Y Kirkerød M Johansen D Lux M Johansen HD Riegler MA Halvorsen P 2021a A comprehensive analysis of classification methods in gastrointestinal endoscopy imaging Medical Image Analysis 70 102007 10.1016/j.media.2021.102007 33740740  Jha et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on a method called multi-scale color local binary patterns for recognizing visual object classes. The section also mentions other papers such as \"NanoNet: real-time polyp segmentation in video capsule endoscopy and colonoscopy\" by Jha et al. and']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: ['M, Johansen D, Lux M, Johansen HD, Riegler MA, and Halvorsen P. The paper was published in 2021 in the journal PeerJ Computer Science. The URL to access the paper is provided as well.\\nexcerpt_keywords: classification methods, gastrointestinal endoscopy imaging, medical image analysis, comprehensive analysis, Jha et al., endoscopy, imaging, machine learning, deep learning, computer-aided diagnosis\\nExcerpt:\\n-----\\n(2021a)  Jha D Ali S Hicks S Thambawita V Borgli H Smedsrud PH de\\xa0Lange T Pogorelov K Wang X Harzig P Tran M-T Meng W Hoang T-H Dias D Ko TH Agrawal T Ostroukhova O Khan Z Tahir MA Liu Y Chang Y Kirkerød M Johansen D Lux M Johansen HD Riegler MA Halvorsen P 2021a A comprehensive analysis of classification methods in gastrointestinal endoscopy imaging Medical Image Analysis 70 102007 10.1016/j.media.2021.102007 33740740  Jha et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on a method called multi-scale color local binary patterns for recognizing visual object classes. The section also mentions other papers such as \"NanoNet: real-time polyp segmentation in video capsule endoscopy and colonoscopy\" by Jha et al. and \"Evaluating the performance of state-of-the-art methods and classifying Covid-19 infected tissues\" by Kamruzzaman et al. These papers discuss topics related to polyp segmentation in medical imaging and the classification of Covid-19 infected tissues.\\nexcerpt_keywords: polyp segmentation, video capsule endoscopy, colonoscopy, real-time, NanoNet, state-of-the-art methods, Covid-19 infected tissues, performance evaluation, classification, convergence technology\\nExcerpt:\\n-----\\n(2021b)   Jha D Tomar NK Ali S Riegler MA Johansen HD Johansen D de\\xa0Lange T Halvorsen P 2021b NanoNet: real-time polyp segmentation in video capsule endoscopy and colonoscopy 2104.11138  Kamruzzaman et\\xa0al. (2022)   Kamruzzaman M Moinuddin M Liton AI Azad MM Hossain MA Rahman W 2022 Evaluating the performance of state-of-the-art methods and classifying Covid-19 infected tissues 2022 IEEE 7th international conference for convergence in technology (I2CT) Piscataway IEEE 1 6  Karkanis et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" that was published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The section also provides a URL to access the full paper. Additionally, it mentions another paper titled \"Detection of lesions in endoscopic video using textural descriptors on wavelet domain supported by artificial neural network architectures\" that was published in 2001. The paper discusses the use of textural descriptors and artificial neural network architectures for detecting lesions in endoscopic videos. The section includes the authors\\' names and the conference in which the paper was presented. Lastly, it mentions a paper by Khan titled \"DowPK\" that was published in 2023 and provides a URL to access it.\\nexcerpt_keywords: endoscopic video, lesions, textural descriptors, wavelet domain, artificial neural network architectures, image processing, detection, international conference, Piscataway, DowPK\\nExcerpt:\\n-----\\n(2001)   Karkanis SA Iakovidis DK Karras D Maroulis D 2001 Detection of lesions in endoscopic video using textural descriptors on wavelet domain supported by artificial neural network architectures Proceedings 2001 international conference on image processing (Cat. No. 01CH37205), vol. 2 Piscataway IEEE 833 836  Khan (2023)   Khan Z 2023 DowPK Zenodo https://zenodo.org/records/8343995  Khan et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The section also provides a URL to access the full paper. Additionally, it mentions several other references and publications by different authors related to medical diagnostics and image detection.\\nexcerpt_keywords: Piscataway, IEEE, Khan, DowPK, Zenodo, medical diagnostic, data bagging, neural network, pattern recognition, majority voting, heterogeneous classifiers, abnormalities, gastro-intestinal tract, preprocessing, medical image detection.\\nExcerpt:\\n-----\\nNo. 01CH37205), vol. 2 Piscataway IEEE 833 836  Khan (2023)   Khan Z 2023 DowPK Zenodo https://zenodo.org/records/8343995  Khan et\\xa0al. (2021)   Khan Z Alvi MUT Tahir MA Memon S 2021 Medical diagnostic by data bagging for various instances of neural network International conference on pattern recognition Cham Springer 291 298  Khan & Tahir (2018)   Khan Z Tahir MA 2018 Majority voting of heterogeneous classifiers for finding abnormalities in the gastro-intestinal tract MediaEval’18, 29-31 October 2018, Sophia Antipolis, France  Kirkerød et\\xa0al. (2018)   Kirkerød M Thambawita V Riegler M Halvorsen P 2018 Using preprocessing as a tool in medical image detection MediaEval’18, 29-31 October 2018,\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the key topics and entities related to the paper titled \"Multi-scale color local binary patterns for visual object classes recognition.\" The paper was published in the journal PeerJ Computer Science. The section also provides a URL to access the full article. Additionally, it mentions other relevant papers and conferences related to medical image detection and classification, as well as texture classification using dominant local binary patterns.\\nexcerpt_keywords: preprocessing, medical image detection, imbalanced medical data classification, discriminant subspace learning, deep convolutional neural networks, texture classification, local binary patterns, weighted discriminant embedding, imbalanced data, image classification\\nExcerpt:\\n-----\\n(2018)   Kirkerød M Thambawita V Riegler M Halvorsen P 2018 Using preprocessing as a tool in medical image detection MediaEval’18, 29-31 October 2018, Sophia Antipolis, France  Ko, Gu & Liu (2018)   Ko TH Gu Z Liu Y 2018 Weighted discriminant embedding: discriminant subspace learning for imbalanced medical data classification MediaEval  Krizhevsky, Sutskever & Hinton (2012)   Krizhevsky A Sutskever I Hinton GE 2012 Imagenet classification with deep convolutional neural networks Advances in neural information processing systems 1097 1105  Liao, Law & Chung (2009)  Liao S Law MW Chung AC 2009 Dominant local binary patterns for texture classification IEEE Transactions on Image Processing 18 5 1107 1118 10.1109/TIP.2009.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are multi-scale color local binary patterns and visual object classes recognition. The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The section also provides a URL to access the full paper. Additionally, the section mentions several other papers and their authors that are related to the topic of texture classification and medical image classification.\\nexcerpt_keywords: texture classification, local binary patterns, dominant local binary patterns, medical image classification, spatial adjacent histogram, adaptive local binary patterns, medical multimedia task, HKBU, MediaEval, Medico\\nExcerpt:\\n-----\\nLaw & Chung (2009)  Liao S Law MW Chung AC 2009 Dominant local binary patterns for texture classification IEEE Transactions on Image Processing 18 5 1107 1118 10.1109/TIP.2009.2015682 19342342  Liu et\\xa0al. (2016)  Liu D Wang S Huang D Deng G Zeng F Chen H 2016 Medical image classification using spatial adjacent histogram based on adaptive local binary patterns Computers in Biology and Medicine 72 185 200 10.1016/j.compbiomed.2016.03.010 27058283  Liu, Gu & Cheung (2017)   Liu Y Gu Z Cheung WK 2017 HKBU at MediaEval 2017-Medico: medical multimedia task MediaEval’17, 13-15 September 2017, Dublin, Ireland  Luo et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The section also includes references to other papers and conferences related to image retrieval and bioinformatics.\\nexcerpt_keywords: Adaptive ensemble, biomedia, ACM MM grandchallenge, image retrieval, Java, Lire, Lucene, CBIR library, secondary structure, T4 phage lysozyme\\nExcerpt:\\n-----\\n(2019)   Luo Z Wang X Xu Z Li X Li J 2019 Adaptive ensemble: solution to the biomedia ACM MM grandchallenge 2019 Proceedings of the 27th ACM international conference on multimedia New York ACM 2583 2587  Lux (2013)   Lux M 2013 Lire: open source image retrieval in java Proceedings of the 21st ACM international conference on multimedia New York ACM 843 846  Lux & Chatzichristofis (2008)   Lux M Chatzichristofis SA 2008 Lire: lucene image retrieval: an extensible java cbir library Proceedings of the 16th ACM international conference on multimedia New York ACM 1085 1088  Matthews (1975)   Matthews BW 1975 Comparison of the predicted and observed secondary structure of T4 phage lysozyme Biochimica Et Biophysica Acta (BBA)—Protein Structure 405 2 442 451 10.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of this section are the title of the paper, \"Multi-scale color local binary patterns for visual object classes recognition,\" and the journal it was published in, \"PeerJ Computer Science.\" The section also includes a URL to the article. Additionally, there are several references mentioned, including McClish (1989), Mehrotra, Namuduri & Ranganathan (1992), and Meng et al. (2019).\\nexcerpt_keywords: ROC curve, Medical Decision Making, Gabor filter, edge detection, Pattern Recognition, data enhancement, sample unbalance, ACM MM grand challenge, multimedia\\nExcerpt:\\n-----\\n1016/0005-2795(75)90109-9  McClish (1989)  McClish DK 1989 Analyzing a portion of the ROC curve Medical Decision Making 9 3 190 195 10.1177/0272989X8900900307 2668680  Mehrotra, Namuduri & Ranganathan (1992)   Mehrotra R Namuduri KR Ranganathan N 1992 Gabor filter-based edge detection Pattern Recognition 25 12 1479 1494 10.1016/0031-3203(92)90121-X  Meng et\\xa0al. (2019)   Meng W Zhang S Yao X Yang X Xu C Huang X 2019 Biomedia ACM MM grand challenge 2019: using data enhancement to solve sample unbalance Proceedings of the 27th ACM international conference on multimedia New York ACM 2588 2592  Naqvi et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the key topics and entities related to the paper titled \"Multi-scale color local binary patterns for visual object classes recognition.\" The paper was published in the journal PeerJ Computer Science. The section also provides additional references to related studies and conferences.\\nexcerpt_keywords: Ensemble, texture features, abnormalities, gastro-intestinal tract, comparative study, classification, featured distributions, CNN architecture, GI disease, anatomical landmark classification\\nExcerpt:\\n-----\\n(2017)   Naqvi SSA Nadeem S Zaid M Tahir MA 2017 Ensemble of texture features for finding abnormalities in the gastro-intestinal tract MediaEval’17, 13-15 September 2017, Dublin, Ireland  Ojala, Pietikäinen & Harwood (1996)   Ojala T Pietikäinen M Harwood D 1996 A comparative study of texture measures with classification based on featured distributions Pattern Recognition 29 1 51 59 10.1016/0031-3203(95)00067-4  Petscharnig, Schöffmann & Lux (2017)   Petscharnig S Schöffmann K Lux M 2017 An inception-like CNN architecture for GI disease and anatomical landmark classification MediaEval’17, 13-15 September 2017, Dublin, Ireland  Pogorelov et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The authors of the paper are Pogorelov K, Eskeland SL, de Lange T, Griwodz C, Randel KR, Stensland HK, Dang-Nguyen D-T, Spampinato C, Johansen D, Riegler M, and Halvorsen P. The section also mentions two other papers by Pogorelov et al. titled \"A holistic multimedia system for gastrointestinal tract disease detection\" and \"Nerthus: a bowel preparation quality video dataset.\"\\nexcerpt_keywords: holistic multimedia system, gastrointestinal tract disease detection, bowel preparation quality video dataset, ACM, multimedia systems conference, Pogorelov, Eskeland, de Lange, Griwodz, Randel, Stensland, Dang-Nguyen, Spampinato, Johansen, Riegler, Halvorsen, Taschwer, Lux, Schmidt PT\\nExcerpt:\\n-----\\n(2017a)   Pogorelov K Eskeland SL de\\xa0Lange T Griwodz C Randel KR Stensland HK Dang-Nguyen D-T Spampinato C Johansen D Riegler M Halvorsen P 2017a A holistic multimedia system for gastrointestinal tract disease detection Proceedings of the 8th ACM on multimedia systems conference New York ACM 112 123  Pogorelov et\\xa0al. (2017b)   Pogorelov K Randel KR de\\xa0Lange T Eskeland SL Griwodz C Johansen D Spampinato C Taschwer M Lux M Schmidt PT Riegler M Halvorsen P 2017b Nerthus: a bowel preparation quality video dataset Proceedings of the 8th ACM on multimedia systems conference New York ACM 170 174  Pogorelov et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color']\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.SUMMARY: 'summary'>} template_vars=['context_str'] kwargs={'query_str': 'You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.'} output_parser=None template_var_mappings=None function_mappings=None template='You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\\n  {context_str}\\n'\n",
      "text_chunks: ['SL, de Lange T, Griwodz C, Randel KR, Stensland HK, Dang-Nguyen D-T, Spampinato C, Johansen D, Riegler M, and Halvorsen P. The section also mentions two other papers by Pogorelov et al. titled \"A holistic multimedia system for gastrointestinal tract disease detection\" and \"Nerthus: a bowel preparation quality video dataset.\"\\nexcerpt_keywords: holistic multimedia system, gastrointestinal tract disease detection, bowel preparation quality video dataset, ACM, multimedia systems conference, Pogorelov, Eskeland, de Lange, Griwodz, Randel, Stensland, Dang-Nguyen, Spampinato, Johansen, Riegler, Halvorsen, Taschwer, Lux, Schmidt PT\\nExcerpt:\\n-----\\n(2017a)   Pogorelov K Eskeland SL de\\xa0Lange T Griwodz C Randel KR Stensland HK Dang-Nguyen D-T Spampinato C Johansen D Riegler M Halvorsen P 2017a A holistic multimedia system for gastrointestinal tract disease detection Proceedings of the 8th ACM on multimedia systems conference New York ACM 112 123  Pogorelov et\\xa0al. (2017b)   Pogorelov K Randel KR de\\xa0Lange T Eskeland SL Griwodz C Johansen D Spampinato C Taschwer M Lux M Schmidt PT Riegler M Halvorsen P 2017b Nerthus: a bowel preparation quality video dataset Proceedings of the 8th ACM on multimedia systems conference New York ACM 170 174  Pogorelov et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The authors of the paper are Pogorelov K, Randel KR, Griwodz C, Eskeland SL, de Lange T, Johansen D, Spampinato C, Dang-Nguyen D-T, Lux M, Schmidt PT, Riegler M, and Halvorsen P. The section also mentions another paper by the same authors titled \"Kvasir: a multi-class image dataset for computer aided gastrointestinal disease detection\" which was published in the Proceedings of the 8th ACM on multimedia systems conference. The authors compare deep learning with global features for gastrointestinal disease detection in this paper.\\nexcerpt_keywords: Pogorelov, Randel, Griwodz, Eskeland, de Lange, Johansen, Spampinato, Dang-Nguyen, Lux, Schmidt, Riegler, Halvorsen, Kvasir, multi-class image dataset, computer aided gastrointestinal disease detection, ACM, multimedia systems conference, deep learning, global features, MediaEval\\'17, Dublin, Ireland\\nExcerpt:\\n-----\\n(2017c)   Pogorelov K Randel KR Griwodz C Eskeland SL de\\xa0Lange T Johansen D Spampinato C Dang-Nguyen D-T Lux M Schmidt PT Riegler M Halvorsen P 2017c Kvasir: a multi-class image dataset for computer aided gastrointestinal disease detection Proceedings of the 8th ACM on multimedia systems conference New York ACM 164 169  Pogorelov et\\xa0al. (2017d)   Pogorelov K Riegler M Halvorsen P Griwodz C de\\xa0Lange T Randel K Eskeland S Dang-Nguyen D-T Ostroukhova O Lux M 2017d A comparison of deep learning with global features for gastrointestinal disease detection MediaEval’17, 13-15 September 2017, Dublin, Ireland  Pogorelov et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The key topics of the section are multi-scale color local binary patterns and visual object classes recognition. The section discusses the use of multi-scale color local binary patterns for recognizing visual object classes. It also mentions the journal in which the paper was published, PeerJ Computer Science, and provides a URL to access the full article. The section also includes references to other related works and workshops in the field of multimedia and medicine.\\nexcerpt_keywords: Pogorelov K, Riegler M, Halvorsen P, Hicks S, Randel KR, Dang Nguyen DT, Lux M, Ostroukhova O, de Lange T, multimedia, Medico, task, MediaEval, workshop, proceedings, vol. 2283, Griwodz C, Eskeland S, Spampinato C, medicine, Dublin, Ireland, Sandler\\nExcerpt:\\n-----\\n(2018)   Pogorelov K Riegler M Halvorsen P Hicks S Randel KR Dang\\xa0Nguyen DT Lux M Ostroukhova O de\\xa0Lange T 2018 Medico multimedia task at mediaeval 2018 CEUR workshop proceedings, vol. 2283  Riegler et\\xa0al. (2017)   Riegler M Pogorelov K Halvorsen P Griwodz C Lange T Randel K Eskeland S Dang-Nguyen D-T Lux M Spampinato C 2017 Multimedia for medicine: the medico task at MediaEval 2017 MediaEval’17, 13-15 September 2017, Dublin, Ireland  Sandler et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper explores the use of multi-scale color local binary patterns for recognizing visual object classes. The section also provides references to other related papers, including \"Mobilenetv2: inverted residuals and linear bottlenecks\" by Sandler et al. (2018) and \"Research on data augmentation for image classification based on convolution neural networks\" by Shijie et al. (2017). Additionally, it mentions a survey on image data augmentation for deep learning conducted by Shorten and Khoshgoftaar (2019).\\nexcerpt_keywords: MobileNetV2, inverted residuals, linear bottlenecks, image classification, convolutional neural networks, data augmentation, Chinese automation congress, deep learning, image data augmentation, survey\\nExcerpt:\\n-----\\n(2018)   Sandler M Howard A Zhu M Zhmoginov A Chen L-C 2018 Mobilenetv2: inverted residuals and linear bottlenecks Proceedings of the IEEE conference on computer vision and pattern recognition Piscataway IEEE 4510 4520  Shijie et\\xa0al. (2017)   Shijie J Ping W Peiyi J Siping H 2017 Research on data augmentation for image classification based on convolution neural networks 2017 Chinese automation congress (CAC) Piscataway IEEE 4165 4170  Shorten & Khoshgoftaar (2019)   Shorten C Khoshgoftaar TM 2019 A survey on image data augmentation for deep learning Journal of Big Data 6 1 1 48 10.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" that was published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The section also provides a URL to access the full paper. Additionally, it mentions two other references, Sikora (2001) and Simonyan & Zisserman (2014), which are likely related to the topic of visual object recognition.\\nexcerpt_keywords: MPEG-7, visual standard, content description, overview, IEEE Transactions, Circuits and Systems, Video Technology, deep convolutional networks, large-scale image recognition\\nExcerpt:\\n-----\\n1186/s40537-018-0162-3  Sikora (2001)   Sikora T 2001 The MPEG-7 visual standard for content description—an overview IEEE Transactions on Circuits and Systems for Video Technology 11 6 696 702 10.1109/76.927422  Simonyan & Zisserman (2014)   Simonyan K Zisserman A 2014 Very deep convolutional networks for large-scale image recognition 1409.1556  Sung et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The section also mentions other related papers and their authors, such as Simonyan & Zisserman (2014) and Sung et al. (2021), which discuss deep convolutional networks for image recognition and global cancer statistics, respectively. Additionally, Suzuki (2012) is mentioned for their review of computer-aided diagnosis in thoracic and colonic imaging.\\nexcerpt_keywords: deep convolutional networks, large-scale image recognition, global cancer statistics, GLOBOCAN, incidence, mortality, computer-aided diagnosis, thoracic imaging, colonic imaging, quantitative imaging\\nExcerpt:\\n-----\\n1109/76.927422  Simonyan & Zisserman (2014)   Simonyan K Zisserman A 2014 Very deep convolutional networks for large-scale image recognition 1409.1556  Sung et\\xa0al. (2021)  Sung H Ferlay J Siegel RL Laversanne M Soerjomataram I Jemal A Bray F 2021 Global cancer statistics 2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries CA: A Cancer Journal for Clinicians 71 3 209 249 33538338  Suzuki (2012)  Suzuki K 2012 A review of computer-aided diagnosis in thoracic and colonic imaging Quantitative Imaging in Medicine and Surgery 2 3 163 23256078  Szegedy et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper explores the use of multi-scale color local binary patterns for recognizing visual object classes. The section also provides references to other related papers, including \"Going deeper with convolutions\" by Szegedy et al. (2015) and \"Rethinking the inception architecture for computer vision\" by Szegedy et al. (2016).\\nexcerpt_keywords: convolutions, computer vision, deep learning, inception architecture, image classification, neural networks, pattern recognition, object detection, image recognition, deep convolutional neural networks\\nExcerpt:\\n-----\\n(2015)   Szegedy C Liu W Jia Y Sermanet P Reed S Anguelov D Erhan D Vanhoucke V Rabinovich A 2015 Going deeper with convolutions Proceedings of the IEEE conference on computer vision and pattern recognition Piscataway IEEE 1 9  Szegedy et\\xa0al. (2016)   Szegedy C Vanhoucke V Ioffe S Shlens J Wojna Z 2016 Rethinking the inception architecture for computer vision Proceedings of the IEEE conference on computer vision and pattern recognition Piscataway 2818 2826  Tajbakhsh et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The section also mentions other papers and their authors, such as Tajbakhsh, Chi, Gurudu, Liang, Tammina, Kumar, Senatore, Gunjan, Tamura, Mori, and Yamawaki, which are relevant to the topic of object recognition and image analysis.\\nexcerpt_keywords: polyp detection, automatic, global geometric constraints, local intensity variation patterns, biomedical imaging, chest x-ray images, deep transfer learning models, CovidSORT, novel covid-19, textural features, visual perception\\nExcerpt:\\n-----\\n(2014)   Tajbakhsh N Chi C Gurudu SR Liang J 2014 Automatic polyp detection using global geometric constraints and local intensity variation patterns 2014 IEEE 11th international symposium on biomedical imaging (ISBI) Piscataway IEEE 97 100  Tammina (2022)   Tammina S 2022  Kumar A Senatore S Gunjan VK CovidSORT: detection of novel covid-19 in chest x-ray images by leveraging deep transfer learning models ICDSMLA 2020. Lecture Notes in Electrical Engineering, vol 783 Singapore Springer 10.1007/978-981-16-3690-5_37  Tamura, Mori & Yamawaki (1978)   Tamura H Mori S Yamawaki T 1978 Textural features corresponding to visual perception IEEE Transactions on Systems, Man, and Cybernetics 8 6 460 473 10.1109/TSMC.1978.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The section also mentions other related papers by Mori & Yamawaki (1978), Tan & Triggs (2010), and Telea (2004) that are relevant to the topic.\\nexcerpt_keywords: textural features, visual perception, systems, man, cybernetics, face recognition, difficult lighting conditions, image inpainting, fast marching method, graphics tools\\nExcerpt:\\n-----\\nMori & Yamawaki (1978)   Tamura H Mori S Yamawaki T 1978 Textural features corresponding to visual perception IEEE Transactions on Systems, Man, and Cybernetics 8 6 460 473 10.1109/TSMC.1978.4309999  Tan & Triggs (2010)  Tan X Triggs W 2010 Enhanced local texture feature sets for face recognition under difficult lighting conditions IEEE Transactions on Image Processing 19 6 1635 1650 10.1109/TIP.2010.2042645 20172829  Telea (2004)   Telea A 2004 An image inpainting technique based on the fast marching method Journal of Graphics Tools 9 1 23 34 10.1080/10867651.2004.10487596  Tomar et\\xa0al.\\n-----\\n\\n[Excerpt from document]\\nTitle of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\nsection_summary: The section discusses the paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The section also includes references to other related papers such as \"DDANet: dual decoder attention network for automatic polyp segmentation\" by Tomar et al., \"Visualizing and understanding convolutional networks\" by Zeiler and Fergus, and \"Automatic detection and classification of colorectal polyps by transferring low-level CNN features from nonmedical domain\" by Zhang et al.\\nexcerpt_keywords: polyp segmentation, DDANet, dual decoder attention network, automatic detection, colorectal polyps, CNN features, nonmedical domain, visualizing convolutional networks, understanding convolutional networks, low-level features\\nExcerpt:\\n-----\\n1080/10867651.2004.10487596  Tomar et\\xa0al. (2021)   Tomar NK Jha D Ali']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m response_summary_cacc \u001b[38;5;241m=\u001b[39m \u001b[43mlist_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_query_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mResponseMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOMPACT_ACCUMULATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_qa_template\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msummary_prompt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/core/base_query_engine.py:30\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     29\u001b[0m     str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/query_engine/retriever_query_engine.py:171\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    168\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    169\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m    170\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m--> 171\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_synthesizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/base.py:146\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    144\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    145\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 146\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetadataMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    155\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/compact_and_accumulate.py:50\u001b[0m, in \u001b[0;36mCompactAndAccumulate.get_response\u001b[0;34m(self, query_str, text_chunks, separator, **response_kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_set_attrs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mprompt_helper):\n\u001b[1;32m     46\u001b[0m     new_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mprompt_helper\u001b[38;5;241m.\u001b[39mrepack(\n\u001b[1;32m     47\u001b[0m         text_qa_template, text_chunks\n\u001b[1;32m     48\u001b[0m     )\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseparator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/accumulate.py:90\u001b[0m, in \u001b[0;36mAccumulate.get_response\u001b[0;34m(self, query_str, text_chunks, separator, **response_kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to stream in Accumulate response mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_give_responses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_async\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext_chunks\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     97\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_list(tasks)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/accumulate.py:91\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to stream in Accumulate response mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_give_responses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_async\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks\n\u001b[1;32m     95\u001b[0m ]\n\u001b[1;32m     97\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_list(tasks)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/accumulate.py:129\u001b[0m, in \u001b[0;36mAccumulate._give_responses\u001b[0;34m(self, query_str, text_chunk, use_async, **response_kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mapredict\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m use_async\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mpredict\n\u001b[1;32m    127\u001b[0m     )\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_qa_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_text_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcur_text_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext_chunks\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mastructured_predict\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m use_async\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mstructured_predict\n\u001b[1;32m    142\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/accumulate.py:130\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mapredict\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m use_async\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mpredict\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 130\u001b[0m         \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_qa_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_text_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m cur_text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks\n\u001b[1;32m    136\u001b[0m     ]\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mastructured_predict\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m use_async\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mstructured_predict\n\u001b[1;32m    142\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/llms/llm.py:220\u001b[0m, in \u001b[0;36mLLM.predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mis_chat_model:\n\u001b[1;32m    219\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_messages(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[0;32m--> 220\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     output \u001b[38;5;241m=\u001b[39m chat_response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/llms/base.py:97\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self) \u001b[38;5;28;01mas\u001b[39;00m callback_manager:\n\u001b[1;32m     89\u001b[0m     event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m     90\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m     91\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m         },\n\u001b[1;32m     96\u001b[0m     )\n\u001b[0;32m---> 97\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/llms/openai.py:234\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     chat_fn \u001b[38;5;241m=\u001b[39m completion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_complete)\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchat_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/llms/openai.py:289\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client()\n\u001b[1;32m    288\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[0;32m--> 289\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m openai_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    295\u001b[0m message \u001b[38;5;241m=\u001b[39m from_openai_message(openai_message)\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:643\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    641\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    642\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/openai/_base_client.py:1112\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1100\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1108\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1109\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1110\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1111\u001b[0m     )\n\u001b[0;32m-> 1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/openai/_base_client.py:859\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    852\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/openai/_base_client.py:887\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    884\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 887\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    893\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpx/_client.py:901\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    893\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    899\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 901\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpx/_client.py:929\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    926\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 929\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpx/_client.py:966\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    964\u001b[0m     hook(request)\n\u001b[0;32m--> 966\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpx/_client.py:1002\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1002\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1006\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:228\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    216\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    217\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    226\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 228\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    233\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    234\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    235\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    236\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    237\u001b[0m )\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/ssl.py:1263\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1261\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/ssl.py:1136\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# VERY SLOW !!! DON\"T RUN!!!!\n",
    "\n",
    "query = \"\"\"You are an expert biologist. Summarize the context provided into the content of a Wikipedia page. First, write a brief overview section, as you would see in a normal wikipedia article. Then organize the document into section and write a summary for each.\"\"\"\n",
    "\n",
    "response_summary_cacc = list_index.as_query_engine(response_mode=ResponseMode.COMPACT_ACCUMULATE, text_qa_template = summary_prompt).query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving object response_summaryPubMedNodes to pickle file ./response_summaryPubMedNodes.pkl\n"
     ]
    }
   ],
   "source": [
    "pickleSave(response_summary, 'response_summaryPubMedNodes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving object response_summary_cacc_PubMedNodes to pickle file ./response_summary_cacc_PubMedNodes.pkl\n"
     ]
    }
   ],
   "source": [
    "pickleSave(response_summary_cacc , 'response_summary_cacc_PubMedNodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Response 1: gastritis, medical specialists, detection accuracy, detection algorithms, classification algorithms, localization algorithms, segmentation algorithms, endoscopic images, and endoscopic videos.\n",
       "excerpt_keywords: endoscopy, colonoscopy, GI tract, abnormalities, polyps, lesions, esophagitis, gastritis, medical specialists, detection accuracy, detection algorithms, classification algorithms, localization algorithms, segmentation algorithms, endoscopic images, endoscopic videos\n",
       "Excerpt:\n",
       "-----\n",
       "The primary objective of endoscopy or colonoscopy is to detect the functionality of the Gastro-Intestinal (GI) tract. During these procedures, various abnormalities can be detected, such as polyps, lesions, esophagitis, and gastritis. However, there is often disagreement among medical specialists regarding the detection accuracy of these abnormalities. Several studies have been conducted on the detection, classification, localization, and segmentation of diseases in endoscopy using different algorithms and techniques. These studies primarily focus on analyzing endoscopic images and videos to improve the accuracy of detection and diagnosis.\n",
       "---------------------\n",
       "Response 2: and Borgli, as well as the Kvasir dataset.\n",
       "excerpt_keywords: multi-scale color local binary patterns, visual object classes recognition, abnormalities, benchmark datasets, Kvasir dataset, polyps, lesions, GI tract, Alexandre, Nobre, Casteleiro, Pogorelov, Agrawal, Szegedy, Jha, Hoang, Khan, Tahir, Borgli\n",
       "---------------------\n",
       "Response 3: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\n",
       "section_summary: This section focuses on the importance of removing reflections in image analysis for accurate object recognition. It discusses how reflections can impact the detection of abnormalities and anatomical landmarks and provides various procedures for removing reflections. The section also mentions the sample set of polyps picture in Figure 1 and references the authors Pogorelov et al., as well as the works of Criminisi, Pérez, and Toyama.\n",
       "---------------------\n",
       "Response 4: another research paper. The section also includes a reference to Figure 3.\n",
       "excerpt_keywords: Multi-scale color local binary patterns, visual object classes recognition, texture features, Lucene Image Retrieval, LIRE, Lux & Chatzichristofis, Ojala, Pietikäinen & Harwood, Haralick & Shanmugam, Fig. 3\n",
       "Excerpt:\n",
       "-----\n",
       "2008 Ojala, Pietikäinen & Harwood, 1996 Haralick & Shanmugam, 1973 Fig. 3 ( Lux & Chatzichristofis, 2008 The texture features are available in Lucene Image Retrieval (LIRE) ( Lux & Chatzichristofis, 2008 Ojala, Pietikäinen & Harwood, 1996 Haralick & Shanmugam, 1973\n",
       "---------------------\n",
       "Response 5: == Overview ==\n",
       "\n",
       "This Wikipedia page discusses the concept of multi-scale color local binary patterns for visual object classes recognition. It provides information on various methods and features used in this context, including texture features, color layout, edge histogram, Tamura, color and edge directivity descriptor (CEDD), fuzzy color and texture histogram (FCTH), color histograms (HSV and RGB), Gabor features, auto color correlation, auto color correlogram, Speeded up robust features (SURF), Pyramid Histogram of Oriented Gradients (PHOG), and Local Binary Patterns (LBP). The page also mentions the researchers and authors who have contributed to the development of these methods and features, such as Lux & Chatzichristofis, Ojala, Pietikäinen & Harwood, Haralick & Shanmugam, Sikora, Tamura, Chatzichristofis, Boutalis, Huang, Mori & Yamawaki, Mehrotra, Namuduri & Ranganathan, Bay, Tuytelaars, Gool, Dalal, Triggs, Liu, Liao, Law, Chung, Zhu, Bichot, and Chen. The page provides references to research papers and articles for further reading on these\n",
       "---------------------\n",
       "Response 6: The section discusses the use of multi-scale color local binary patterns for visual object classes recognition. It mentions the publication title \"Multi-scale color local binary patterns for visual object classes recognition\" and the journal it was published in, \"PeerJ Computer Science\". The section also provides a URL to access the full article. Additionally, it mentions the use of Haralick features for statistics of GLCM (Gray-Level Co-occurrence Matrix) and deep feature extraction using CNN-based approaches for detecting abnormalities in endoscopy and colonoscopy images.\n",
       "---------------------\n",
       "Response 7: Multi-scale color local binary patterns for visual object classes recognition\n",
       "Journal it was published in: PeerJ Computer Science\n",
       "URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\n",
       "\n",
       "Overview:\n",
       "This Wikipedia page provides a summary of the paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper introduces an approach for recognizing visual object classes using multi-scale color local binary patterns. The page discusses the key topics covered in the paper, including the use of neural networks, optimization algorithms, and threshold detection. It also mentions the performance metrics and accuracy achieved by the approach.\n",
       "\n",
       "Sections:\n",
       "\n",
       "1. Introduction\n",
       "This section provides an overview of the paper and introduces the concept of multi-scale color local binary patterns for visual object classes recognition. It highlights the importance of accurate recognition in various applications and mentions the key topics covered in the paper.\n",
       "\n",
       "2. Neural Network Architecture\n",
       "The section discusses the neural network architecture used in the approach. It describes the different layers and activation functions employed in the network. It also mentions the configuration of fully connected neurons and their role in the recognition process.\n",
       "\n",
       "3. Optimization Algorithms\n",
       "This section focuses on the evaluation of various optimization algorithms for the neural network. It highlights the Nadam optimizer, which showed good\n",
       "---------------------\n",
       "Response 8: paper: Multi-scale color local binary patterns for visual object classes recognition\n",
       "Journal it was published in:: PeerJ Computer Science\n",
       "URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\n",
       "\n",
       "Overview:\n",
       "This Wikipedia page provides a summary of the paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in PeerJ Computer Science. The paper discusses the use of multi-scale color local binary patterns for recognizing visual object classes. It explores various topics such as the differences between different object classes, the application of a neural network architecture for detection, the use of a genetic algorithm for threshold detection, and the evaluation of the proposed methodology using different datasets. The page also includes information about the experimental setup, results of recognition experiments, and evaluation measures used in the research.\n",
       "\n",
       "Sections:\n",
       "\n",
       "1. Differences between Visual Object Classes and Neural Network Architecture\n",
       "This section discusses the differences between different visual object classes in terms of detection probabilities. It also explains the use of a neural network architecture for detection and the application of a genetic algorithm (GA-Boost) to learn thresholds for each detected class.\n",
       "\n",
       "2. Crossover Operator and Experimental Setup\n",
       "This section describes the crossover operator employed in GA-Boost, which involves an addition with modulus operation. It also provides information about the experimental setup used for the\n",
       "---------------------\n",
       "Response 9: Image Crop and Unsupervised Detection were evaluated on multiple datasets, including the Kvasir V1 dataset. The impact of reflection removal on the abnormalities detection F1-score is presented in Table 7. The section highlights the importance of reflection removal in improving the accuracy of abnormalities detection. The entities mentioned in this section are the authors of the paper (Pogorelov et al., Borgli et al.) and the datasets used in the evaluation.\n",
       "---------------------\n",
       "Response 10: Multi-scale color local binary patterns for visual object classes recognition\n",
       "Journal it was published in: PeerJ Computer Science\n",
       "URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\n",
       "\n",
       "Overview:\n",
       "This Wikipedia page discusses the research paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in PeerJ Computer Science. The paper focuses on the impact of different steps and techniques in the proposed approach for visual object classes recognition. It explores the use of reflection removal techniques, dataset performance, data augmentation, feature selection, and various methods for visual object recognition. The paper concludes by highlighting the improvements in detection accuracy achieved through these approaches.\n",
       "\n",
       "Sections:\n",
       "\n",
       "1. Results and Discussion:\n",
       "This section presents the results of various steps in terms of improvements in detection accuracy and time. It discusses the impact of different steps in the proposed approach for visual object classes recognition, with a specific focus on the reflection removal step. The section evaluates two common approaches, Image Crop and Unsupervised Detection, on multiple datasets. The impact of reflection removal on the abnormalities detection F1-score is presented in Table 7.\n",
       "\n",
       "2. Impact of Reflection Removal on Datasets:\n",
       "This section discusses the performance of different datasets in the context of visual object classes recognition. It compares the F1-score of different datasets with and without reflection removal using Telea-based\n",
       "---------------------\n",
       "Response 11: of the tailored approach for each dataset. The entities mentioned include the authors of the paper (Borgli et al.) and the DowPK dataset.\n",
       "excerpt_keywords: enhancements, F1-score, dataset, tailored approach, rectification, Borgli et al. 2020, DowPK\n",
       "---------------------\n",
       "Response 12: Summary:\n",
       "\n",
       "The Wikipedia page titled \"Multi-scale color local binary patterns for visual object classes recognition\" provides an overview of a research paper published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The page is organized into several sections, each summarizing a different aspect of the research.\n",
       "\n",
       "1. Overview:\n",
       "This section provides a brief overview of the research paper, including the title, journal, and authors. It also mentions the availability of data and code related to the research.\n",
       "\n",
       "2. Enhancements for Improved F1-score:\n",
       "This section discusses the enhancements introduced to improve the overall F1-score in visual object classes recognition. It highlights the impact of these enhancements on different datasets and the effectiveness of a tailored approach for each dataset.\n",
       "\n",
       "3. Reflection Removal and Thresholds:\n",
       "The section focuses on the removal of reflections in images and the use of different thresholds for object class recognition. It explains how these techniques helped rectify some images to their correct class and improved polyp detection while reducing misclassification of other classes.\n",
       "\n",
       "4. Class Imbalance and Image Similarity:\n",
       "This section addresses the class imbalance problem in the datasets and the lower accuracy of minor classes compared to major classes. It also mentions the difficulty in differentiating certain classes due to high similarity in their images\n",
       "---------------------\n",
       "Response 13: Excerpt:\n",
       "-----\n",
       "Bhattacharjee, Barbhuiya & Dutta (2021)   Dutta A Bhattacharjee RK Barbhuiya FA 2021 Efficient detection of lesions during endoscopy International conference on pattern recognition Cham Springer 315 322  Esgiar et al. (2021)   Esgiar AN Naguib RNG Sharif BS Bennett MK Murray A Ethiraj C Bolla S Faigel DO Cave DR Galdran A Carneiro F Ballester MA 2021 Convolutional neural networks for colonic mucosa microscopic image analysis with augmented capsule International conference on pattern recognition Cham Springer 323 330  Figueiredo et al.\n",
       "-----\n",
       "---------------------\n",
       "Response 14: \"Deep learning for computer-aided diagnosis in gastrointestinal endoscopy: a systematic review\" by Jha et al. These papers are related to the topic of the section, which is the use of machine learning and deep learning techniques for medical image analysis in gastrointestinal endoscopy. The section provides a URL to access the full paper and mentions the authors of the paper (Jha, Ali, Hicks, Thambawita, Borgli, Smedsrud, de Lange, Pogorelov, Wang, Harzig, Tran, Meng, Hoang, Dias, Ko, Agrawal, Ostroukhova, Khan, Tahir, Liu, Chang, Kirkerød, Johansen, Lux, Johansen, Riegler, and Halvorsen).\n",
       "---------------------\n",
       "Response 15: local binary patterns for visual object classes recognition\n",
       "Journal it was published in:: PeerJ Computer Science\n",
       "URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\n",
       "section_summary: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The authors of the paper are Pogorelov K, Eskeland SL, de Lange T, Griwodz C, Randel KR, Stensland HK, Dang-Nguyen D-T, Spampinato C, Johansen D, Riegler M, and Halvorsen P. The section also mentions two other papers by Pogorelov et al. titled \"A holistic multimedia system for gastrointestinal tract disease detection\" and \"Nerthus: a bowel preparation quality video dataset.\" These papers discuss topics related to gastrointestinal tract disease detection, bowel preparation quality video dataset, and the use of multimedia systems in medical diagnostics. The section also includes the authors' names and the conference in which the papers were presented.\n",
       "---------------------\n",
       "Response 16: The section discusses a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper focuses on the use of multi-scale color local binary patterns for recognizing visual object classes. The section also mentions other related papers, including \"DDANet: dual decoder attention network for automatic polyp segmentation\" by Tomar et al., \"Visualizing and understanding convolutional networks\" by Zeiler and Fergus, and \"Automatic detection and classification of colorectal polyps by transferring low-level CNN features from nonmedical domain\" by Zhang et al. These papers explore topics such as polyp segmentation, automatic detection of colorectal polyps, and understanding convolutional networks.\n",
       "---------------------\n",
       "Response 17: [Excerpt from document]\n",
       "Title of this paper: Multi-scale color local binary patterns for visual object classes recognition\n",
       "Journal it was published in:: PeerJ Computer Science\n",
       "URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\n",
       "section_summary: The section provides information about a paper titled \"Multi-scale color local binary patterns for visual object classes recognition\" published in the journal PeerJ Computer Science. The paper discusses the fusion of selected deep CNN and handcrafted features for gastritis detection from wireless capsule endoscopy images. The section also mentions another paper by Zhu, Bichot, and Chen published in 2010, which introduces the concept of multi-scale color local binary patterns for visual object classes recognition. Additionally, the section highlights the 2021 International Congress on Image and Signal Processing, Biomedical Engineering and Informatics (CISP-BMEI) where the fusion of selected deep CNN and handcrafted features for gastritis detection was presented.\n",
       "excerpt_keywords: gastritis detection, wireless capsule endoscopy, deep CNN, handcrafted features, fusion, image processing, biomedical engineering, informatics, color local binary patterns, visual object classes recognition"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f'{response_summary_cacc}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response_synthesizers import Accumulate\n",
    "from llama_index.retrievers import ListIndexRetriever\n",
    "from llama_index.indices import SummaryIndex\n",
    "from llama_index import PromptTemplate\n",
    "\n",
    "map_prompt_template = \"\"\"Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\n",
    "{context_str}\n",
    "\n",
    "Return your answer in the following format:\n",
    "Title | Summary...\n",
    "e.g. \n",
    "Why Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\n",
    "\n",
    "TITLE AND CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "query = \"\"\"Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\n",
    "\n",
    "Return your answer in the following format:\n",
    "Title | Summary...\n",
    "e.g. \n",
    "Why Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\n",
    "\n",
    "TITLE AND CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template)\n",
    "\n",
    "list_index = SummaryIndex(nodes=nodes)\n",
    "\n",
    "nodes_with_scores = ListIndexRetriever(index=list_index).retrieve(query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices import TreeIndex\n",
    "from llama_index import PromptTemplate\n",
    "\n",
    "\n",
    "# Create a TreeIndex from the documents\n",
    "index = TreeIndex.from_documents([documents[0], documents[1], documents[2]])\n",
    "\n",
    "\n",
    "\n",
    "# # To save the index for future use\n",
    "# index.storage_context.persist()\n",
    "\n",
    "# # To load the index from saved state\n",
    "# storage_context = StorageContext.from_defaults(persist_dir='./storage')  # Adjust the directory as needed\n",
    "# index = load_index_from_storage(storage_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the index into a query engine\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "bacteriophage_name = \"Phi X 174 Phage\"\n",
    "\n",
    "query_tpl = \"\"\"\n",
    "You are an expert microbiologist.\n",
    "Write in a scholarly tone and always cite references for your statements.\n",
    "\n",
    "Write a comprehensive introduction for the bacteriophage {bacteriophage_name}. \n",
    "Include details on its discovery, historical background, and significance in the realm of antibiotic-resistant bacterial treatments. \n",
    "Cite relevant academic journals and scientific studies to support the information.\n",
    "\"\"\"\n",
    "\n",
    "query = query_tpl.format(bacteriophage_name=bacteriophage_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "The bacteriophage Phi X 174 Phage, also known as ΦX174, is a well-studied and widely recognized bacteriophage that has played a significant role in the field of microbiology. Discovered in 1962 by F. W. Sanger and his colleagues, this phage has since become a model organism for studying viral replication, genetics, and evolution.\n",
      "\n",
      "Phi X 174 Phage belongs to the family Microviridae and is characterized by its small size and single-stranded DNA genome. Its genome consists of approximately 5,386 nucleotides and encodes for 11 proteins that are essential for its replication and assembly. The phage's life cycle involves the attachment of its tail fibers to specific receptors on the surface of its host bacterium, followed by the injection of its genetic material into the host cell. Once inside, the phage hijacks the host's cellular machinery to produce more phage particles, eventually leading to the lysis of the host cell and the release of progeny phages.\n",
      "\n",
      "One of the most significant aspects of Phi X 174 Phage is its potential in combating antibiotic-resistant bacteria. With the rise of antibiotic resistance posing a global threat to public health, alternative strategies for treating bacterial infections are urgently needed. Bacteriophages, including Phi X 174 Phage, offer a promising avenue for addressing this challenge. By specifically targeting and infecting bacteria, bacteriophages can effectively kill bacterial pathogens without harming the beneficial bacteria in the host's microbiota. This targeted approach reduces the selective pressure for antibiotic resistance development and provides a potential solution to the growing problem of antibiotic resistance.\n",
      "\n",
      "In conclusion, Phi X 174 Phage is a well-studied bacteriophage that has contributed significantly to our understanding of viral replication, genetics, and evolution. Its small size, single-stranded DNA genome, and ability to combat antibiotic-resistant bacteria make it a valuable tool in the fight against infectious diseases. Further research and exploration of Phi X 174 Phage and other bacteriophages hold great promise for the development of novel therapeutic strategies to combat antibiotic resistance.\n",
      "\n",
      "References:\n",
      "1. Sanger, F. W., Air, G. M., Barrell, B. G., Brown, N. L., Coulson, A. R., Fiddes, J. C., ... & Waterston, R. (1977). Nucleotide sequence of bacteriophage ΦX174 DNA. Nature, 265(5596), 687-695.\n",
      "2. Suttle, C. A. (2005). Viruses in the sea. Nature, 437(7057), 356-361.\n",
      "3. Chanishvili, N. (2012). A Literature Review of the Practical Application of Bacteriophage Research. Nova Science Publishers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query the engine\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The bacteriophage Phi X 174 Phage, also known as ΦX174, is a well-studied and widely recognized bacteriophage that has played a significant role in the field of microbiology. Discovered in 1962 by F. W. Sanger and his colleagues, this phage has since become a model organism for studying viral replication, genetics, and evolution.\n",
       "\n",
       "Phi X 174 Phage belongs to the family Microviridae and is characterized by its small size and single-stranded DNA genome. Its genome consists of approximately 5,386 nucleotides and encodes for 11 proteins that are essential for its replication and assembly. The phage's life cycle involves the attachment of its tail fibers to specific receptors on the surface of its host bacterium, followed by the injection of its genetic material into the host cell. Once inside, the phage hijacks the host's cellular machinery to produce more phage particles, eventually leading to the lysis of the host cell and the release of progeny phages.\n",
       "\n",
       "One of the most significant aspects of Phi X 174 Phage is its potential in combating antibiotic-resistant bacteria. With the rise of antibiotic resistance posing a global threat to public health, alternative strategies for treating bacterial infections are urgently needed. Bacteriophages, including Phi X 174 Phage, offer a promising avenue for addressing this challenge. By specifically targeting and infecting bacteria, bacteriophages can effectively kill bacterial pathogens without harming the beneficial bacteria in the host's microbiota. This targeted approach reduces the selective pressure for antibiotic resistance development and provides a potential solution to the growing problem of antibiotic resistance.\n",
       "\n",
       "In conclusion, Phi X 174 Phage is a well-studied bacteriophage that has contributed significantly to our understanding of viral replication, genetics, and evolution. Its small size, single-stranded DNA genome, and ability to combat antibiotic-resistant bacteria make it a valuable tool in the fight against infectious diseases. Further research and exploration of Phi X 174 Phage and other bacteriophages hold great promise for the development of novel therapeutic strategies to combat antibiotic resistance.\n",
       "\n",
       "References:\n",
       "1. Sanger, F. W., Air, G. M., Barrell, B. G., Brown, N. L., Coulson, A. R., Fiddes, J. C., ... & Waterston, R. (1977). Nucleotide sequence of bacteriophage ΦX174 DNA. Nature, 265(5596), 687-695.\n",
       "2. Suttle, C. A. (2005). Viruses in the sea. Nature, 437(7057), 356-361.\n",
       "3. Chanishvili, N. (2012). A Literature Review of the Practical Application of Bacteriophage Research. Nova Science Publishers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f'{response.response}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'> Source (Doc id: 6f84a526-987e-48c6-b48b-8cdfcf45b516): Article\\nTargeted suppression of human IBD-associated gut\\nmicrobiota commensals by phage consortia...'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get_formatted_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Article\n",
       "Targeted suppression of human IBD-associated gut\n",
       "microbiota commensals by phage consortia fortreatment of intestinal inﬂammation\n",
       "Graphical abstract\n",
       "Highlights\n",
       "dKlebsiella pneumoniae (Kp) strains are associated with IBD\n",
       "severity across geography\n",
       "dIsolated Kp strains induce gut inﬂammation upon\n",
       "colonization in animal IBD models\n",
       "dA Kp-targeting ﬁve-phage combination suppressesintestinal inﬂammation in IBD models\n",
       "dPhages consumed by healthy humans are safe and viableand accumulate in the lower gutAuthors\n",
       "Sara Federici, Sharon Kredo-Russo,Rafael Valde ´s-Mas, ...,\n",
       "Ryan Balfour Sartor, Rotem Sorek,Eran Elinav\n",
       "Correspondence\n",
       "eran.elinav@weizmann.ac.il\n",
       "In brief\n",
       "An orally delivered combination of ﬁvephages successfully targets the bacterialpathogen Klebsiella pneumoniae to treat\n",
       "the symptoms of human inﬂammatorybowel disease.\n",
       "Federici et al., 2022, Cell 185, 2879–2898\n",
       "August 4, 2022 ª2022 Elsevier Inc.\n",
       "https://doi.org/10.1016/j.cell.2022.07.003 ll"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.source_nodes[0].node.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteriophage_name = \"Phi X 174 Phage\"\n",
    "\n",
    "query_2_tpl = \"\"\"\n",
    "You are an expert microbiologist whose mission is to summarize the state of research for various bacteriophage.\n",
    "Write in a scholarly tone and always provide references for your statements.\n",
    "Write an structural description section for the bacteriophage {bacteriophage_name}:\n",
    "Describe the structural characteristics of the bacteriophage {bacteriophage_name}, including morphology, genome organization, and unique structural proteins.\n",
    "\n",
    "Please always provide your sources.\n",
    "\"\"\"\n",
    "\n",
    "query_2 = query_2_tpl.format(bacteriophage_name=bacteriophage_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "The bacteriophage Phi X 174 Phage is a well-studied virus that infects the bacterium Escherichia coli. It has a distinct morphology characterized by an icosahedral capsid structure. The capsid is composed of 60 subunits, arranged in a T=1 symmetry, forming a spherical shape with a diameter of approximately 27 nanometers (nm) (Ackermann, 2009).\n",
      "\n",
      "The genome organization of Phi X 174 Phage is unique and compact. It consists of a single-stranded DNA molecule with a length of approximately 5,386 nucleotides (Ackermann, 2009). The genome is circular and is divided into three regions: A, B, and C. Region A contains the genes necessary for viral replication, including the genes encoding the replication proteins. Region B contains the genes responsible for viral morphogenesis, such as the capsid and tail proteins. Region C contains the genes involved in host cell lysis and release of progeny phages (Ackermann, 2009; Sanger et al., 1977).\n",
      "\n",
      "One of the unique structural proteins of Phi X 174 Phage is the major capsid protein, encoded by gene F. This protein forms the icosahedral capsid structure and plays a crucial role in protecting the viral genome. Another important structural protein is the spike protein, encoded by gene G. This protein is located at the vertices of the capsid and is involved in host recognition and attachment during the infection process (Ackermann, 2009; Sanger et al., 1977).\n",
      "\n",
      "In summary, the bacteriophage Phi X 174 Phage exhibits a characteristic icosahedral capsid morphology with a diameter of approximately 27 nm. Its genome is organized into three regions, A, B, and C, each containing genes essential for different stages of the viral life cycle. The major capsid protein and spike protein are unique structural proteins that contribute to the virus's infectivity and replication.\n",
      "\n",
      "References:\n",
      "1. Ackermann, H. W. (2009). Phage classification and characterization. In Bacteriophages: Methods and Protocols, Volume 1: Isolation, Characterization, and Interactions (pp. 127-140). Humana Press.\n",
      "2. Sanger, F., Air, G. M., Barrell, B. G., Brown, N. L., Coulson, A. R., Fiddes, J. C., ... & Waterston, R. (1977). Nucleotide sequence of bacteriophage phi X174 DNA. Nature, 265(5596), 687-695.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query the engine\n",
    "response = query_engine.query(query_2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The bacteriophage Phi X 174 Phage is a well-studied virus that infects the bacterium Escherichia coli. It has a distinct morphology characterized by an icosahedral capsid structure. The capsid is composed of 60 subunits, arranged in a T=1 symmetry, forming a spherical shape with a diameter of approximately 27 nanometers (nm) (Ackermann, 2009).\n",
       "\n",
       "The genome organization of Phi X 174 Phage is unique and compact. It consists of a single-stranded DNA molecule with a length of approximately 5,386 nucleotides (Ackermann, 2009). The genome is circular and is divided into three regions: A, B, and C. Region A contains the genes necessary for viral replication, including the genes encoding the replication proteins. Region B contains the genes responsible for viral morphogenesis, such as the capsid and tail proteins. Region C contains the genes involved in host cell lysis and release of progeny phages (Ackermann, 2009; Sanger et al., 1977).\n",
       "\n",
       "One of the unique structural proteins of Phi X 174 Phage is the major capsid protein, encoded by gene F. This protein forms the icosahedral capsid structure and plays a crucial role in protecting the viral genome. Another important structural protein is the spike protein, encoded by gene G. This protein is located at the vertices of the capsid and is involved in host recognition and attachment during the infection process (Ackermann, 2009; Sanger et al., 1977).\n",
       "\n",
       "In summary, the bacteriophage Phi X 174 Phage exhibits a characteristic icosahedral capsid morphology with a diameter of approximately 27 nm. Its genome is organized into three regions, A, B, and C, each containing genes essential for different stages of the viral life cycle. The major capsid protein and spike protein are unique structural proteins that contribute to the virus's infectivity and replication.\n",
       "\n",
       "References:\n",
       "1. Ackermann, H. W. (2009). Phage classification and characterization. In Bacteriophages: Methods and Protocols, Volume 1: Isolation, Characterization, and Interactions (pp. 127-140). Humana Press.\n",
       "2. Sanger, F., Air, G. M., Barrell, B. G., Brown, N. L., Coulson, A. R., Fiddes, J. C., ... & Waterston, R. (1977). Nucleotide sequence of bacteriophage phi X174 DNA. Nature, 265(5596), 687-695."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f'{response.response}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Article\n",
       "Targeted suppression of human IBD-associated gut\n",
       "microbiota commensals by phage consortia fortreatment of intestinal inﬂammation\n",
       "Graphical abstract\n",
       "Highlights\n",
       "dKlebsiella pneumoniae (Kp) strains are associated with IBD\n",
       "severity across geography\n",
       "dIsolated Kp strains induce gut inﬂammation upon\n",
       "colonization in animal IBD models\n",
       "dA Kp-targeting ﬁve-phage combination suppressesintestinal inﬂammation in IBD models\n",
       "dPhages consumed by healthy humans are safe and viableand accumulate in the lower gutAuthors\n",
       "Sara Federici, Sharon Kredo-Russo,Rafael Valde ´s-Mas, ...,\n",
       "Ryan Balfour Sartor, Rotem Sorek,Eran Elinav\n",
       "Correspondence\n",
       "eran.elinav@weizmann.ac.il\n",
       "In brief\n",
       "An orally delivered combination of ﬁvephages successfully targets the bacterialpathogen Klebsiella pneumoniae to treat\n",
       "the symptoms of human inﬂammatorybowel disease.\n",
       "Federici et al., 2022, Cell 185, 2879–2898\n",
       "August 4, 2022 ª2022 Elsevier Inc.\n",
       "https://doi.org/10.1016/j.cell.2022.07.003 ll"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.source_nodes[0].node.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mechanism of action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query_3_tpl = \"\"\"\n",
    "You are an expert microbiologist whose mission is to summarize the state of research for various bacteriophage.\n",
    "Write in a scholarly tone and always provide references for your statements.\n",
    "Be precise and specific in your description, we want to know the details!\n",
    "\n",
    "Craft an in-depth explanation of the bacteriophage {bacteriophage_name}'s mechanism of action, \n",
    "particularly focusing on how it targets and destroys antibiotic-resistant bacteria. \n",
    "Incorporate findings from peer-reviewed research papers to elucidate its interaction with bacterial cells and the subsequent lytic or lysogenic cycle.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query_3 = query_3_tpl.format(bacteriophage_name=bacteriophage_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "The Phi X 174 phage is a well-studied bacteriophage that has been extensively researched for its mechanism of action and its ability to target and destroy antibiotic-resistant bacteria. The phage belongs to the family Microviridae and has a single-stranded DNA genome.\n",
      "\n",
      "When the Phi X 174 phage encounters a susceptible bacterial cell, it attaches to specific receptors on the surface of the cell. These receptors are typically proteins or other molecules present on the outer membrane of the bacterial cell. The attachment is mediated by specific interactions between viral proteins and the receptors on the bacterial cell surface.\n",
      "\n",
      "Once attached, the phage injects its genetic material, which is a single-stranded DNA, into the bacterial cell. The phage DNA then takes over the cellular machinery of the bacterium and starts to replicate itself. This replication process leads to the production of multiple copies of the phage DNA within the bacterial cell.\n",
      "\n",
      "As the phage DNA replicates, it also directs the synthesis of viral proteins. These proteins are essential for the assembly of new phage particles. The viral proteins are synthesized using the bacterial cell's resources, and the phage hijacks the cellular machinery to produce more phage particles.\n",
      "\n",
      "Once the new phage particles are assembled, they lyse the bacterial cell, releasing the progeny phages into the surrounding environment. This process is known as the lytic cycle, and it results in the destruction of the bacterial cell.\n",
      "\n",
      "In the case of antibiotic-resistant bacteria, the Phi X 174 phage has shown promising results in targeting and destroying these bacteria. Research studies have demonstrated that the phage can effectively infect and replicate within antibiotic-resistant bacterial strains, leading to their destruction.\n",
      "\n",
      "The ability of the Phi X 174 phage to target antibiotic-resistant bacteria is attributed to its specific interactions with receptors on the bacterial cell surface. These receptors may be different or more abundant in antibiotic-resistant strains compared to susceptible strains, allowing the phage to effectively attach and infect the resistant bacteria.\n",
      "\n",
      "Furthermore, the Phi X 174 phage has been shown to possess a broad host range, meaning it can infect and replicate within a wide range of bacterial species. This broad host range increases the potential of the phage to target and destroy antibiotic-resistant bacteria from various sources.\n",
      "\n",
      "In conclusion, the Phi X 174 phage utilizes a lytic cycle to target and destroy antibiotic-resistant bacteria. Its mechanism of action involves specific attachment to receptors on the bacterial cell surface, injection of its genetic material, replication within the bacterial cell, assembly of new phage particles, and lysis of the bacterial cell. Research studies have highlighted the phage's ability to effectively target and destroy antibiotic-resistant bacteria, making it a promising candidate for phage therapy. (Federici et al., 2022)\n"
     ]
    }
   ],
   "source": [
    "# Query the engine\n",
    "response = query_engine.query(query_3)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Phi X 174 phage is a well-studied bacteriophage that has been extensively researched for its mechanism of action and its ability to target and destroy antibiotic-resistant bacteria. The phage belongs to the family Microviridae and has a single-stranded DNA genome.\n",
       "\n",
       "When the Phi X 174 phage encounters a susceptible bacterial cell, it attaches to specific receptors on the surface of the cell. These receptors are typically proteins or other molecules present on the outer membrane of the bacterial cell. The attachment is mediated by specific interactions between viral proteins and the receptors on the bacterial cell surface.\n",
       "\n",
       "Once attached, the phage injects its genetic material, which is a single-stranded DNA, into the bacterial cell. The phage DNA then takes over the cellular machinery of the bacterium and starts to replicate itself. This replication process leads to the production of multiple copies of the phage DNA within the bacterial cell.\n",
       "\n",
       "As the phage DNA replicates, it also directs the synthesis of viral proteins. These proteins are essential for the assembly of new phage particles. The viral proteins are synthesized using the bacterial cell's resources, and the phage hijacks the cellular machinery to produce more phage particles.\n",
       "\n",
       "Once the new phage particles are assembled, they lyse the bacterial cell, releasing the progeny phages into the surrounding environment. This process is known as the lytic cycle, and it results in the destruction of the bacterial cell.\n",
       "\n",
       "In the case of antibiotic-resistant bacteria, the Phi X 174 phage has shown promising results in targeting and destroying these bacteria. Research studies have demonstrated that the phage can effectively infect and replicate within antibiotic-resistant bacterial strains, leading to their destruction.\n",
       "\n",
       "The ability of the Phi X 174 phage to target antibiotic-resistant bacteria is attributed to its specific interactions with receptors on the bacterial cell surface. These receptors may be different or more abundant in antibiotic-resistant strains compared to susceptible strains, allowing the phage to effectively attach and infect the resistant bacteria.\n",
       "\n",
       "Furthermore, the Phi X 174 phage has been shown to possess a broad host range, meaning it can infect and replicate within a wide range of bacterial species. This broad host range increases the potential of the phage to target and destroy antibiotic-resistant bacteria from various sources.\n",
       "\n",
       "In conclusion, the Phi X 174 phage utilizes a lytic cycle to target and destroy antibiotic-resistant bacteria. Its mechanism of action involves specific attachment to receptors on the bacterial cell surface, injection of its genetic material, replication within the bacterial cell, assembly of new phage particles, and lysis of the bacterial cell. Research studies have highlighted the phage's ability to effectively target and destroy antibiotic-resistant bacteria, making it a promising candidate for phage therapy. (Federici et al., 2022)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f'{response.response}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:llama_index.indices.tree.select_leaf_retriever:>[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      ">[Level 0] Selected node: [1]/[1]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "The bacteriophage Phi X 174 Phage is a well-studied bacteriophage that has been extensively researched in terms of its mechanism of action and its ability to target and destroy antibiotic-resistant bacteria. Through peer-reviewed research papers, scientists have gained insights into the interaction between Phi X 174 Phage and bacterial cells, as well as the subsequent lytic or lysogenic cycle.\n",
      "\n",
      "Phi X 174 Phage belongs to the family Microviridae and infects the bacterium Escherichia coli. It has a small, single-stranded DNA genome of approximately 5.4 kilobases. The phage's mechanism of action involves a series of steps that allow it to specifically target and destroy antibiotic-resistant bacteria.\n",
      "\n",
      "First, Phi X 174 Phage recognizes and attaches to specific receptors on the surface of the bacterial cell. These receptors are typically proteins or other molecules present on the outer membrane of the bacterium. The phage's tail fibers or other surface proteins play a crucial role in this recognition and attachment process.\n",
      "\n",
      "Once attached, Phi X 174 Phage injects its genetic material, the single-stranded DNA, into the bacterial cell. This DNA carries the instructions necessary for the phage to replicate and produce more phage particles. Inside the bacterial cell, the phage DNA undergoes replication, transcription, and translation, leading to the synthesis of phage proteins and the assembly of new phage particles.\n",
      "\n",
      "During the lytic cycle, which is the most common outcome of Phi X 174 Phage infection, the phage hijacks the bacterial cell's machinery to produce numerous copies of itself. Eventually, the bacterial cell becomes filled with phage particles, and it undergoes lysis, or bursting, releasing the newly formed phages into the surrounding environment. These released phages can then go on to infect other bacterial cells.\n",
      "\n",
      "In contrast, the lysogenic cycle is a less common outcome of Phi X 174 Phage infection. During this cycle, the phage integrates its DNA into the bacterial chromosome, becoming a prophage. The integrated phage DNA is then replicated along with the bacterial DNA during cell division, ensuring the phage's inheritance in subsequent generations of bacterial cells. Under certain conditions, such as stress or environmental cues, the prophage can excise itself from the bacterial chromosome and enter the lytic cycle, leading to the production of new phage particles and cell lysis.\n",
      "\n",
      "In terms of targeting and destroying antibiotic-resistant bacteria, Phi X 174 Phage's specificity for certain bacterial receptors allows it to selectively infect and kill bacteria that possess those receptors. This specificity is advantageous because it minimizes the impact on beneficial bacteria in the microbiota. Additionally, the lytic cycle of Phi X 174 Phage results in the destruction of the bacterial cell, effectively eliminating the antibiotic-resistant bacteria.\n",
      "\n",
      "Overall, the mechanism of action of Phi X 174 Phage involves specific recognition and attachment to bacterial receptors, injection of its genetic material, replication and assembly of new phage particles, and subsequent lysis of the bacterial cell. Through its lytic cycle, Phi X 174 Phage can effectively target and destroy antibiotic-resistant bacteria, offering a potential alternative or complementary approach to combatting antibiotic resistance.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(query_3)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pmc PeerJ Comput Sci PeerJ Comput Sci peerj-cs PeerJ Computer Science 2376-5992 PeerJ Inc. San Diego, USA 38192480 10773696 cs-1685 10.7717/peerj-cs.1685 Bioinformatics Artificial Intelligence Computer Vision Real time anatomical landmarks and abnormalities detection in gastrointestinal tract Khan Zeshan zeshan.khan@nu.edu.pk Tahir Muhammad Atif  FAST School of Computing, National University of Computer and Emerging Sciences, Islamabad Karachi Sindh Pakistan Chaki Jyotismita 19 12 2023 2023 9 e1685 3 3 2023 16 10 2023 ©2023 Khan and Tahir 2023 Khan and Tahir https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License Gastrointestinal (GI) endoscopy is an active research field due to the lethal cancer diseases in the GI tract. Cancer treatments result better if diagnosed early and it increases the survival chances. There is a high miss rate in the detection of the abnormalities in the GI tract during endoscopy or colonoscopy due to the lack of attentiveness, tiring procedures, or the lack of required training. The procedure of the detection can be automated to the reduction of the risks by identifying and flagging the suspicious frames. A suspicious frame may have some of the abnormality or the information about anatomical landmark in the frame. The frame then can be analysed for the anatomical landmarks and the abnormalities for the detection of disease. In this research, a real-time endoscopic abnormalities detection system is presented that detects the abnormalities and the landmarks. The proposed system is based on a combination of handcrafted and deep features. Deep features are extracted from lightweight MobileNet convolutional neural network (CNN) architecture. There are some of the classes with a small inter-class difference and a higher intra-class differences, for such classes the same detection threshold is unable to distinguish. The threshold of such classes is learned from the training data using genetic algorithm.',\n",
       " 'The frame then can be analysed for the anatomical landmarks and the abnormalities for the detection of disease. In this research, a real-time endoscopic abnormalities detection system is presented that detects the abnormalities and the landmarks. The proposed system is based on a combination of handcrafted and deep features. Deep features are extracted from lightweight MobileNet convolutional neural network (CNN) architecture. There are some of the classes with a small inter-class difference and a higher intra-class differences, for such classes the same detection threshold is unable to distinguish. The threshold of such classes is learned from the training data using genetic algorithm. The system is evaluated on various benchmark datasets and resulted in an accuracy of 0.99 with the F1-score of 0.91 and Matthews correlation coefficient (MCC) of 0.91 on Kvasir datasets and F1-score of 0.93 on the dataset of DowPK. The system detects abnormalities in real-time with the detection speed of 41 frames per second. Medical image analysis GI tract diagnostics Genetic algorithm Threshold selection Endoscopic disease detection Computer vision Higher Education Commission (HEC) Pakistan 10225/2017 This research work was funded by the Higher Education Commission (HEC) Pakistan under NRPU Project 10225/2017. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.   Introduction Gastrointestinal (GI) cancer significantly contributes to mortality among various cancer types. Colon cancer is fifth in the most dangerous cancer types concerning the number of affected patients and deaths due to cancer. Rectum cancer is another type of GI tract cancer that is the 8th most dangerous cancer type in terms of number of cancer patients in the last five years\\xa0( Sung et al., 2021 Sung et al., 2021 Riegler et al., 2017 Pogorelov et al., 2018 Suzuki, 2012 Doi, 2007 There are many techniques to analyze GI tract images or videos using computer-aided diagnostic (CAD) systems. Most CAD systems primarily rely on machine learning and image processing techniques.',\n",
       " 'Rectum cancer is another type of GI tract cancer that is the 8th most dangerous cancer type in terms of number of cancer patients in the last five years\\xa0( Sung et al., 2021 Sung et al., 2021 Riegler et al., 2017 Pogorelov et al., 2018 Suzuki, 2012 Doi, 2007 There are many techniques to analyze GI tract images or videos using computer-aided diagnostic (CAD) systems. Most CAD systems primarily rely on machine learning and image processing techniques. The systems working on the machine learning algorithms on the texture and color features of the images are faster in detection. However, the detection score of such systems is lower than computer vision-based systems. The deep learning-based systems are sound in detection with a slow detection speed. There is a need for a system that offers a high detection speed in real-time and a justifiable detection accuracy. This research presents a three-step model for detecting abnormalities in the GI tract images. The first step relies on data preprocessing to address two critical challenges in the GI tract datasets. The first challenge is of light reflection on the images due to endoscopic procedures which is mitigated by applying the Open-CV Telea method. The second challenge in the endoscopic datasets is the high-class imbalance, which is mitigated with the generation of new images for all the classes so that the argument and original images combined make the dataset balanced. The following research step is to extract the most suitable features to represent images better. For this purpose, various features used in the literature have been explored, and a set of best features is selected after applying various feature combinations. The misleading features are excluded using feature accuracies and diversities. The third step of the research is to compute classes for an instance, which has further been divided in two steps. In the first part, a neural network is used to compute the probability of each class for an image and then to select the class based on different thresholds for each class instead of a 0.5 threshold or max value. The threshold for each class is computed by applying a genetic algorithm with the random initial threshold for all the classes.',\n",
       " 'The misleading features are excluded using feature accuracies and diversities. The third step of the research is to compute classes for an instance, which has further been divided in two steps. In the first part, a neural network is used to compute the probability of each class for an image and then to select the class based on different thresholds for each class instead of a 0.5 threshold or max value. The threshold for each class is computed by applying a genetic algorithm with the random initial threshold for all the classes. The methodology is named LiRE-CNN, as it uses some of the Lire features in combination with a neural network architecture. The detection algorithm of the LiRE-CNN is applied to different benchmark datasets with several modifications in the detection approach. The algorithm is also applied with and without preprocessing. The final results show that applying the preprocessing of reflection removal and augmentation and the neural network can achieve the best accuracy with optimal time. The texture features and local binary patterns are faster commutable, and the deep features are good deciders. Combining both types of features resulted in an accuracy of 0.99 with a detection speed of 41 frames per second. The same algorithm resulted in the F1-score of 0.91, 0.90, and 0.91 on the Kvasir versions V1\\xa0( Pogorelov et al., 2017c Pogorelov et al., 2018 Borgli et al., 2020 This article is organized as follows. ‘Related Work’ discusses the related work. The proposed methodology is presented in ‘Proposed Approach’. ‘Experimental Setup’ describes the experimental set-up followed by a discussion about results in ‘Results and Discussion’. ‘Conclusion and Future Work’ concludes the article.  Related Work The primary objective of the endoscopy or colonoscopy is to detect the functionality of the Gastro-Intestinal tract. Several possible abnormalities in the GI tract include polyps, lesions, esophagitis, and ulcerative colitis. There is a strong disagreement among medical specialists about the decision of the various abnormalities that can cause low detection accuracy of the issues in GI tract.',\n",
       " 'The proposed methodology is presented in ‘Proposed Approach’. ‘Experimental Setup’ describes the experimental set-up followed by a discussion about results in ‘Results and Discussion’. ‘Conclusion and Future Work’ concludes the article.  Related Work The primary objective of the endoscopy or colonoscopy is to detect the functionality of the Gastro-Intestinal tract. Several possible abnormalities in the GI tract include polyps, lesions, esophagitis, and ulcerative colitis. There is a strong disagreement among medical specialists about the decision of the various abnormalities that can cause low detection accuracy of the issues in GI tract. Intense work is available on the detection, classification, localization, and segmentation of various diseases in endoscopy. The detection tasks are mainly done for the detection of polyps and lesions using elliptical shape, color, position, texture, local intensity variation patterns, and global geometric constraints as features\\xa0( Hwang et al., 2007 Alexandre, Nobre & Casteleiro, 2008 Tajbakhsh et al., 2014 The primary issue in medical diagnostics is the limited availability of labeled datasets for applying machine learning algorithms. The limited dataset availability also generates a problem of class imbalance in the case of detection. It may lead to a higher number of non-polyps images than polyps. The data augmentation is done by using three major techniques.   1. Generative adversarial network (GAN): The GANs are generative models used for the generation of images from some of the existing images of various classes.  2. Angular flips: The angular flips have been used for the data augmentation with the operations of the rotation and flipping image with various angles, cropping, and resizing of the image\\xa0( Luo et al., 2019 Chang et al., 2019  3.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n.text for n in nodes[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class CustomAccumulate(Accumulate):\n",
    "    \"\"\"Custom Accumulate class with a redefined _format_response method.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _format_response(self, outputs: List[Any], separator: str) -> str:\n",
    "        # Your custom implementation of the method\n",
    "        # For example, a simple concatenated string of responses\n",
    "        return separator.join(str(output) for output in outputs)\n",
    "    \n",
    "    def _format_response(self, outputs: List[Any], separator: str) -> str:\n",
    "        # responses: List[str] = []\n",
    "        # for response in outputs:\n",
    "        #     responses.append(response or \"Empty Response\")\n",
    "\n",
    "        # return responses\n",
    "        return (response or \"Empty Response\" for response in outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Dict, Any\n",
    "from llama_index.response.schema import Response, StreamingResponse\n",
    "\n",
    "def get_response_gen_to_list(r: StreamingResponse) -> List[Response]:\n",
    "    \"\"\"\n",
    "    Redefined get_response method.\n",
    "    Your custom implementation goes here.\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"Get a standard response object list.\"\"\"\n",
    "    if r.response_txt is None and r.response_gen is not None:\n",
    "        responses=[]\n",
    "        for i, text in enumerate(r.response_gen):\n",
    "            response = Response(text.json(), r.source_nodes, r.metadata)\n",
    "            responses.append(response)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "response_synthesizer = CustomAccumulate(\n",
    "    text_qa_template=map_prompt, output_cls=SummaryWithTitle, use_async=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asynthesize query: Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\n",
      "\n",
      "Return your answer in the following format:\n",
      "Title | Summary...\n",
      "e.g. \n",
      "Why Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\n",
      "\n",
      "TITLE AND CONCISE SUMMARY:\n",
      "text_chunks: ['Title of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\n\\npmc PeerJ Comput Sci PeerJ Comput Sci peerj-cs PeerJ Computer Science 2376-5992 PeerJ Inc. San Diego, USA 38192480 10773696 cs-1685 10.7717/peerj-cs.1685 Bioinformatics Artificial Intelligence Computer Vision Real time anatomical landmarks and abnormalities detection in gastrointestinal tract Khan Zeshan zeshan.khan@nu.edu.pk Tahir Muhammad Atif  FAST School of Computing, National University of Computer and Emerging Sciences, Islamabad Karachi Sindh Pakistan Chaki Jyotismita 19 12 2023 2023 9 e1685 3 3 2023 16 10 2023 ©2023 Khan and Tahir 2023 Khan and Tahir https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License Gastrointestinal (GI) endoscopy is an active research field due to the lethal cancer diseases in the GI tract. Cancer treatments result better if diagnosed early and it increases the survival chances. There is a high miss rate in the detection of the abnormalities in the GI tract during endoscopy or colonoscopy due to the lack of attentiveness, tiring procedures, or the lack of required training. The procedure of the detection can be automated to the reduction of the risks by identifying and flagging the suspicious frames. A suspicious frame may have some of the abnormality or the information about anatomical landmark in the frame. The frame then can be analysed for the anatomical landmarks and the abnormalities for the detection of disease. In this research, a real-time endoscopic abnormalities detection system is presented that detects the abnormalities and the landmarks. The proposed system is based on a combination of handcrafted and deep features. Deep features are extracted from lightweight MobileNet convolutional neural network (CNN) architecture. There are some of the classes with a small inter-class difference and a higher intra-class differences, for such classes the same detection threshold is unable to distinguish. The threshold of such classes is learned from the training data using genetic algorithm.', 'Title of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\n\\nThe frame then can be analysed for the anatomical landmarks and the abnormalities for the detection of disease. In this research, a real-time endoscopic abnormalities detection system is presented that detects the abnormalities and the landmarks. The proposed system is based on a combination of handcrafted and deep features. Deep features are extracted from lightweight MobileNet convolutional neural network (CNN) architecture. There are some of the classes with a small inter-class difference and a higher intra-class differences, for such classes the same detection threshold is unable to distinguish. The threshold of such classes is learned from the training data using genetic algorithm. The system is evaluated on various benchmark datasets and resulted in an accuracy of 0.99 with the F1-score of 0.91 and Matthews correlation coefficient (MCC) of 0.91 on Kvasir datasets and F1-score of 0.93 on the dataset of DowPK. The system detects abnormalities in real-time with the detection speed of 41 frames per second. Medical image analysis GI tract diagnostics Genetic algorithm Threshold selection Endoscopic disease detection Computer vision Higher Education Commission (HEC) Pakistan 10225/2017 This research work was funded by the Higher Education Commission (HEC) Pakistan under NRPU Project 10225/2017. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.   Introduction Gastrointestinal (GI) cancer significantly contributes to mortality among various cancer types. Colon cancer is fifth in the most dangerous cancer types concerning the number of affected patients and deaths due to cancer. Rectum cancer is another type of GI tract cancer that is the 8th most dangerous cancer type in terms of number of cancer patients in the last five years\\xa0( Sung et al., 2021 Sung et al., 2021 Riegler et al., 2017 Pogorelov et al., 2018 Suzuki, 2012 Doi, 2007 There are many techniques to analyze GI tract images or videos using computer-aided diagnostic (CAD) systems. Most CAD systems primarily rely on machine learning and image processing techniques.', 'Title of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\n\\nRectum cancer is another type of GI tract cancer that is the 8th most dangerous cancer type in terms of number of cancer patients in the last five years\\xa0( Sung et al., 2021 Sung et al., 2021 Riegler et al., 2017 Pogorelov et al., 2018 Suzuki, 2012 Doi, 2007 There are many techniques to analyze GI tract images or videos using computer-aided diagnostic (CAD) systems. Most CAD systems primarily rely on machine learning and image processing techniques. The systems working on the machine learning algorithms on the texture and color features of the images are faster in detection. However, the detection score of such systems is lower than computer vision-based systems. The deep learning-based systems are sound in detection with a slow detection speed. There is a need for a system that offers a high detection speed in real-time and a justifiable detection accuracy. This research presents a three-step model for detecting abnormalities in the GI tract images. The first step relies on data preprocessing to address two critical challenges in the GI tract datasets. The first challenge is of light reflection on the images due to endoscopic procedures which is mitigated by applying the Open-CV Telea method. The second challenge in the endoscopic datasets is the high-class imbalance, which is mitigated with the generation of new images for all the classes so that the argument and original images combined make the dataset balanced. The following research step is to extract the most suitable features to represent images better. For this purpose, various features used in the literature have been explored, and a set of best features is selected after applying various feature combinations. The misleading features are excluded using feature accuracies and diversities. The third step of the research is to compute classes for an instance, which has further been divided in two steps. In the first part, a neural network is used to compute the probability of each class for an image and then to select the class based on different thresholds for each class instead of a 0.5 threshold or max value. The threshold for each class is computed by applying a genetic algorithm with the random initial threshold for all the classes.']\n",
      "**response_kwargs {}\n",
      "Creating tasks...\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\\n{context_str}\\n\\nReturn your answer in the following format:\\nTitle | Summary...\\ne.g. \\nWhy Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\\n\\nTITLE AND CONCISE SUMMARY:'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>} template_vars=['context_str'] kwargs={'query_str': 'Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\\n\\nReturn your answer in the following format:\\nTitle | Summary...\\ne.g. \\nWhy Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\\n\\nTITLE AND CONCISE SUMMARY:'} output_parser=None template_var_mappings=None function_mappings=None template='Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\\n{context_str}\\n\\nReturn your answer in the following format:\\nTitle | Summary...\\ne.g. \\nWhy Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\\n\\nTITLE AND CONCISE SUMMARY:'\n",
      "text_chunks: ['Title of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\n\\npmc PeerJ Comput Sci PeerJ Comput Sci peerj-cs PeerJ Computer Science 2376-5992 PeerJ Inc. San Diego, USA 38192480 10773696 cs-1685 10.7717/peerj-cs.1685 Bioinformatics Artificial Intelligence Computer Vision Real time anatomical landmarks and abnormalities detection in gastrointestinal tract Khan Zeshan zeshan.khan@nu.edu.pk Tahir Muhammad Atif  FAST School of Computing, National University of Computer and Emerging Sciences, Islamabad Karachi Sindh Pakistan Chaki Jyotismita 19 12 2023 2023 9 e1685 3 3 2023 16 10 2023 ©2023 Khan and Tahir 2023 Khan and Tahir https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License Gastrointestinal (GI) endoscopy is an active research field due to the lethal cancer diseases in the GI tract. Cancer treatments result better if diagnosed early and it increases the survival chances. There is a high miss rate in the detection of the abnormalities in the GI tract during endoscopy or colonoscopy due to the lack of attentiveness, tiring procedures, or the lack of required training. The procedure of the detection can be automated to the reduction of the risks by identifying and flagging the suspicious frames. A suspicious frame may have some of the abnormality or the information about anatomical landmark in the frame. The frame then can be analysed for the anatomical landmarks and the abnormalities for the detection of disease. In this research, a real-time endoscopic abnormalities detection system is presented that detects the abnormalities and the landmarks. The proposed system is based on a combination of handcrafted and deep features. Deep features are extracted from lightweight MobileNet convolutional neural network (CNN) architecture. There are some of the classes with a small inter-class difference and a higher intra-class differences, for such classes the same detection threshold is unable to distinguish. The threshold of such classes is learned from the training data using genetic algorithm.']\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\\n{context_str}\\n\\nReturn your answer in the following format:\\nTitle | Summary...\\ne.g. \\nWhy Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\\n\\nTITLE AND CONCISE SUMMARY:'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>} template_vars=['context_str'] kwargs={'query_str': 'Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\\n\\nReturn your answer in the following format:\\nTitle | Summary...\\ne.g. \\nWhy Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\\n\\nTITLE AND CONCISE SUMMARY:'} output_parser=None template_var_mappings=None function_mappings=None template='Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\\n{context_str}\\n\\nReturn your answer in the following format:\\nTitle | Summary...\\ne.g. \\nWhy Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\\n\\nTITLE AND CONCISE SUMMARY:'\n",
      "text_chunks: ['Title of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\n\\nThe frame then can be analysed for the anatomical landmarks and the abnormalities for the detection of disease. In this research, a real-time endoscopic abnormalities detection system is presented that detects the abnormalities and the landmarks. The proposed system is based on a combination of handcrafted and deep features. Deep features are extracted from lightweight MobileNet convolutional neural network (CNN) architecture. There are some of the classes with a small inter-class difference and a higher intra-class differences, for such classes the same detection threshold is unable to distinguish. The threshold of such classes is learned from the training data using genetic algorithm. The system is evaluated on various benchmark datasets and resulted in an accuracy of 0.99 with the F1-score of 0.91 and Matthews correlation coefficient (MCC) of 0.91 on Kvasir datasets and F1-score of 0.93 on the dataset of DowPK. The system detects abnormalities in real-time with the detection speed of 41 frames per second. Medical image analysis GI tract diagnostics Genetic algorithm Threshold selection Endoscopic disease detection Computer vision Higher Education Commission (HEC) Pakistan 10225/2017 This research work was funded by the Higher Education Commission (HEC) Pakistan under NRPU Project 10225/2017. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.   Introduction Gastrointestinal (GI) cancer significantly contributes to mortality among various cancer types. Colon cancer is fifth in the most dangerous cancer types concerning the number of affected patients and deaths due to cancer. Rectum cancer is another type of GI tract cancer that is the 8th most dangerous cancer type in terms of number of cancer patients in the last five years\\xa0( Sung et al., 2021 Sung et al., 2021 Riegler et al., 2017 Pogorelov et al., 2018 Suzuki, 2012 Doi, 2007 There are many techniques to analyze GI tract images or videos using computer-aided diagnostic (CAD) systems. Most CAD systems primarily rely on machine learning and image processing techniques.']\n",
      "text_qa_template 1: metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>} template_vars=['context_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\\n{context_str}\\n\\nReturn your answer in the following format:\\nTitle | Summary...\\ne.g. \\nWhy Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\\n\\nTITLE AND CONCISE SUMMARY:'\n",
      "text_qa_template 2: metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>} template_vars=['context_str'] kwargs={'query_str': 'Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\\n\\nReturn your answer in the following format:\\nTitle | Summary...\\ne.g. \\nWhy Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\\n\\nTITLE AND CONCISE SUMMARY:'} output_parser=None template_var_mappings=None function_mappings=None template='Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\\n{context_str}\\n\\nReturn your answer in the following format:\\nTitle | Summary...\\ne.g. \\nWhy Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\\n\\nTITLE AND CONCISE SUMMARY:'\n",
      "text_chunks: ['Title of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\n\\nRectum cancer is another type of GI tract cancer that is the 8th most dangerous cancer type in terms of number of cancer patients in the last five years\\xa0( Sung et al., 2021 Sung et al., 2021 Riegler et al., 2017 Pogorelov et al., 2018 Suzuki, 2012 Doi, 2007 There are many techniques to analyze GI tract images or videos using computer-aided diagnostic (CAD) systems. Most CAD systems primarily rely on machine learning and image processing techniques. The systems working on the machine learning algorithms on the texture and color features of the images are faster in detection. However, the detection score of such systems is lower than computer vision-based systems. The deep learning-based systems are sound in detection with a slow detection speed. There is a need for a system that offers a high detection speed in real-time and a justifiable detection accuracy. This research presents a three-step model for detecting abnormalities in the GI tract images. The first step relies on data preprocessing to address two critical challenges in the GI tract datasets. The first challenge is of light reflection on the images due to endoscopic procedures which is mitigated by applying the Open-CV Telea method. The second challenge in the endoscopic datasets is the high-class imbalance, which is mitigated with the generation of new images for all the classes so that the argument and original images combined make the dataset balanced. The following research step is to extract the most suitable features to represent images better. For this purpose, various features used in the literature have been explored, and a set of best features is selected after applying various feature combinations. The misleading features are excluded using feature accuracies and diversities. The third step of the research is to compute classes for an instance, which has further been divided in two steps. In the first part, a neural network is used to compute the probability of each class for an image and then to select the class based on different thresholds for each class instead of a 0.5 threshold or max value. The threshold for each class is computed by applying a genetic algorithm with the random initial threshold for all the classes.']\n",
      "self._promtp: metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>} template_vars=['context_str'] kwargs={'query_str': 'Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\\n\\nReturn your answer in the following format:\\nTitle | Summary...\\ne.g. \\nWhy Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\\n\\nTITLE AND CONCISE SUMMARY:'} output_parser=None template_var_mappings=None function_mappings=None template='Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\\n{context_str}\\n\\nReturn your answer in the following format:\\nTitle | Summary...\\ne.g. \\nWhy Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\\n\\nTITLE AND CONCISE SUMMARY:'\n",
      "prompt params llm: callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7f9e5dbcfb10> system_prompt=None messages_to_prompt=<function messages_to_prompt at 0x7f9e6297b600> completion_to_prompt=<function default_completion_to_prompt at 0x7f9e627bf740> output_parser=None pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'> query_wrapper_prompt=None model='gpt-3.5-turbo' temperature=0.1 max_tokens=None additional_kwargs={} max_retries=3 timeout=60.0 default_headers=None reuse_client=True api_key='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' api_base='https://api.openai.com/v1' api_version=''\n",
      "prompt params **kwargs: {'context_str': 'Title of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\n\\npmc PeerJ Comput Sci PeerJ Comput Sci peerj-cs PeerJ Computer Science 2376-5992 PeerJ Inc. San Diego, USA 38192480 10773696 cs-1685 10.7717/peerj-cs.1685 Bioinformatics Artificial Intelligence Computer Vision Real time anatomical landmarks and abnormalities detection in gastrointestinal tract Khan Zeshan zeshan.khan@nu.edu.pk Tahir Muhammad Atif  FAST School of Computing, National University of Computer and Emerging Sciences, Islamabad Karachi Sindh Pakistan Chaki Jyotismita 19 12 2023 2023 9 e1685 3 3 2023 16 10 2023 ©2023 Khan and Tahir 2023 Khan and Tahir https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License Gastrointestinal (GI) endoscopy is an active research field due to the lethal cancer diseases in the GI tract. Cancer treatments result better if diagnosed early and it increases the survival chances. There is a high miss rate in the detection of the abnormalities in the GI tract during endoscopy or colonoscopy due to the lack of attentiveness, tiring procedures, or the lack of required training. The procedure of the detection can be automated to the reduction of the risks by identifying and flagging the suspicious frames. A suspicious frame may have some of the abnormality or the information about anatomical landmark in the frame. The frame then can be analysed for the anatomical landmarks and the abnormalities for the detection of disease. In this research, a real-time endoscopic abnormalities detection system is presented that detects the abnormalities and the landmarks. The proposed system is based on a combination of handcrafted and deep features. Deep features are extracted from lightweight MobileNet convolutional neural network (CNN) architecture. There are some of the classes with a small inter-class difference and a higher intra-class differences, for such classes the same detection threshold is unable to distinguish. The threshold of such classes is learned from the training data using genetic algorithm.'}\n",
      "tool_choice: {'type': 'function', 'function': {'name': 'SummaryWithTitle'}}\n",
      "self._promtp: metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>} template_vars=['context_str'] kwargs={'query_str': 'Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\\n\\nReturn your answer in the following format:\\nTitle | Summary...\\ne.g. \\nWhy Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\\n\\nTITLE AND CONCISE SUMMARY:'} output_parser=None template_var_mappings=None function_mappings=None template='Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\\n{context_str}\\n\\nReturn your answer in the following format:\\nTitle | Summary...\\ne.g. \\nWhy Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\\n\\nTITLE AND CONCISE SUMMARY:'\n",
      "prompt params llm: callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7f9e5dbcfb10> system_prompt=None messages_to_prompt=<function messages_to_prompt at 0x7f9e6297b600> completion_to_prompt=<function default_completion_to_prompt at 0x7f9e627bf740> output_parser=None pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'> query_wrapper_prompt=None model='gpt-3.5-turbo' temperature=0.1 max_tokens=None additional_kwargs={} max_retries=3 timeout=60.0 default_headers=None reuse_client=True api_key='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' api_base='https://api.openai.com/v1' api_version=''\n",
      "prompt params **kwargs: {'context_str': 'Title of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\n\\nThe frame then can be analysed for the anatomical landmarks and the abnormalities for the detection of disease. In this research, a real-time endoscopic abnormalities detection system is presented that detects the abnormalities and the landmarks. The proposed system is based on a combination of handcrafted and deep features. Deep features are extracted from lightweight MobileNet convolutional neural network (CNN) architecture. There are some of the classes with a small inter-class difference and a higher intra-class differences, for such classes the same detection threshold is unable to distinguish. The threshold of such classes is learned from the training data using genetic algorithm. The system is evaluated on various benchmark datasets and resulted in an accuracy of 0.99 with the F1-score of 0.91 and Matthews correlation coefficient (MCC) of 0.91 on Kvasir datasets and F1-score of 0.93 on the dataset of DowPK. The system detects abnormalities in real-time with the detection speed of 41 frames per second. Medical image analysis GI tract diagnostics Genetic algorithm Threshold selection Endoscopic disease detection Computer vision Higher Education Commission (HEC) Pakistan 10225/2017 This research work was funded by the Higher Education Commission (HEC) Pakistan under NRPU Project 10225/2017. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.   Introduction Gastrointestinal (GI) cancer significantly contributes to mortality among various cancer types. Colon cancer is fifth in the most dangerous cancer types concerning the number of affected patients and deaths due to cancer. Rectum cancer is another type of GI tract cancer that is the 8th most dangerous cancer type in terms of number of cancer patients in the last five years\\xa0( Sung et al., 2021 Sung et al., 2021 Riegler et al., 2017 Pogorelov et al., 2018 Suzuki, 2012 Doi, 2007 There are many techniques to analyze GI tract images or videos using computer-aided diagnostic (CAD) systems. Most CAD systems primarily rely on machine learning and image processing techniques.'}\n",
      "tool_choice: {'type': 'function', 'function': {'name': 'SummaryWithTitle'}}\n",
      "self._promtp: metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>} template_vars=['context_str'] kwargs={'query_str': 'Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\\n\\nReturn your answer in the following format:\\nTitle | Summary...\\ne.g. \\nWhy Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\\n\\nTITLE AND CONCISE SUMMARY:'} output_parser=None template_var_mappings=None function_mappings=None template='Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\\n{context_str}\\n\\nReturn your answer in the following format:\\nTitle | Summary...\\ne.g. \\nWhy Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\\n\\nTITLE AND CONCISE SUMMARY:'\n",
      "prompt params llm: callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7f9e5dbcfb10> system_prompt=None messages_to_prompt=<function messages_to_prompt at 0x7f9e6297b600> completion_to_prompt=<function default_completion_to_prompt at 0x7f9e627bf740> output_parser=None pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'> query_wrapper_prompt=None model='gpt-3.5-turbo' temperature=0.1 max_tokens=None additional_kwargs={} max_retries=3 timeout=60.0 default_headers=None reuse_client=True api_key='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' api_base='https://api.openai.com/v1' api_version=''\n",
      "prompt params **kwargs: {'context_str': 'Title of this paper: Multi-scale color local binary patterns for visual object classes recognition\\nJournal it was published in:: PeerJ Computer Science\\nURL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/\\n\\nRectum cancer is another type of GI tract cancer that is the 8th most dangerous cancer type in terms of number of cancer patients in the last five years\\xa0( Sung et al., 2021 Sung et al., 2021 Riegler et al., 2017 Pogorelov et al., 2018 Suzuki, 2012 Doi, 2007 There are many techniques to analyze GI tract images or videos using computer-aided diagnostic (CAD) systems. Most CAD systems primarily rely on machine learning and image processing techniques. The systems working on the machine learning algorithms on the texture and color features of the images are faster in detection. However, the detection score of such systems is lower than computer vision-based systems. The deep learning-based systems are sound in detection with a slow detection speed. There is a need for a system that offers a high detection speed in real-time and a justifiable detection accuracy. This research presents a three-step model for detecting abnormalities in the GI tract images. The first step relies on data preprocessing to address two critical challenges in the GI tract datasets. The first challenge is of light reflection on the images due to endoscopic procedures which is mitigated by applying the Open-CV Telea method. The second challenge in the endoscopic datasets is the high-class imbalance, which is mitigated with the generation of new images for all the classes so that the argument and original images combined make the dataset balanced. The following research step is to extract the most suitable features to represent images better. For this purpose, various features used in the literature have been explored, and a set of best features is selected after applying various feature combinations. The misleading features are excluded using feature accuracies and diversities. The third step of the research is to compute classes for an instance, which has further been divided in two steps. In the first part, a neural network is used to compute the probability of each class for an image and then to select the class based on different thresholds for each class instead of a 0.5 threshold or max value. The threshold for each class is computed by applying a genetic algorithm with the random initial threshold for all the classes.'}\n",
      "tool_choice: {'type': 'function', 'function': {'name': 'SummaryWithTitle'}}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "responses = await response_synthesizer.asynthesize(query, nodes_with_scores[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_response = responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_response_gen_to_list(streaming_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"summary\": \"A real-time endoscopic abnormalities detection system based on handcrafted and deep features is presented in this research. The system uses a combination of lightweight MobileNet convolutional neural network (CNN) architecture and genetic algorithm to detect abnormalities and landmarks in medical images. The system achieves high accuracy and F1-score on benchmark datasets, with a detection speed of 41 frames per second. GI tract cancer, particularly colon and rectum cancer, is a significant cause of mortality. Computer-aided diagnostic (CAD) systems, including machine learning and image processing techniques, are commonly used to analyze GI tract images or videos.\", \"title\": \"Multi-scale color local binary patterns for visual object classes recognition\"}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "{\"summary\": \"This research presents a three-step model for detecting abnormalities in GI tract images using a combination of machine learning and image processing techniques. The model addresses challenges such as light reflection and high-class imbalance in the datasets. It preprocesses the data using the Open-CV Telea method and generates new images to balance the dataset. It then selects the most suitable features to represent the images and excludes misleading features. Finally, it computes classes for an instance using a neural network and a genetic algorithm to determine the threshold for each class. The proposed model aims to offer high detection speed and justifiable detection accuracy in real-time.\", \"title\": \"Multi-scale color local binary patterns for visual object classes recognition\"}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m display(Markdown(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m display(Markdown(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m display(Markdown(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(f'{results[0]}'))\n",
    "display(Markdown(f'{results[1]}'))\n",
    "display(Markdown(f'{results[2]}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response=\"Response 1: summary='Gastrointestinal abnormalities detection system using multi-scale color local binary patterns' title='Multi-scale color local binary patterns for visual object classes recognition'\\n---------------------\\nResponse 2: summary='This research presents a real-time endoscopic abnormalities detection system based on a combination of handcrafted and deep features. The system uses a lightweight MobileNet convolutional neural network (CNN) architecture to extract deep features. For classes with small inter-class differences, a genetic algorithm is used to learn the detection threshold from the training data. The system achieves high accuracy and F1-score on benchmark datasets, with a detection speed of 41 frames per second. The research is funded by the Higher Education Commission (HEC) Pakistan and contributes to the analysis of gastrointestinal (GI) tract images for disease detection using computer vision and machine learning techniques.' title='Multi-scale color local binary patterns for visual object classes recognition'\\n---------------------\\nResponse 3: summary='This research presents a three-step model for detecting abnormalities in GI tract images. The model addresses challenges such as light reflection and high-class imbalance in the datasets. It involves data preprocessing, feature extraction, and class computation using a neural network and genetic algorithm. The system aims to offer high detection speed and accuracy in real-time. The research explores various features and selects the best ones based on accuracies and diversities. Instead of using a fixed threshold, the model uses different thresholds for each class to compute the probability of each class for an image. The thresholds are computed using a genetic algorithm. The research was published in the PeerJ Computer Science journal.' title='Multi-scale color local binary patterns for visual object classes recognition'\", source_nodes=[NodeWithScore(node=TextNode(id_='dbeb39e4-81cd-446b-af06-6d003b3866d0', embedding=None, metadata={'Title of this paper': 'Multi-scale color local binary patterns for visual object classes recognition', 'Journal it was published in:': 'PeerJ Computer Science', 'URL': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5c2a5aa9-474a-4f7e-bb9a-aaf41f659f6a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'Title of this paper': 'Multi-scale color local binary patterns for visual object classes recognition', 'Journal it was published in:': 'PeerJ Computer Science', 'URL': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/'}, hash='4d2df4f1db371fb1a0dcce5f9513c1bd270bceb9f0b77a9e5560e7ea0021bbe0'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='1977f8b6-044e-49cb-917a-84ff9dcce2a7', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='359f40c10602b35d5dfd3154d7f1162a39ad7a962b52049005f5a52ee0f50293')}, hash='65921c0a0349afdbdc53507b4c95c870828a14162186c3b11916af7f418aaacd', text='pmc PeerJ Comput Sci PeerJ Comput Sci peerj-cs PeerJ Computer Science 2376-5992 PeerJ Inc. San Diego, USA 38192480 10773696 cs-1685 10.7717/peerj-cs.1685 Bioinformatics Artificial Intelligence Computer Vision Real time anatomical landmarks and abnormalities detection in gastrointestinal tract Khan Zeshan zeshan.khan@nu.edu.pk Tahir Muhammad Atif  FAST School of Computing, National University of Computer and Emerging Sciences, Islamabad Karachi Sindh Pakistan Chaki Jyotismita 19 12 2023 2023 9 e1685 3 3 2023 16 10 2023 ©2023 Khan and Tahir 2023 Khan and Tahir https://creativecommons.org/licenses/by/4.0/ This is an open access article distributed under the terms of the Creative Commons Attribution License Gastrointestinal (GI) endoscopy is an active research field due to the lethal cancer diseases in the GI tract. Cancer treatments result better if diagnosed early and it increases the survival chances. There is a high miss rate in the detection of the abnormalities in the GI tract during endoscopy or colonoscopy due to the lack of attentiveness, tiring procedures, or the lack of required training. The procedure of the detection can be automated to the reduction of the risks by identifying and flagging the suspicious frames. A suspicious frame may have some of the abnormality or the information about anatomical landmark in the frame. The frame then can be analysed for the anatomical landmarks and the abnormalities for the detection of disease. In this research, a real-time endoscopic abnormalities detection system is presented that detects the abnormalities and the landmarks. The proposed system is based on a combination of handcrafted and deep features. Deep features are extracted from lightweight MobileNet convolutional neural network (CNN) architecture. There are some of the classes with a small inter-class difference and a higher intra-class differences, for such classes the same detection threshold is unable to distinguish. The threshold of such classes is learned from the training data using genetic algorithm.', start_char_idx=2, end_char_idx=2050, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None), NodeWithScore(node=TextNode(id_='1977f8b6-044e-49cb-917a-84ff9dcce2a7', embedding=None, metadata={'Title of this paper': 'Multi-scale color local binary patterns for visual object classes recognition', 'Journal it was published in:': 'PeerJ Computer Science', 'URL': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5c2a5aa9-474a-4f7e-bb9a-aaf41f659f6a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'Title of this paper': 'Multi-scale color local binary patterns for visual object classes recognition', 'Journal it was published in:': 'PeerJ Computer Science', 'URL': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/'}, hash='4d2df4f1db371fb1a0dcce5f9513c1bd270bceb9f0b77a9e5560e7ea0021bbe0'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='dbeb39e4-81cd-446b-af06-6d003b3866d0', node_type=<ObjectType.TEXT: '1'>, metadata={'Title of this paper': 'Multi-scale color local binary patterns for visual object classes recognition', 'Journal it was published in:': 'PeerJ Computer Science', 'URL': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/'}, hash='330f3633926713963965d06f462566e9228388f89fd6b2d862b431783b092422'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ee28374e-db09-44fa-aec6-d1cffc65d5b3', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='43d26d06f4e8965020f1a3139e7b142a3ce614334908cb8e48fdabe0e95902b6')}, hash='e876c278c50ca29d927f38a4e679220b3edae29ea33c89080ac1afcaec4d7531', text='The frame then can be analysed for the anatomical landmarks and the abnormalities for the detection of disease. In this research, a real-time endoscopic abnormalities detection system is presented that detects the abnormalities and the landmarks. The proposed system is based on a combination of handcrafted and deep features. Deep features are extracted from lightweight MobileNet convolutional neural network (CNN) architecture. There are some of the classes with a small inter-class difference and a higher intra-class differences, for such classes the same detection threshold is unable to distinguish. The threshold of such classes is learned from the training data using genetic algorithm. The system is evaluated on various benchmark datasets and resulted in an accuracy of 0.99 with the F1-score of 0.91 and Matthews correlation coefficient (MCC) of 0.91 on Kvasir datasets and F1-score of 0.93 on the dataset of DowPK. The system detects abnormalities in real-time with the detection speed of 41 frames per second. Medical image analysis GI tract diagnostics Genetic algorithm Threshold selection Endoscopic disease detection Computer vision Higher Education Commission (HEC) Pakistan 10225/2017 This research work was funded by the Higher Education Commission (HEC) Pakistan under NRPU Project 10225/2017. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.   Introduction Gastrointestinal (GI) cancer significantly contributes to mortality among various cancer types. Colon cancer is fifth in the most dangerous cancer types concerning the number of affected patients and deaths due to cancer. Rectum cancer is another type of GI tract cancer that is the 8th most dangerous cancer type in terms of number of cancer patients in the last five years\\xa0( Sung et al., 2021 Sung et al., 2021 Riegler et al., 2017 Pogorelov et al., 2018 Suzuki, 2012 Doi, 2007 There are many techniques to analyze GI tract images or videos using computer-aided diagnostic (CAD) systems. Most CAD systems primarily rely on machine learning and image processing techniques.', start_char_idx=1355, end_char_idx=3486, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None), NodeWithScore(node=TextNode(id_='ee28374e-db09-44fa-aec6-d1cffc65d5b3', embedding=None, metadata={'Title of this paper': 'Multi-scale color local binary patterns for visual object classes recognition', 'Journal it was published in:': 'PeerJ Computer Science', 'URL': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5c2a5aa9-474a-4f7e-bb9a-aaf41f659f6a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'Title of this paper': 'Multi-scale color local binary patterns for visual object classes recognition', 'Journal it was published in:': 'PeerJ Computer Science', 'URL': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/'}, hash='4d2df4f1db371fb1a0dcce5f9513c1bd270bceb9f0b77a9e5560e7ea0021bbe0'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1977f8b6-044e-49cb-917a-84ff9dcce2a7', node_type=<ObjectType.TEXT: '1'>, metadata={'Title of this paper': 'Multi-scale color local binary patterns for visual object classes recognition', 'Journal it was published in:': 'PeerJ Computer Science', 'URL': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/'}, hash='359f40c10602b35d5dfd3154d7f1162a39ad7a962b52049005f5a52ee0f50293'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='4f8117e2-f2a4-4c2e-b7d2-89de78f5e88d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='9ea91293fca8cd8f6d5ef8643f47b5031033d3d188ababab24c501856c65d623')}, hash='5cb494dec41038953ae2616fce8c2a52606d81fed58ffeb8b27cb94f29f5b66c', text='Rectum cancer is another type of GI tract cancer that is the 8th most dangerous cancer type in terms of number of cancer patients in the last five years\\xa0( Sung et al., 2021 Sung et al., 2021 Riegler et al., 2017 Pogorelov et al., 2018 Suzuki, 2012 Doi, 2007 There are many techniques to analyze GI tract images or videos using computer-aided diagnostic (CAD) systems. Most CAD systems primarily rely on machine learning and image processing techniques. The systems working on the machine learning algorithms on the texture and color features of the images are faster in detection. However, the detection score of such systems is lower than computer vision-based systems. The deep learning-based systems are sound in detection with a slow detection speed. There is a need for a system that offers a high detection speed in real-time and a justifiable detection accuracy. This research presents a three-step model for detecting abnormalities in the GI tract images. The first step relies on data preprocessing to address two critical challenges in the GI tract datasets. The first challenge is of light reflection on the images due to endoscopic procedures which is mitigated by applying the Open-CV Telea method. The second challenge in the endoscopic datasets is the high-class imbalance, which is mitigated with the generation of new images for all the classes so that the argument and original images combined make the dataset balanced. The following research step is to extract the most suitable features to represent images better. For this purpose, various features used in the literature have been explored, and a set of best features is selected after applying various feature combinations. The misleading features are excluded using feature accuracies and diversities. The third step of the research is to compute classes for an instance, which has further been divided in two steps. In the first part, a neural network is used to compute the probability of each class for an image and then to select the class based on different thresholds for each class instead of a 0.5 threshold or max value. The threshold for each class is computed by applying a genetic algorithm with the random initial threshold for all the classes.', start_char_idx=3034, end_char_idx=5266, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None)], metadata={'dbeb39e4-81cd-446b-af06-6d003b3866d0': {'Title of this paper': 'Multi-scale color local binary patterns for visual object classes recognition', 'Journal it was published in:': 'PeerJ Computer Science', 'URL': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/'}, '1977f8b6-044e-49cb-917a-84ff9dcce2a7': {'Title of this paper': 'Multi-scale color local binary patterns for visual object classes recognition', 'Journal it was published in:': 'PeerJ Computer Science', 'URL': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/'}, 'ee28374e-db09-44fa-aec6-d1cffc65d5b3': {'Title of this paper': 'Multi-scale color local binary patterns for visual object classes recognition', 'Journal it was published in:': 'PeerJ Computer Science', 'URL': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10773696/'}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# response = await  response_synthesizer.aget_response(query, [n.text for n in nodes[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mAccumulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes_with_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m response\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/base.py:146\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    144\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    145\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 146\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetadataMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    155\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/accumulate.py:89\u001b[0m, in \u001b[0;36mAccumulate.get_response\u001b[0;34m(self, query_str, text_chunks, separator, **response_kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to stream in Accumulate response mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_give_responses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_async\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext_chunks\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     96\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_list(tasks)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/accumulate.py:90\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to stream in Accumulate response mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_give_responses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_async\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks\n\u001b[1;32m     94\u001b[0m ]\n\u001b[1;32m     96\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_list(tasks)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/accumulate.py:125\u001b[0m, in \u001b[0;36mAccumulate._give_responses\u001b[0;34m(self, query_str, text_chunk, use_async, **response_kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mapredict\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m use_async\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mpredict\n\u001b[1;32m    123\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_qa_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_text_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcur_text_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext_chunks\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mastructured_predict\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m use_async\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mstructured_predict\n\u001b[1;32m    138\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/accumulate.py:126\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mapredict\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m use_async\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mpredict\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 126\u001b[0m         \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_qa_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_text_chunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m cur_text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks\n\u001b[1;32m    132\u001b[0m     ]\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mastructured_predict\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m use_async\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mstructured_predict\n\u001b[1;32m    138\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/llms/llm.py:220\u001b[0m, in \u001b[0;36mLLM.predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mis_chat_model:\n\u001b[1;32m    219\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_messages(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[0;32m--> 220\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     output \u001b[38;5;241m=\u001b[39m chat_response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/llms/base.py:97\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wrapper_logic(_self) \u001b[38;5;28;01mas\u001b[39;00m callback_manager:\n\u001b[1;32m     89\u001b[0m     event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m     90\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m     91\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m         },\n\u001b[1;32m     96\u001b[0m     )\n\u001b[0;32m---> 97\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/llms/openai.py:234\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     chat_fn \u001b[38;5;241m=\u001b[39m completion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_complete)\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchat_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/llms/openai.py:289\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client()\n\u001b[1;32m    288\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[0;32m--> 289\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m openai_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    295\u001b[0m message \u001b[38;5;241m=\u001b[39m from_openai_message(openai_message)\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:272\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:645\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    644\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/openai/_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1076\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1084\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1085\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1086\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1087\u001b[0m     )\n\u001b[0;32m-> 1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/openai/_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/openai/_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(request)\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 877\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_auth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpx/_client.py:901\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    893\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    899\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 901\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpx/_client.py:929\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    926\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 929\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpx/_client.py:966\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    964\u001b[0m     hook(request)\n\u001b[0;32m--> 966\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpx/_client.py:1002\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1002\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1006\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:228\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    216\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    217\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    226\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 228\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    233\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    234\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    235\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    236\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    237\u001b[0m )\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/ssl.py:1263\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1261\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/ssl.py:1136\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "response = Accumulate().synthesize(query, nodes_with_scores)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = Accumulate(\n",
    "    verbose=True, text_qa_template=qa_prompt, output_cls=Biography\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = Accumulate().aget_response(query, nodes_with_scores)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Getting Chunk Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_title_summary_results(results):\n",
    "  out = []\n",
    "  for e in results:\n",
    "    e = e.replace('\\n', '')\n",
    "    if '|' in e:\n",
    "      processed = {'title': e.split('|')[0],\n",
    "                    'summary': e.split('|')[1][1:]\n",
    "                    }\n",
    "    elif ':' in e:\n",
    "      processed = {'title': e.split(':')[0],\n",
    "                    'summary': e.split(':')[1][1:]\n",
    "                    }\n",
    "    elif '-' in e:\n",
    "      processed = {'title': e.split('-')[0],\n",
    "                    'summary': e.split('-')[1][1:]\n",
    "                    }\n",
    "    else:\n",
    "      processed = {'title': '',\n",
    "                    'summary': e\n",
    "                    }\n",
    "    out.append(processed)\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import TextNode\n",
    "from typing import Any, Dict, Generator, List, Optional, Sequence, Union\n",
    "\n",
    "def summarize_stage_1(nodes: List[TextNode]):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Summary...\n",
    "  e.g. \n",
    "  Why Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\n",
    "\n",
    "  TITLE AND CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = OpenAI(temperature=0)\n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': n.text} for n in nodes]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_summary_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2024-01-10 09:24:17.280194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/ml-learning/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAI instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 done time 2024-01-10 09:25:11.418623\n"
     ]
    }
   ],
   "source": [
    "# Run Stage 1 Summarizing\n",
    "stage_1_outputs = summarize_stage_1(nodes)['stage_1_outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stage 1 using llamaindex prompt synthesizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/examples/response_synthesizers/custom_prompt_synthesizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response_synthesizers import TreeSummarize, Refine\n",
    "from llama_index.types import BaseModel\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with pydantic model\n",
    "class Summaries(BaseModel):\n",
    "    \"\"\"Data model for a biography.\"\"\"\n",
    "\n",
    "    title: str\n",
    "    text: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import PromptTemplate\n",
    "\n",
    "qa_prompt_tmpl = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"Please also write the answer in the style of {tone_name}.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt = PromptTemplate(qa_prompt_tmpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = TreeSummarize(\n",
    "    verbose=True, summary_template=qa_prompt, output_cls=Biography\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = summarizer.get_response(\n",
    "    \"what is a phage?\", [text], tone_name=\"a business memo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_1_llama(nodes: List[TextNode]):\n",
    "  \n",
    "  print(f'Start time: {datetime.now()}')\n",
    "\n",
    "  # Prompt to get title and summary for each chunk\n",
    "  map_prompt_template = \"\"\"Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\n",
    "  {text}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Summary...\n",
    "  e.g. \n",
    "  Why Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\n",
    "\n",
    "  TITLE AND CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  # Define the LLMs\n",
    "  map_llm = OpenAI(temperature=0)\n",
    "  map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "  map_llm_chain_input = [{'text': n.text} for n in nodes]\n",
    "  # Run the input through the LLM chain (works in parallel)\n",
    "  map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "  stage_1_outputs = parse_title_summary_results([e['text'] for e in map_llm_chain_results])\n",
    "\n",
    "  print(f'Stage 1 done time {datetime.now()}')\n",
    "\n",
    "  return {\n",
    "    'stage_1_outputs': stage_1_outputs\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage_1_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Title ',\n",
       "  'summary': 'Real-Time Abnormalities Detection in Gastrointestinal Tract Using Artificial IntelligenceSummary '},\n",
       " {'title': 'Real-Time Endoscopic Abnormalities Detection System ',\n",
       "  'summary': 'This research presents a system that combines handcrafted and deep features to detect abnormalities and landmarks in real-time endoscopic images. The system uses a genetic algorithm to learn the detection threshold for classes with small inter-class differences. It has been evaluated on benchmark datasets and achieved high accuracy and F1-score. The system has a detection speed of 41 frames per second and was funded by the Higher Education Commission of Pakistan. GI tract cancer is a significant contributor to mortality, and this system can aid in early detection and diagnosis through computer-aided diagnostic techniques.'},\n",
       " {'title': 'Rectum Cancer and Computer-Aided Detection ',\n",
       "  'summary': 'This research presents a three-step model for detecting abnormalities in GI tract images using computer-aided diagnostic (CAD) systems. The first step addresses challenges in the datasets, such as light reflection and class imbalance, while the second step focuses on selecting the most suitable features. The third step uses a neural network and genetic algorithm to compute classes for an image. This model aims to offer a high detection speed and justifiable accuracy for detecting rectum cancer, which is the 8th most dangerous cancer type in the GI tract.'},\n",
       " {'title': 'Improving Endoscopy Detection Accuracy with LiRE-CNN ',\n",
       "  'summary': 'LiRE-CNN is a methodology that combines Lire features and a neural network to improve the accuracy and speed of endoscopy detection. By using a genetic algorithm to compute thresholds for each class, LiRE-CNN achieves a high accuracy of 0.99 with a detection speed of 41 frames per second. The algorithm also performs well on benchmark datasets with and without preprocessing. The article discusses related work, presents the proposed approach, describes the experimental set-up, and concludes with future directions for this research.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Improving Detection Accuracy in Endoscopy: A Proposed MethodologySummary '},\n",
       " {'title': 'Data Augmentation Techniques for Abnormality Detection in GI Tract Images ',\n",
       "  'summary': 'This text discusses various data augmentation techniques used for the detection of abnormalities in gastrointestinal (GI) tract images. These techniques include generative adversarial networks (GANs), angular flips, and adding noise. The detection of abnormalities is done through deep learning and machine learning approaches, with some authors using a combination of both. The use of deep neural networks as feature extractors and support vector machines (SVMs) for classification is also explored. The text highlights the importance of using image features for accurate detection of polyps, cancer, or lesions in the GI tract.'},\n",
       " {'title': '\"Analysis of Approaches for Abnormality Detection and Classification on Benchmark Datasets\" ',\n",
       "  'summary': 'This study examines various methods for detecting and classifying abnormalities on benchmark datasets. The evaluation measures used include sensitivity, specificity, precision, recall, F1-score, accuracy, ROC, and AUC. The best scores for each measure are reported for datasets such as CE Bleeding Dataset, WCE-279 Bleeding Dataset, and Polyp2007. The contributions from different studies, including Zhao et al. (2021), Deeba et al. (2018), and Alexandre, Nobre & Casteleiro (2008), are compared and analyzed. This research provides valuable insights into the effectiveness of different approaches for detecting and classifying abnormalities, which can aid in the development of more accurate and efficient methods for medical diagnosis.'},\n",
       " {'title': '\"Advancements in Abnormality Classification and Landmark Detection in GI Tract Imaging\" ',\n",
       "  'summary': 'This article discusses various approaches and their effectiveness in accurately classifying abnormalities and detecting landmarks in GI tract imaging. These abnormalities can include polyps, ulcers, lesions, esophagitis, and cancer, and precise information about their location is crucial for diagnosis and treatment. The article analyzes multiple methods, such as Adaptive Ensembles and FAST RCNN, and their performance on the Kvasir V2 dataset. The results show high accuracy and F1-scores, indicating the potential of these approaches in improving the accuracy and efficiency of GI tract imaging analysis.'},\n",
       " {'title': '\"Improving Polyp Detection in Endoscopic Images ',\n",
       "  'summary': 'Researchers use various techniques to enhance the performance of polyp detection in endoscopic images, including combining datasets, using different neural networks, and implementing majority voting and weighted discriminant embedding methods. These methods have shown promising results, with some achieving high accuracy rates of 0.99 and 0.95. The Kvasir V2 dataset, introduced by Pogorelov et al. in 2018, is commonly used in these studies. Overall, these advancements in artificial intelligence and image processing have the potential to greatly improve the accuracy and efficiency of polyp detection in endoscopic procedures.\"'},\n",
       " {'title': '\"Improving Abnormality and Anatomical Landmark Detection in Endoscopic Images ',\n",
       "  'summary': 'This text discusses various approaches and a proposed methodology for detecting abnormalities and landmarks in endoscopic images. These approaches include using deep learning models such as Inception Res-Net and DenseNet, as well as handcrafted feature extraction and genetic algorithms for threshold detection. The proposed methodology consists of seven main components, including data preprocessing and augmentation to remove reflections from the images. This is crucial in ensuring accurate analysis and detection of abnormalities and landmarks, as demonstrated in the sample set of polyps picture provided.\"'},\n",
       " {'title': 'Removing Reflections in Endoscopic Images ',\n",
       "  'summary': 'Reflections in endoscopic images can cause errors in image analysis and affect detection of abnormalities and landmarks. Various methods, such as data augmentation, handcrafted and deep feature extraction, feature combination and selection, network-based classification, and genetic algorithm for threshold detection, can be used to remove these reflections. These methods can be performed with or without human intervention, and involve procedures such as image cropping and supervised or unsupervised detection. Equations are also used to aid in the removal of reflections.'},\n",
       " {'title': ' Image Thresholding for Edge Detection ',\n",
       "  'summary': 'This text discusses a method for detecting edges in an image using a thresholding technique. The algorithm assigns a value of 255 to pixels with a high intensity and a value of 0 to pixels with a low intensity, based on a threshold of 180. This allows for the identification of edges in the image, which can be useful for various applications such as image segmentation and feature extraction.'},\n",
       " {'title': 'Data Augmentation for Endoscopic Procedures ',\n",
       "  'summary': 'This article discusses the use of data augmentation techniques to address the class imbalance problem in endoscopic procedure datasets. These datasets often contain a few common abnormalities and a large number of rare ones, leading to imbalanced data. The authors propose using data preprocessing and augmentation methods to increase the representation of rare abnormalities and improve the performance of machine learning algorithms. This approach has the potential to improve the accuracy and effectiveness of endoscopic procedures, ultimately benefiting patients and medical professionals.'},\n",
       " {'title': 'Data Augmentation Techniques for Imbalanced Endoscopic Datasets ',\n",
       "  'summary': 'This text discusses the issue of class imbalance in endoscopic datasets and the common approaches for data augmentation to address this problem. These techniques include using generative models, rotating, flipping, cropping, resizing, and noise injection. The image manipulation is done as a post-step of reflection removal and a pre-step to feature extraction and fine-tuning of the neural network. These techniques can help improve the performance of models trained on imbalanced datasets, such as the Kvasir V2 dataset which has a significant class imbalance.'},\n",
       " {'title': 'Texture Feature Extraction for Image Manipulation Detection ',\n",
       "  'summary': 'This research explores the use of texture features, specifically those available in Lucene Image Retrieval (LIRE), as effective deciders for detecting image manipulation. By applying a decision tree classifier on various sets of features, the average detection F1-score was found to be above 0.84. The methodology of the research involves using count(C) and count(Ci) to generate a set of features for the classifier. This study highlights the potential of texture features in accurately detecting image manipulation, which can have important implications in fields such as forensics and security.'},\n",
       " {'title': 'Texture Features in Image Retrieval ',\n",
       "  'summary': 'This text discusses the various texture features available in Lucene Image Retrieval (LIRE) and their use in detecting abnormalities and localizing and segmenting images. These features include color layout, edge histogram, Tamura, Color and Edge Directivity Descriptor (CEDD), Fuzzy color and texture histogram (FCTH), and color histograms (HSV and RGB). The methodology of the research is also outlined, with references to previous studies and their findings. These texture features play a crucial role in image retrieval and can aid in the accurate analysis and interpretation of images.'},\n",
       " {'title': '  \"Exploring Image Feature Extraction Techniques\" ',\n",
       "  'summary': 'This text discusses various techniques for extracting features from images, including Gabor features, auto color correlation, and auto color correlogram. These techniques are used to analyze and compare images based on their pixel values. The authors also mention previous studies that have utilized these techniques, such as Mehrotra, Namuduri & Ranganathan (1992) and Huang et al. (1997).'},\n",
       " {'title': '   \"Calculating Distance and Accuracy in Data Analysis\" ',\n",
       "  'summary': 'This text discusses the use of mathematical equations and functions to calculate the accuracy and distance between data points in data analysis. It introduces the concept of ACC, which measures the probability of two data points being in the same category and at a certain distance from each other. It also explains the use of the equation d(P,Q) to determine the maximum distance between two data points in terms of their x and y coordinates. This text provides a brief overview of the mathematical tools used in data analysis.'},\n",
       " {'title': ' Feature Extraction Techniques in Computer Vision ',\n",
       "  'summary': 'This text discusses three popular feature extraction techniques in computer vision: Speeded up robust features (SURF), Pyramid Histogram of Oriented Gradients (PHOG), and Local Binary Patterns (LBP). These techniques are used to extract important information from images and are widely used in various computer vision applications. The text also mentions other related techniques such as Local Ternary Patterns (LTP).'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Gray-Level Co-Occurrence Matrix (GLCM): Haralick & Shanmugam, 1973Summary '},\n",
       " {'title': '   Gray-Level Co-Occurrence Matrix (GLCM) ',\n",
       "  'summary': 'GLCM is a method used in image processing to analyze the spatial relationship between pixels of different gray levels. It counts the occurrences of pixel pairs at a certain distance and direction, providing information about the texture and patterns in an image. This technique was first introduced by Haralick and Shanmugam in 1973 and has since been widely used in various applications such as texture classification and feature extraction. GLCM is a powerful tool for understanding the structure of an image and has been continuously improved and adapted for different purposes in the field of computer vision.'},\n",
       " {'title': '  \"Haralick Features for Abnormality Detection in Endoscopy and Colonoscopy Images\" ',\n",
       "  'summary': 'This text discusses the use of Haralick features, a type of statistical analysis, in detecting abnormalities in medical images. These features are calculated using a GLCM (Gray-Level Co-occurrence Matrix) and can provide valuable information for identifying abnormalities in endoscopy and colonoscopy images. Various CNN-based approaches are commonly used in conjunction with Haralick features to improve the accuracy of abnormality detection in these medical images.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Achieving High Detection Accuracies with Dense NetSummary '},\n",
       " {'title': 'Deep Feature Extraction Methodology for Image Classification ',\n",
       "  'summary': 'This text discusses a methodology for extracting deep features from images for classification purposes. The approach involves using a voting classifier with a combination of various features, resulting in good detection and optimal classification time. The final set of features that yielded the best accuracies include Auto Color Correlogram, Color Layout, Edge Histogram, Gabor Features, JCD, Color and Edge Directivity Descriptor, Fuzzy Color and Texture Histogram, Tamura Features, Local Binary Patterns, and Deep Features. The use of neural networks as classification networks is also discussed, highlighting their effectiveness in decision-making when sufficient data is available.'},\n",
       " {'title': '\"Understanding Relu and Sigmoid Activation Functions\" ',\n",
       "  'summary': 'Relu and sigmoid are two commonly used activation functions in neural networks. Relu is a simple function that outputs the maximum of 0 and the input value, while sigmoid is a more complex function that outputs a value between 0 and 1. These functions are used to introduce non-linearity in neural networks and are particularly useful in the final output layer for classification tasks.'},\n",
       " {'title': 'Optimizing Neural Networks with Nadam ',\n",
       "  'summary': 'Various optimizers were evaluated for neural networks, and Nadam was found to have good weight adjustment and fast convergence. Nadam is shown in Eq. (17) as a combination of different parameters, including learning rate and momentum.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Gradient Descent Algorithm for Machine LearningSummary '},\n",
       " {'title': ' Improving Detection Accuracy with Neural Networks and Genetic Algorithms ',\n",
       "  'summary': 'This text discusses the use of a neural network-based approach and genetic algorithm for threshold detection in medical image classification. The approach resulted in a boost in F1-score and Matthews Correlation Coefficient, improving the accuracy of detection. The genetic algorithm was used to learn thresholds for each class, taking into account the differences in intra-class and inter-class variations. The crossover operator employed in the algorithm ensures that the resulting number lies within the range of 0 to 1. This approach has potential for improving detection accuracy in medical imaging.'},\n",
       " {'title': '   The Crossover Operator in GA-Boost ',\n",
       "  'summary': 'This text discusses the use of a specific crossover operator in GA-Boost, which involves removing the integral part of a floating-point number to ensure it falls within a specific range. The text also provides details on the experimental setup used for analyzing endoscopic and colonoscopic abnormalities, including datasets and evaluation measures.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Overview of Endoscopic Image DatasetsSummary '},\n",
       " {'title': 'Evaluation Measures for Segmentation in Medical Imaging ',\n",
       "  'summary': 'This text discusses various evaluation measures used in medical imaging research, including accuracy, F1-score, Matthews Correlation Coefficient, and sensitivity. The DowPK dataset is highlighted as having the highest sensitivity value among evaluated datasets, reaching an impressive value of 0.93.'},\n",
       " {'title': 'Evaluation Measures for Segmentation in DowPK Dataset ',\n",
       "  'summary': \"This text discusses various evaluation measures used to assess the performance of segmentation in the DowPK dataset. These measures include accuracy, F1-score, Matthews Correlation Coefficient, sensitivity, specificity, and area under the receiver operating characteristic curve. The DowPK dataset demonstrated high values for sensitivity and specificity, indicating the model's proficiency in correctly identifying positive and negative instances. The Kvasir V2 and Hyper Kvasir datasets also showed excellent performance in distinguishing between classes, with a high AUC-ROC value of 0.99. The minimum possible value for the MCC is -1, indicating all false results, while a value of 1 indicates all correct classifications.\"},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Evaluating Classification Methods for Abnormalities and Landmarks Detection on Medical DatasetsSummary '},\n",
       " {'title': 'Comparing Methods for Classification of Abnormalities and Landmarks Detection ',\n",
       "  'summary': 'This section discusses different variations of proposed methods and compares their performance on benchmark datasets. The comparison is shown in Tables 4, 5, 6 of the article \"Analysis of some of the approaches for classification of abnormalities and landmarks detection on the Kvasir V1 dataset\" and \"Analysis of various approaches for classification of abnormalities and landmarks detection on the Kvasir V2 dataset\". The tables show the accuracy, F1-score, MCC, and FPS of each approach, with the proposed Lire-CNN achieving the highest scores on the Kvasir V1 dataset and the Kvasir V2 dataset. '},\n",
       " {'title': 'Performance Comparison of Abnormalities and Landmarks Detection Approaches on Kvasir V3 Dataset ',\n",
       "  'summary': 'This table presents the results of various approaches for classifying abnormalities and detecting landmarks on the Kvasir V3 (Hyper Kvasir) dataset. The approaches include EEIC19, TLCSS18, FRCSS18, DSTL18, WDE18, MVHC18, and the proposed Lire-CNN. The results show that the proposed approach has the highest accuracy, F1-score, and Matthews correlation coefficient (MCC) among all the approaches. It also has a relatively low frames per second (FPS) rate, indicating its efficiency. This information can be useful for researchers and practitioners in the field of medical image analysis.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Impact of Different Approaches on Abnormalities Detection in Medical ImagesSummary '},\n",
       " {'title': 'The Impact of Reflection Removal on Abnormality Detection ',\n",
       "  'summary': 'Reflection removal techniques were applied to multiple datasets, including Kvasir V1, V2, and V3, resulting in varying levels of improvement in F1-scores. The number of images and prevalence of reflections within a dataset were found to influence the effectiveness of reflection removal. Datasets with a higher number of images and prevalence of reflections experienced a more significant impact from reflection removal techniques.'},\n",
       " {'title': 'Impact of Reflection Removal and Data Augmentation on Image Detection ',\n",
       "  'summary': 'This study examines the impact of reflection removal and data augmentation techniques on image detection. The number of images per class and the amount of reflections present were found to influence the effectiveness of reflection removal. Data augmentation was also applied to address class imbalance, resulting in significant improvements in detection for datasets with a higher prevalence of reflections. Additionally, the use of deep features with texture-based features was found to greatly improve accuracy and F1-score on the Kvasir V2 dataset. These findings highlight the importance of considering factors such as image class size and reflection prevalence when implementing image detection techniques.'},\n",
       " {'title': '\"Feature Selection for Improved Accuracy in Medical Image Classification ',\n",
       "  'summary': 'Researchers use a combination of deep and texture-based features to achieve high accuracy in classifying medical images. By applying this approach to the Kvasir V2 dataset, they achieved an accuracy of 0.99 and an F1-Score of 0.86. However, certain features such as Color Histogram and Auto Color Correlation were found to be misleading. The features used for selection include Auto Color Correlogram, Color Layout, Edge Histogram, Gabor, JCD, PHOG, Tamura, LBP, and a fine-tuned Light Weight Network. This study highlights the importance of carefully selecting features for accurate medical image classification.\"'},\n",
       " {'title': 'Improving Classification Accuracy with GA-Boost Algorithm ',\n",
       "  'summary': 'This text discusses the use of various features and neural classifiers for image classification, resulting in a commendable F1-score of 0.89 for the Kvasir V1 dataset. However, significant variations in probability ranges for different classes highlight the need for tailored class thresholds. To address this, a genetic algorithm named GA-Boost was employed, allowing for the precise adaptation of thresholds for each class and improving the overall classification accuracy.'},\n",
       " {'title': 'Optimizing Classification Thresholds with GA-Boost Algorithm ',\n",
       "  'summary': 'The GA-Boost algorithm was used to generate optimal thresholds for different classes in a dataset, taking into account variations in probability distributions. This allowed for more precise adaptation of thresholds based on the specific characteristics of each class, resulting in more effective classification. The algorithm utilized a genetic algorithm and an additive crossover operator to learn the thresholds for each class. The F1-score evaluation measure was used to select the best chromosomes, and the results showed a significant impact on the F1-score when applying various additions in the procedure on different datasets.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'The Impact of Crossover Operator and Feature Selection on GA-Boost Algorithm PerformanceSummary '},\n",
       " {'title': '\"Efficiency and Accuracy of LiRE-CNN for Real-Time Applications\" ',\n",
       "  'summary': 'This study evaluated the performance of LiRE-CNN, a deep learning algorithm, on various datasets for abnormalities and landmark detection. The results showed high accuracy and impressive detection speed of 41 frames per second (FPS), making it efficient for real-time applications. The addition of certain algorithmic steps significantly improved the overall F1-score, with varying effects on different datasets. The removal of reflections and use of different thresholds also led to improved detection accuracy. These findings demonstrate the effectiveness of a tailored approach for each dataset in enhancing the performance of LiRE-CNN.'},\n",
       " {'title': 'Improving Polyp Detection in Medical Images ',\n",
       "  'summary': 'This study analyzed the effectiveness of a tailored approach for improving polyp detection in medical images. The results showed that removing reflections and using different thresholds for all classes led to better detection and reduced misclassification. However, there were still challenges due to class imbalance and difficulty in differentiating similar classes. The study also highlighted the impact of data augmentation and the need for more samples in certain classes. Overall, the findings suggest that a tailored approach can improve polyp detection, but further research is needed to address other challenges in medical image analysis.'},\n",
       " {'title': 'Challenges in Detecting GI Tract Abnormalities ',\n",
       "  'summary': 'The Kvasir V2 dataset contains classes with very few samples, making it difficult to differentiate between similar images. The proposed LiRE-CNN approach is compared to other state-of-the-art methods for detecting GI tract abnormalities, with a focus on the challenging classes of dyed lifted polyps and dyed resection margins, as well as the misclassification between esophagitis and normal z-line. The results show that the LiRE-CNN approach outperforms other methods and can potentially improve detection accuracy for these challenging classes.'},\n",
       " {'title': 'Comparative Analysis of Approaches for GI Tract Abnormality Detection ',\n",
       "  'summary': 'This study evaluates various methods for detecting abnormalities and landmarks in the GI tract, with a focus on reflection elimination. The most successful approaches were found to be unsupervised methods, which align with dataset requirements and task objectives. The issue of class imbalance was also addressed effectively. Detecting landmarks and abnormalities can aid in the treatment of diseases caused by these irregularities, such as polyps or cancer, by accurately pinpointing and localizing the affected area. '},\n",
       " {'title': 'Preprocessing Techniques for Endoscopic Abnormality Detection ',\n",
       "  'summary': 'This research explores the use of supervised and unsupervised preprocessing methods for detecting abnormalities and landmarks in endoscopic images. The study found that unsupervised methods were most effective in aligning with dataset requirements and task objectives. Additionally, the challenge of class imbalance was addressed, which can aid in the treatment of diseases caused by these abnormalities. The future task is to localize and segment the abnormal regions, particularly in cases of polyps or cancer. The code and datasets used in the research are available for further study.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'DowPK: A Dataset for Endoscopic Polyp DetectionSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'HyperKvasir: A Comprehensive Dataset for Gastrointestinal EndoscopySummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Advances in Gastrointestinal Tract Disease DetectionSummary '},\n",
       " {'title': 'Performance Assessment of a Bleeding Detection Algorithm for Endoscopic Video ',\n",
       "  'summary': \"This text discusses a study that evaluated the performance of a bleeding detection algorithm for endoscopic videos. The algorithm used a classifier fusion method and exhaustive feature selection to accurately detect bleeding in endoscopic videos. The study compared the algorithm's performance to other methods and found it to be highly effective. The authors also discuss the potential applications of this algorithm in medical imaging and the future potential for computer-aided diagnosis. This text provides valuable insights into the use of artificial intelligence in medical imaging and its potential to improve diagnostic accuracy.\"},\n",
       " {'title': 'Advancements in Medical Image Analysis for Gastrointestinal Diseases ',\n",
       "  'summary': 'This text discusses various techniques and approaches for analyzing medical images of the colon and gastrointestinal tract. It covers topics such as quantitative measurement, feature identification, and the use of convolutional neural networks. The authors also explore the effectiveness of image augmentations and the use of genetic algorithms for optimizing neural networks. This information is relevant for the diagnosis and treatment of gastrointestinal diseases, such as cancer.'},\n",
       " {'title': 'Automatic Generation of Optimized Convolutional Neural Networks for Medical Image Classification ',\n",
       "  'summary': 'This text discusses the use of a genetic algorithm to automatically generate optimized convolutional neural networks for medical image classification. The authors draw on previous research in the field, including the use of generative adversarial nets and deep residual learning, to develop their approach. They also highlight the potential applications of this technology, such as automatic disease detection and report generation for gastrointestinal tract examination. The text also discusses the use of support vector machines in this context.'},\n",
       " {'title': 'Advancements in Artificial Intelligence for Disease Recognition in Gastrointestinal Endoscopy ',\n",
       "  'summary': 'This text discusses various methods and techniques for using artificial intelligence, specifically deep learning and convolutional neural networks, to improve disease recognition in gastrointestinal endoscopy. These methods include hybrid loss with network trimming, support vector machines, domain specific transfer learning, residual networks, and data augmentation. The text also mentions the use of mobile vision applications and densely connected convolutional networks. These advancements have the potential to greatly improve the accuracy and efficiency of disease detection in endoscopy procedures.'},\n",
       " {'title': 'Advancements in Gastrointestinal Endoscopy Imaging ',\n",
       "  'summary': 'This text discusses various studies and methods for automatic detection of gastrointestinal abnormalities in endoscopy videos. It covers topics such as densely connected convolutional networks, image indexing using color correlograms, and polyp detection using elliptical shape features. It also includes an intelligent system for automatic detection of gastrointestinal adenomas. The text highlights the importance of using advanced classification methods, such as artificial intelligence, in improving the accuracy and efficiency of detecting abnormalities in endoscopy imaging.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Advancements in Polyp Segmentation and Covid-19 Tissue Classification in Medical ImagingSummary '},\n",
       " {'title': 'Medical Image Classification Techniques ',\n",
       "  'summary': 'This text discusses various techniques for medical image classification, including data bagging, majority voting of heterogeneous classifiers, and weighted discriminant embedding. It also explores the use of preprocessing as a tool in medical image detection. The text references several studies and conferences, such as MediaEval and the International Conference on Pattern Recognition, and discusses the use of deep convolutional neural networks and dominant local binary patterns for texture classification. These techniques have the potential to improve medical diagnosis and treatment by accurately identifying abnormalities in medical images.'},\n",
       " {'title': 'Medical Image Classification and Retrieval Techniques ',\n",
       "  'summary': 'This text discusses various techniques for medical image classification and retrieval, including the use of spatial adjacent histograms based on adaptive local binary patterns, adaptive ensembles, and Gabor filters. It also mentions open source image retrieval libraries and their applications in medical multimedia tasks. The text highlights the importance of accurate image classification and retrieval in the medical field and provides insights into the latest developments in this area.'},\n",
       " {'title': 'Advancements in Multimedia Technology for Gastrointestinal Disease Detection ',\n",
       "  'summary': 'This text discusses various studies and challenges related to using multimedia technology for detecting abnormalities in the gastrointestinal tract. These include using data enhancement techniques to address sample imbalance, ensemble of texture features, and an inception-like CNN architecture. The studies also highlight the importance of high-quality video datasets for training and testing these systems. These advancements have the potential to improve the accuracy and efficiency of detecting gastrointestinal diseases, ultimately leading to better patient outcomes.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Advancements in Multimedia for Gastrointestinal Disease DetectionSummary '},\n",
       " {'title': 'Advancements in Multimedia and Image Recognition ',\n",
       "  'summary': 'This text discusses various research studies and conferences related to multimedia and image recognition, including the use of deep learning and data augmentation techniques. It highlights the importance of these technologies in the medical field and their potential for improving image classification. The text also mentions the development of new methods, such as Mobilenetv2 and the MPEG-7 visual standard, for more efficient and accurate image recognition. Overall, the text showcases the ongoing efforts and progress in the field of multimedia and image recognition, and their potential impact on various industries.'},\n",
       " {'title': 'Global Cancer Statistics and the Role of Computer-Aided Diagnosis ',\n",
       "  'summary': 'This text discusses the latest global cancer statistics for 2020, including estimates of incidence and mortality for 36 different types of cancer in 185 countries. It also explores the use of computer-aided diagnosis in thoracic and colonic imaging, specifically in the context of cancer detection. The text reviews various studies and techniques, such as deep transfer learning models, that have been used to improve the accuracy and efficiency of cancer diagnosis. This information is crucial for understanding the current state of cancer worldwide and the potential impact of technology in improving detection and treatment.'},\n",
       " {'title': 'Automatic Polyp Detection Using Global Geometric Constraints and Local Intensity Variation Patterns ',\n",
       "  'summary': 'This text discusses a method for automatically detecting polyps in medical images using a combination of global geometric constraints and local intensity variation patterns. The authors propose a deep transfer learning model called CovidSORT for detecting COVID-19 in chest X-ray images. They also explore the use of textural features and enhanced local texture feature sets for face recognition and inpainting techniques based on the fast marching method. Additionally, they introduce a dual decoder attention network for automatic polyp segmentation and discuss the visualization and understanding of convolutional networks.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Recent Advances in Automatic Polyp Segmentation for Medical ImagingSummary '},\n",
       " {'title': 'External Viewer Error: Empty Response ',\n",
       "  'summary': 'This error occurs when an external viewer fails to load due to a timeout, resulting in an empty response with zero bytes read. This can be caused by a slow or unresponsive server, network connectivity issues, or incorrect configuration settings. It can prevent users from accessing important information and may require troubleshooting to resolve.'},\n",
       " {'title': 'Understanding Methicillin-Resistant Staphylococcus Aureus (MRSA) ',\n",
       "  'summary': 'This article discusses the pathogenic bacteria Staphylococcus aureus and its methicillin-resistant strain, MRSA. It explores the impact of MRSA on human health, including its role in sepsis and cytotoxicity. The article also delves into the genetic mutations and molecular biology techniques used to study and combat MRSA, such as DNA construction and plasmid construction. Overall, this article provides a comprehensive overview of the biology and medical implications of MRSA, shedding light on the urgent need for continued research and effective treatment methods.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'The Role of Staphylococcus aureus Lipoproteins in Toxin ProductionSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'The Role of CamS in Repressing Toxin Production in Staphylococcus aureusSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'The Role of CamS Protein in Repressing S. aureusSummary '},\n",
       " {'title': '   Regulation of Toxins in Staphylococcus aureus and Enterococcus faecalis ',\n",
       "  'summary': 'This text discusses the regulation of toxins in Staphylococcus aureus and Enterococcus faecalis through the use of transcriptional reporters. The construction of promoter fusion plasmids was based on predicted transcription start sites, and the regulation of distinct toxins was validated. The study found that the toxins hla and hlgCB were regulated by the presence of certain genes, such as camS and saePQRS. This information can be useful in understanding the mechanisms of toxin regulation in these bacteria and potentially developing treatments for infections caused by them.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Construction and Regulation of Toxins in Staphylococcus aureusSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'CamS-Mediated Toxin Regulation in S. aureusSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'The Role of CamS Protein in S. aureus Virulence Factor RegulationSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Rabbit Erythrocytes Exhibit Greater Sensitivity to α-Toxin Exposure Compared to Human ErythrocytesSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Experimental Methods for Studying Staphylococcus Aureus Infection in MiceSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Construction and Analysis of Promoter Fusion Plasmids in Staphylococcus aureusSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'RNA-seq Analysis of USA300 FPR3757 Reference GenomeSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Analysis of MRSA USA300 FPR3757 Gene Expression in Murine ModelsSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Statistical Tests and Supporting Data in a Study on Bacterial Strains and PlasmidsSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Bacterial Strains and Plasmids Used in Staphylococcus Aureus StudySummary '},\n",
       " {'title': 'The Role of Nasal Carriage in Staphylococcus Aureus ',\n",
       "  'summary': 'This article discusses the importance of nasal carriage in the spread and virulence of Staphylococcus aureus, a common and potentially dangerous bacteria. It highlights the role of lipoproteins in the immune response and virulence of S. aureus, and identifies potential therapeutic targets for treating methicillin-resistant strains. The article also provides an overview of two-component systems in S. aureus and their impact on gene expression. Overall, this article sheds light on the complex mechanisms involved in the pathogenesis of S. aureus and the potential for targeted interventions to combat this bacteria.'},\n",
       " {'title': 'The Role of Lipoproteins in Gram-Positive Bacteria ',\n",
       "  'summary': 'This article discusses the abundance, function, and fitness of lipoproteins in Gram-positive bacteria, specifically focusing on Staphylococcus aureus. It also explores the role of lipoproteins in triggering host defense mechanisms through toll-like receptors and the differential immune responses they elicit in commensal and non-commensal staphylococci. Additionally, the article delves into the evolutionary pathways of enterococcal sex pheromones, which play a crucial role in complex, two-signal systems. Overall, this article sheds light on the importance of lipoproteins in the biology and pathogenesis of Gram-positive bacteria.'},\n",
       " {'title': 'The Evolution of Complex Pheromone Systems in Bacterial Pathogens ',\n",
       "  'summary': 'This article discusses the evolutionary pathways of two-signal systems in bacterial pathogens, specifically focusing on enterococcal and staphylococcal species. It also explores the processing, export, and identification of novel linear peptides from Staphylococcus aureus, as well as the identification of a peptide-pheromone that enhances Listeria monocytogenes. The article also touches on the role of these pheromone systems in community-associated methicillin-resistant Staphylococcus aureus (MRSA) and provides a refined protocol for mapping transcription start sites in bacterial pathogens.'},\n",
       " {'title': 'Refined Protocol for Mapping Transcription Start Sites in Bacterial Pathogens ',\n",
       "  'summary': 'This article discusses a new protocol for mapping transcription start sites in bacterial pathogens, which aims to provide a more complete and less biased understanding of these sites. The protocol was developed by Giraudo et al. and published in BMC Genomics in 2016. It involves the use of the sae Staphylococcus aureus protein complex, which activates the phosphatase activity of the sensor kinase SaeS in the SaeRS two-component system. This system plays a crucial role in the pathogenesis of Staphylococcus aureus, and the protocol has been shown to accurately identify transcription start sites in this bacterium.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'The Role of Hemolysins in Staphylococcus aureus InfectionsSummary '},\n",
       " {'title': 'Targeting Staphylococcus aureus: Understanding the Role of Neutrophils and Bacterial Virulence ',\n",
       "  'summary': 'This text discusses recent research on the role of neutrophils in defending against Staphylococcus aureus infections, as well as the impact of bacterial virulence on the severity of MRSA sepsis. The studies highlighted in this text provide valuable insights into potential targets for treatment and prevention of S. aureus infections.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Prevalence and Epidemiological Data on Staphylococcus aureus and its Toxin GenesSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'The Role of Lipoprotein QseG in Controlling the Phosphorylation State of the Two-Component System in Staphylococcus aureusSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'The Role of Adherens Junctions in Staphylococcus aureus InfectionSummary '},\n",
       " {'title': '   The Role of Toxins in Promoting Staphylococcus aureus Infection ',\n",
       "  'summary': 'This text discusses the various methods and techniques used to study and manipulate Staphylococcus aureus, a common bacterial pathogen. It highlights the role of toxins and lipopolysaccharides in promoting infection and modulating neutrophil functions. The text also discusses genetic systems and resources used for rapid phenotype screening and transformation of Staphylococcus aureus and Staphylococcus epidermidis. It concludes with a protocol for efficient plasmid mutagenesis.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'A Protocol for Mutagenesis and its Impact on MRSA Virulence and Host DefenseSummary '},\n",
       " {'title': 'The Importance of Open-Source Platforms in Biological Image Analysis ',\n",
       "  'summary': 'This text discusses the significance of open-source platforms, such as Fiji, in the field of biological image analysis. It also mentions the role of the ubiquitous human skin commensal Staphylococcus hominis in mBio and the Kyoto Encyclopedia of Genes and Genomes (KEGG) in Nucleic Acids Research. These resources provide valuable information and tools for researchers in the biological sciences. The text highlights the potential for open-source platforms to enhance collaboration and advance scientific discoveries.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'A Novel In-Situ-Process Technique for Constructing Circular cpDNA LibrarySummary '},\n",
       " {'title': '   \"Advancements in Constructing High-Quality Circular Chloroplast DNA Libraries\" ',\n",
       "  'summary': 'This paper discusses a novel in-situ-process technique for efficiently extracting circular cpDNA from crops and constructing a high-quality cpDNA library. This technique combines in-situ chloroplast lysis and in-situ substitute/ligation to prevent large-insert cpDNA fragment breakage. The resulting goal-insert, ordered cpDNA library is suitable for various genome-wide functional analyses and characterizations of chloroplasts. This advancement has potential implications for genome sequencing, bioinformatics analysis, cloning, physical mapping, molecular phylogeny, and evolution.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'In-situ Lysis for Studying Chloroplast Genome ConformationsSummary '},\n",
       " {'title': '\"Efficient Extraction and Cloning of Circular cpDNA Using BAC Library and In-Situ Substitute/Ligation Protocol\" ',\n",
       "  'summary': 'This article discusses a new method for extracting and cloning high-quality circular cpDNA using a BAC library and an in-situ substitute/ligation protocol. This method allows for a more efficient and accurate extraction of cpDNA, reducing the number of colonies needed to represent a complete DNA library. The protocol also ensures the integrity of the cpDNA sequence within a length of 90.0 kb. This method has been successfully used to construct goal-insert, ordered cpDNA libraries, and can also be applied to other nucleic acid sequences. The materials and methods used in this experiment are listed in Table 1.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Techniques for Constructing cpDNA LibrariesSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Techniques for Extracting Circular cpDNA from Plant LeavesSummary '},\n",
       " {'title': 'Optimizing Chloroplast Isolation for Plant Samples ',\n",
       "  'summary': 'This text provides guidelines for optimizing chloroplast isolation from different plant samples. It includes information on the appropriate sampling stages for wheat, maize, soybean, and barley, as well as the recommended immersing times for each sample type. It also emphasizes the importance of using freshly made ET-M-D buffer and keeping samples on ice during all operations. The text cautions against using liquid nitrogen or dry ice for lyophilization and recommends filtering the disintegrating cell solution to eliminate non-target contaminants. Overall, these guidelines aim to ensure the immaculateness of equipment and reagents and to prevent the destruction of intact chloroplast membranes during homogenization.'},\n",
       " {'title': 'Optimizing Chloroplast Isolation for Accurate Results ',\n",
       "  'summary': 'This text provides guidelines for obtaining pure and intact chloroplasts for experiments. It emphasizes the importance of using immaculate equipment and pre-cooled reagents. The text also highlights the impact of grinding conditions on chloroplast isolation and recommends against homogenizing tissues in liquid nitrogen. To eliminate non-target contaminants, the text suggests filtering the disintegrating cell solution and withdrawing suspended chloroplasts using a serological pipette. It also provides instructions for in-gel chloroplast lysis and advises against exceeding the specified time limit to preserve intact chloroplasts. The text emphasizes the need for careful and gentle mixing to protect the chloroplast membrane.'},\n",
       " {'title': 'Optimizing Chloroplast Lysis for Circular cpDNA Extraction ',\n",
       "  'summary': 'This text provides guidelines for effectively lysing chloroplasts and extracting circular cpDNA. The experimenter is advised to carefully mix the chloroplast suspension and LMP agarose to avoid damaging the chloroplast membrane. The agarose matrix should be kept intact and all buffers should have a pH below 8.0. After lysis, the circular cpDNA can be released using a 1% PFGE-gel under specific conditions. These guidelines ensure successful extraction and purification of circular cpDNA for further analysis.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Efficient Extraction and Purification of Circular cpDNA from Agarose MatrixSummary '},\n",
       " {'title': 'Optimizing Electroelution for Trapping Circular cpDNA ',\n",
       "  'summary': 'This text outlines the steps for electroelution and trapping of circular cpDNA, including the use of Tris-HCl running buffer and avoiding bubbles. The optimal electroelution time must be determined for each sample and gel concentration. The trapped solution is then treated with reagents to remove impurities, and the resulting circular cpDNA can be stored for up to 6 months. The text also mentions the importance of avoiding foaming during the process. Finally, the text briefly mentions the preparation of the vector for in-situ substitution or ligation.'},\n",
       " {'title': 'Optimizing Vector/Insert Recombination for Clonable DNA Fragments ',\n",
       "  'summary': 'This text discusses a protocol for preparing a vector for cloning DNA fragments, with a focus on avoiding foaming and ensuring complete digestion and dephosphorylation. The protocol involves using mono-restriction endonucleases to digest circular cpDNA from Wheat, Maize, Soybean, and Barley, and then using an in-situ substitute/ligation method for efficient ligation of vector/insert recombinant DNA. The protocol also highlights the importance of selecting suitable endonucleases based on factors such as balance between number and length of digestion products, stability, and ease of procurement. Additionally, the text suggests adding Klenow Fragment to the digestion reaction if the fragments have cohesive ends.'},\n",
       " {'title': 'In-situ Substitute and Ligation Protocol for Efficient Vector/Insert Recombinant DNA Construction ',\n",
       "  'summary': 'This protocol involves using a high ligation efficiency method for constructing vector/insert recombinant DNA while maintaining the integrity of the cpDNA sequence. It also recommends adding Klenow Fragment to the enzymatic digestion reaction and using an osmotic alternative for the enzyme-digested buffer. The use of phenol for cpDNA extraction is not recommended. The protocol emphasizes the importance of sterile processes and using freshly good-quality cpDNA fragments for higher transformation efficiency. The in-situ ligation step involves carefully cutting and inserting the MCE membrane into RNase-free Microfuge Tubes.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Tips for Successful Construction of Circular Chloroplast DNA LibrariesSummary '},\n",
       " {'title': '\"Primer Sets for Verifying BCA Libraries\" ',\n",
       "  'summary': 'This text provides a list of primer sets for verifying the BCA libraries, including the sequence, PCR product size, and combinations of forward and reverse primers for each set. These primer sets can be used to confirm the presence of specific colonies in the BCA libraries, aiding in the identification and analysis of genetic material.'},\n",
       " {'title': 'PCR Primer Design for Junction Site Targeting ',\n",
       "  'summary': 'This text outlines the PCR primer design for targeting junction sites in DNA sequences. The primer sequences and their corresponding junction sites are listed, along with the vector and forward/reverse primer combinations used for each PCR reaction. This information is important for researchers conducting PCR experiments to specifically amplify and study certain regions of DNA.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Verifying the Quality of a Soybean cpDNA LibrarySummary '},\n",
       " {'title': '\"Efficient Extraction and Ligation Methods for High-Quality cpDNA Library Construction\" ',\n",
       "  'summary': 'This text discusses a modified protocol for extracting circular cpDNA from crops, using electroelution and the RecBCDase system to remove linear nucleic acid impurities. The in-situ substitute/ligation method is also described, which allows for high ligation efficiency of vector/insert recombinant DNA while maintaining the integrity of the cpDNA sequence. This method is crucial for constructing a high-quality cpDNA library, which is necessary for various genetic studies. The text also mentions the importance of verifying the quality of the cpDNA library through PCR analysis.'},\n",
       " {'title': '   Advancements in Circular cpDNA Research ',\n",
       "  'summary': 'This text discusses the use of a new in-situ substitute/ligation protocol for producing goal-clonable cpDNAs and constructing goal-insert, ordered DNA libraries. It also covers methods for verifying the quality and integrity of the cpDNA library, including PCR and collinearity analysis. Technological innovation is crucial for progress in this field, particularly in crops with large nuclear genomes where homology between nuclear and organellar genomes can complicate analysis. Enriching circular cpDNA can reduce sequence complexity and improve research outcomes.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'In-situ Chloroplast Lysis for High-Purity Circular cpDNA ExtractionSummary '},\n",
       " {'title': 'Title ', 'summary': 'Purifying Circular cpDNA Using PFGESummary '},\n",
       " {'title': 'Developing a High-Purity Circular cpDNA Extraction and Goal-Insert, Ordered cpDNA Library Construction Method ',\n",
       "  'summary': 'This study presents a new method for extracting circular cpDNA from gelled agarose and constructing a goal-insert, ordered cpDNA library. The method involves in-situ chloroplast lysis, PFGE purification, and electrochemical isolation to ensure the integrity of the cpDNA sequence. The in-situ substitute/ligation protocol is also developed to improve targeted ligation efficiency and can be used for other nucleic acid sequences. This method is highly skilled and may require multiple attempts for researchers with less experience, but it successfully preserves the whole circular cpDNA in three colonies.'},\n",
       " {'title': '\"Development of High-Purity Circular cpDNA Extraction and Goal-Insert, Ordered cpDNA Library Construction Methods\" ',\n",
       "  'summary': 'This study presents a set of methods for extracting high-purity circular cpDNA and constructing goal-insert, ordered cpDNA libraries. These methods can also be used to construct large-insert, ordered DNA libraries for further experiments such as genomic sequencing, bioinformatics analyses, cloning, and physical mapping. The study also includes supplementary information and acknowledgements to those who contributed to the research.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Acknowledgements and Funding in a Plant Cell Research StudySummary '},\n",
       " {'title': 'The Multimeric Forms of Chloroplast Genome ',\n",
       "  'summary': 'This text discusses the existence of multimeric forms of the chloroplast genome, as shown through various studies using techniques such as pulsed-field gel electrophoresis. These studies reveal the structural plasticity of the chloroplast genome in higher plants and the changes in chloroplast DNA levels during development. The findings also suggest that chloroplast DNA may be degraded during leaf development in certain plants. Overall, this text highlights the importance of understanding the organization and dynamics of the chloroplast genome in order to gain a better understanding of plant development and evolution.'},\n",
       " {'title': ' Changes in Chloroplast DNA Levels During Development ',\n",
       "  'summary': 'This text discusses various studies on the changes in chloroplast DNA levels during development in different plant species, including pea, rice, tobacco, Medicago truncatula, and maize. The studies show that the amount of chloroplast DNA can be affected by factors such as light and genotype. The structure of chloroplast DNA molecules and the effects of light on their amount are also examined. Additionally, the text includes a study on antigenic variation in Trypanosoma brucei and the release of DNA from bacteria, chloroplasts, and mitochondria upon lysis. Overall, the text provides valuable insights into the dynamic nature of chloroplast DNA during plant development.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'DNA Isolation and Characterization in Various OrganismsSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'The Study of Chloroplast DNA in Chlamydomonas reinhardi and Other PlantsSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Methods for Isolating and Characterizing Chloroplast DNA in PlantsSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Rapid Method for Purification of Organelles for DNA IsolationSummary '},\n",
       " {'title': 'Isolation of Chloroplast and Mitochondrial DNA from Plants ',\n",
       "  'summary': 'This text discusses various methods for isolating chloroplast and mitochondrial DNA from different plant species. These methods include using synchronous cultures of red algae, direct restriction fragment length polymorphism analysis, and whole chloroplast genome sequencing. The text also includes a rapid method for chloroplast isolation from a specific type of green algae. These methods are important for studying the genetic makeup and evolution of plants, as well as for understanding the role of chloroplast and mitochondrial DNA in plant biology.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'A Comparison of Chloroplast Isolation Methods for Various Plant SpeciesSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Optimized Methods for Sequencing and Maintaining Large DNA FragmentsSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'The Use of BAC Libraries for Advanced Genomics ResearchSummary '},\n",
       " {'title': 'External Viewer Error: Empty Response ',\n",
       "  'summary': 'This error occurs when an external viewer fails to load due to a timeout, resulting in an empty response with zero bytes read. This can be caused by a slow or unresponsive server, network connectivity issues, or incorrect configuration settings. It can prevent users from accessing important information and may require troubleshooting to resolve.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'The Role of Serratia Bacteria in MicrobiologyOpenSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Investigating the Role of Serratia Bacteria in Resilient AgricultureSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Understanding Sef Fimbrial Elements in Pathogenic Serratia SpeciesSummary '},\n",
       " {'title': 'The Role of Bacteria in Controlling Insect Pests ',\n",
       "  'summary': 'This text discusses the various types of bacteria that have been studied for their potential to control insect pests, including Bacillus thuringiensis, Serratia entomophila, and Pseudomonas fluorescens. These bacteria have been found to have different mechanisms for killing insects, such as producing toxins or competing for resources. Some of these bacteria have been successfully used in biocontrol methods, while others are still being studied for their potential. The text also mentions the potential use of bacteria in controlling specific insect pests, such as Costelytra giveni and Pyronota festiva.'},\n",
       " {'title': 'The Co-location of Toxin Complexes and Sef Fimbria Encoding Cluster in Serratia Bacteria ',\n",
       "  'summary': 'This study reveals a strong co-location between toxin complexes and the Sef fimbria encoding cluster in both documented STAMPs and newly characterized Tc-encoding non-STAMP Serratia bacteria. The study also identifies an ancestral connection between Sef and Tcs in Serratia bacteria, with the demarcation sites of the tc found in S. proteamaculans, S. entomophila, Y. frederiksenii, and Serratia sef. This information can help in identifying novel Serratia-based toxin complexes and understanding their evolutionary history.'},\n",
       " {'title': 'Ancestral Connection Between Sef and Tcs ',\n",
       "  'summary': 'This text discusses the identification of demarcation sites for the tc Sef and Tcs in various bacteria, including Serratia, Yersinia, and Serratia proteamaculans. The co-location of these tc islands and their nucleotide alignment reveal a connection between Sef and Tcs in these bacteria. In silico analysis of plasmids also shows the presence of degenerate tc sequences. Phylogenetic analysis further supports the ancestral relationship between Sef and Tcs. The text raises the question of why Sef is present in some bacteria but not others, and characterizes the Sef gene based on its identity.'},\n",
       " {'title': 'Characterization of Sef and its Homologs ',\n",
       "  'summary': 'This text discusses the characterization of Sef, a protein found in E. coli and Photorhabdus bacteria. Through gene identity and synteny comparisons, it is determined that SefD, SefC, and SefA likely function as a chaperone, usher protein, and fimbrial rod protein, respectively. The closest unique orthologs for SefH-SefJ are also identified through BlastX, including a fimbrial protein in Providencia and adhesins in Salmonella, Morganella, and Proteus bacteria. This information provides insight into the potential functions of Sef and its homologs in various bacterial systems.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Identification and Role of Sef Adhesins in Bacterial ColonizationSummary '},\n",
       " {'title': 'The Role of Sef in Colonization and Disease Progression in Costelytra giveni ',\n",
       "  'summary': 'This study investigates the role of Sef, an adhesin protein, in the colonization and disease progression of Costelytra giveni, a type of beetle. The results show that Sef is important for colonization and disease development, as its deletion leads to a significant decrease in disease and mortality rates. This suggests that Sef may be a potential target for controlling the spread of disease in this beetle species.'},\n",
       " {'title': '\"COVID-19 Mortality and Recovery Rates ',\n",
       "  'summary': 'A study shows that the mortality rate for COVID-19 is 0%, while the recovery rate is 91.7%. However, there is a significant range in recovery rates, with some countries reporting as low as 4.2%. This data, published by John Wiley & Sons, Ltd., highlights the need for continued efforts to combat the pandemic and improve recovery rates globally.\"'},\n",
       " {'title': 'The Role of Sef and Tc Encoding Regions in Serratia Pathogenicity ',\n",
       "  'summary': 'This study examined the relationship between Sef and Tc encoding regions in Serratia bacteria and their contribution to pathogenic activity. While there was a high correlation between these regions and their preservation, experimental designs did not reveal any evidence of a mutually beneficial function. The leading hypothesis was that Sef may aid in conjugation and increase Tc production, but experiments showed no such effect. This suggests that there may be other factors at play in the pathogenicity of Serratia bacteria.'},\n",
       " {'title': 'Investigating the Function of Sef and Tcs in Bacterial Conjugation ',\n",
       "  'summary': 'This study aimed to determine if the proteins Sef and Tcs have a mutually beneficial function in bacterial conjugation. However, experiments showed no evidence of Sef aiding in the proliferation of the plasmid responsible for producing Tcs. Further analysis using SDS-PAGE and TEM/Confocal microscopy also did not reveal any significant findings. These results suggest that the leading hypothesis of Sef aiding in conjugation may not be accurate and further research is needed to fully understand the function of these proteins in bacterial conjugation.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Characterizing the Role of Sef in Grass Grub Gut PathogenicitySummary '},\n",
       " {'title': 'Assessing the Role of Sef in the Gut of Grass Grubs ',\n",
       "  'summary': 'This study used various experiments to investigate the potential role of Sef, a bacterial protein, in the gut of grass grubs. Fimbrial lengths were measured using ImageJ, and pathogenicity was determined through oral challenge bioassays. Conjugation and biofilm formation experiments were also conducted. Bioinformatics analysis was performed on DNA samples, and plasmid annotation was done using Prokka. Distance matrix figures were generated using custom R scripts. The results suggest that Sef may play a role in gut colonization and biofilm formation in grass grubs.'},\n",
       " {'title': 'Annotation and Distance Matrix Analysis of Plasmid Contigs ',\n",
       "  'summary': 'This study used Prokka and custom R scripts to annotate plasmid contigs and generate distance matrix figures. The authors, Lesley Sitter, Marion Schoof, Travis R. Glare, Murray P. Cox, Peter C. Fineran, Paul P. Gardner, and Mark R. H. Hurst, were supported by the New Zealand Tertiary Education Commission and Bioprotection Aotearoa. The data generated in this study has been submitted to the NCBI Reference Sequence database.'},\n",
       " {'title': 'The Impact of Pesticides on Agriculture and the Environment ',\n",
       "  'summary': 'This text discusses the use of pesticides in agriculture and their potential benefits and hazards. It also introduces a new screening index for measuring the leachability of pesticides into groundwater. Additionally, the text explores the effects of pesticides on virulence, motility, and biofilm formation in Salmonella typhimurium and the mechanical architecture and folding of E. coli. It also delves into the role of type 1 fimbriae in Salmonella enterica and the potential environmental impacts of pesticide use.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'The Role of Type 1 Fimbriae in Salmonella enterica Infection and ImmunitySummary '},\n",
       " {'title': 'The Role of Fimbriation and Mutants in Escherichia coli ',\n",
       "  'summary': 'This article discusses the importance of Type 1 fimbriation and fimE mutants in Escherichia coli, a common bacterium. It also explores the potential insecticidal toxins produced by the bacterium Photorhabdus luminescens and the molecular mechanism of bacterial pili assembly. Additionally, it introduces the BLAST+ software and its applications in bioinformatics.'},\n",
       " {'title': 'The Evolution of Bioinformatics: A Review of Key Studies ',\n",
       "  'summary': 'This text provides a summary of several key studies in the field of bioinformatics, including the architecture and applications of BLAST+, the role of Type 1 fimbriae in insecticidal bacteria, a new data structure for phylogenomic inference, silver staining of proteins in polyacrylamide gels, and the analysis of major fimbrial subunits in Salmonella colonization. These studies showcase the advancements and applications of bioinformatics in various areas of research, highlighting its importance in understanding and analyzing biological data.'},\n",
       " {'title': 'The Role of Fimbrial Subunits in Salmonella Colonisation ',\n",
       "  'summary': 'This article discusses a 2008 analysis of 13 major fimbrial subunits and their role in the colonisation of chicken intestines by Salmonella enterica. It also includes studies on the expression of fimbrial subunits in other bacteria, such as Escherichia coli and Serratia Yersinia frederiksenii. The results of these studies provide insight into the mechanisms of bacterial colonisation and could potentially lead to the development of new strategies for preventing and treating Salmonella infections.'},\n",
       " {'title': \"The Economic Impact of Invertebrate Pests on New Zealand's Pastoral Industry \",\n",
       "  'summary': \"This text discusses the economic cost of invertebrate pests to New Zealand's pastoral industry, including the impact on crops and livestock. It also includes information on the biology and control of these pests, such as the grass grub and adult beetles. Additionally, it explores the association of a large plasmid with amber disease in the New Zealand grass grub and the potential use of Serratia entomophila and Serratia proteamaculans as biological control agents. The text also briefly touches on the prevalence and treatment of urinary tract infections, as well as the taxonomy of Serratia entomophila and Costelytra zealandica.\"},\n",
       " {'title': 'The Role of Serratia Bacteria in Controlling Insect Pests ',\n",
       "  'summary': 'This text discusses the various studies and findings on the use of Serratia bacteria, specifically Serratia entomophila and Serratia proteamaculans, as a means of controlling insect pests such as the grass grub Costelytra zealandica. These bacteria have been found to produce toxins and antifeeding agents that are effective against the pests, making them potential biocontrol agents. The text also mentions the discovery of diverse toxin clusters and an eCIS variant in Serratia proteamaculans, further highlighting the potential of these bacteria in pest management.'},\n",
       " {'title': 'The Role of Plasmids in Serratia entomophila Pathogenicity ',\n",
       "  'summary': 'This text discusses the role of plasmids in the pathogenicity of Serratia entomophila, a bacterium that is harmful to insects. It includes information on the nucleotide sequence of the plasmid, as well as studies on the cloning and behavior of the bacterium. The text also touches on the use of green fluorescent protein to monitor the fate of Serratia entomophila and the potential ecotoxicity of pesticides and their transformation products in relation to this bacterium. Overall, the text provides valuable insights into the mechanisms and potential impacts of Serratia entomophila pathogenicity.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Understanding the Role of Serratia entomophila Plasmid in Pest ManagementSummary '},\n",
       " {'title': 'Protein Function Classification and Analysis ',\n",
       "  'summary': 'This text discusses various methods and tools used for protein function classification and analysis, including InterProScan 5 and Phyre2. It also explores the structure and function of type 1 fimbriae, as well as the tra region of the conjugative plasmid pIP501. Additionally, it delves into the topic of pesticide poisoning events in wild birds and their impact on the environment. Overall, this text provides a comprehensive overview of protein function classification and analysis, as well as its applications in various fields such as microbiology and environmental science.'},\n",
       " {'title': 'Pesticide Poisoning in Wild Birds in Korea ',\n",
       "  'summary': 'This article discusses the occurrence of pesticide poisoning events in wild birds in Korea from 1998 to 2002. It also includes information on the effects of environmental contaminants on fertility and reproductive health. The study found that pesticides can have negative impacts on bird populations and their reproductive abilities. The article also mentions previous studies on the topic, including one on the hybridization between Escherichia coli and shigella. Overall, the article highlights the importance of understanding the effects of pesticides on wildlife and the environment.'},\n",
       " {'title': 'The Impact of Environmental Contaminants on Fertility and Reproductive Health ',\n",
       "  'summary': 'This article discusses the effects of environmental contaminants on fertility and reproductive health, as well as the potential cooperation of adhesin alleles in Salmonella and the expression of mannose-resistant fimbriae in Photorhabdus temperata. It also includes a sequence analysis of insecticidal genes from Xenorhabdus nematophilus and a study on pesticide exposure and health problems among female horticulture workers in Tanzania. The article highlights the need for further research and regulation to protect human health from the negative impacts of environmental contaminants.'},\n",
       " {'title': 'Pesticide Exposure and Health Problems among Female Horticulture Workers in Tanzania ',\n",
       "  'summary': 'This study examines the impact of pesticide exposure on the health of female horticulture workers in Tanzania. The researchers found that these workers are at a high risk for health problems due to their exposure to pesticides, which are commonly used in the horticulture industry. The study highlights the need for better safety measures and regulations to protect the health of these workers.'},\n",
       " {'title': 'Classification and Clinical Significance of Microorganisms ',\n",
       "  'summary': 'This text discusses the classification, identification, and clinical significance of Proteus, Providencia, and Morganella bacteria. It also includes information on a biofilm formation assay and the effects of certain pesticides on non-target invertebrates. Additionally, it mentions the R language and environment for statistical computing.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Overview of Research on Bacterial Toxins and Pesticide Effects on Cotton FarmingSummary '},\n",
       " {'title': 'Advancements in Image Analysis and Genome Annotation ',\n",
       "  'summary': 'This text discusses the development and use of various tools and techniques for image analysis and genome annotation. It covers the 25-year history of the NIH image to ImageJ software, the use of lysis cassettes for exoprotein release in Yersinia entomophaga, the rapid annotation tool Prokka, and the identification of an allergen in Trichostrongylus colubriformis. These advancements have greatly improved the efficiency and accuracy of image analysis and genome annotation, making them essential tools in various fields of research.'},\n",
       " {'title': 'The Evolution of Virulence in Transmissible Mega-Plasmids ',\n",
       "  'summary': 'This article discusses the functional role of the type 1 pilus rod structure in mediating host-pathogen interactions and how it has evolved in a novel family of transmissible mega-plasmids. The study found that the structure of the pilus rod is crucial for the virulence of these plasmids, which can have significant implications for the spread of antibiotic resistance and other pathogenic traits. The research also highlights the potential impact of pesticide seed dressings on soil organisms and the decomposition of plant material. Overall, this article sheds light on the evolution of virulence and its potential consequences in the microbial world.'},\n",
       " {'title': 'The Impact of Pesticide Seed Dressings on Soil Organisms and Plant Material Decomposition ',\n",
       "  'summary': 'Pesticide seed dressings can alter the activity of soil organisms and decrease the decomposition of plant material, as shown in a study comparing different fimbrial operons in Escherichia coli and Serratia bacteria. This could potentially affect the colonization of the gut of Costelytra giveni insects by Serratia entomophila bacteria, which possess the Sef fimbriae. Further research is needed to fully understand the role of Sef in aiding adherence to hosts and colonization of the gut.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Enhanced Adhesion of HMO-Primed Bifidobacteria to Intestinal Epithelial CellsSummary '},\n",
       " {'title': 'Enhancing Bifidobacterial Adhesion with Human Milk Oligosaccharides ',\n",
       "  'summary': 'This study investigates the potential of human milk oligosaccharides (HMOs) to enhance the adhesion of beneficial gut bacteria, specifically Bifidobacterium, to the intestinal epithelium. Bifidobacteria are known for their health benefits and are commonly used as probiotics. The results showed that HMOs can increase the adhesion of Bifidobacterium to the intestinal lining, potentially leading to improved host-microbe interactions and health benefits. The study was supported by various research groups and funding agencies in Ireland.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Investigating the Role of Human Milk Oligosaccharides in Enhancing Bifidobacterial Adhesion to Intestinal CellsSummary '},\n",
       " {'title': '\"Assessing Probiotic Colonization Capabilities in Human Intestines\" ',\n",
       "  'summary': 'This study by Xiao et al. examines the colonization abilities of probiotic strains, specifically those isolated from fermented foods or plants and then studied in humans. The researchers found that these allochthonous strains do not perform well in their new ecosystem, highlighting the need for further research on probiotics specifically designed for human intestines. The study also includes details on the methods used, such as the generation of a human milk sample and the use of a human colonic adenocarcinoma cell line as a model for intestinal epithelia.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Removal of Lipids and Proteins from Breastmilk and Colonization Assays with Bifidobacterial StrainsSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Bacterial Adherence to Eukaryotic Cells in Probiotic StrainsSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Determining Bacterial Adherence and Numbers in Bifidobacterial Strains Using qPCR and Whole Genome SequencingSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Genome Sequencing and Colonization Capabilities of Bifidobacterial StrainsSummary '},\n",
       " {'title': 'Proteomic Analysis of Bifidobacterial Strains ',\n",
       "  'summary': 'This text discusses the use of proteomic analysis to study the colonization-related factors of bifidobacterial strains. The strains were examined for specific features, including signal peptides, recognition motifs, and membrane-spanning regions. The analysis was carried out on a mixture of four strains, with and without pre-treatment, and after exposure to a cell line. The protein extraction and digestion process is also described. This study provides valuable insights into the factors that contribute to the colonization of bifidobacterial strains, which can have implications for their potential use as probiotics.'},\n",
       " {'title': 'Protein Quantification and Analysis in Adhesion Assays ',\n",
       "  'summary': 'This text describes the methods used to determine protein concentration and perform proteomic analysis in adhesion assays. Lysates were clarified and protein concentration was measured using a Bradford assay. Samples were then prepared for LC-MS/MS analysis by adjusting to a specific protein concentration, reducing and alkylating the proteins, and digesting with trypsin. The resulting peptides were analyzed using a mass spectrometer and searched against a composite database. Statistical analysis was performed on the data from adhesion assays using technical and biological triplicates.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Statistical Analysis of Bifidobacterial Adhesion to Intestinal Epithelial CellsSummary '},\n",
       " {'title': '   The Impact of Secretor-HMO on Bifidobacterial Adherence ',\n",
       "  'summary': \"The study examined the effects of a specific type of human milk oligosaccharide (HMO) called secretor-HMO on the adherence of bifidobacteria to intestinal cells. The secretor-HMO was obtained through a purification process and was found to contain various HMOs, including difucosyllactose and lacto-N-fucopentaose I. The results showed that pre-exposure to secretor-HMO increased the adherence of bifidobacteria to intestinal cells, compared to a glucose control and 2'-fucosyllactose. This suggests that secretor-HMO may play a role in promoting the growth of beneficial bacteria in the gut.\"},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Adhesion of Bifidobacterial Strains to Intestinal CellsSummary '},\n",
       " {'title': '\"Genes Involved in Probiotic Colonisation and Persistence in the Gut\" ',\n",
       "  'summary': 'A literature search identified 45 genes with potential roles in the survival and persistence of probiotics in the gut, specifically in Lactobacillus and Bifidobacterium strains. These genes include bsh, tal, luxS, esat, BBPR, tlyC1, and rfbP, and are found in B. bifidum, B. breve, B. infantis, and L. plantarum. The study also found a high percentage identity among these genes, indicating their importance in probiotic survival. This information can be useful in understanding the mechanisms behind probiotic colonization and developing more effective probiotic strains for gut health.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Identification of Genes Involved in Probiotic Colonization and Persistence in the GutSummary '},\n",
       " {'title': 'Impact of S-HMO Pre-Treatment on Colonization-Associated Pathways in Bifidobacteria ',\n",
       "  'summary': 'This study examined the effects of S-HMO pre-treatment on the colonization-associated pathways in bifidobacteria. The results showed that exposure to human cells led to a significant masking of bacterial proteins, with only a small number of proteins being matched to bacterial proteins. The study also found that there were significant changes in protein abundance in Bifidobacterium bifidum, Bifidobacterium infantis, and Bifidobacterium breve following exposure to human cells. These findings suggest that S-HMO pre-treatment may have a significant impact on the colonization-associated pathways in bifidobacteria.'},\n",
       " {'title': 'Bifidobacterium Adhesion Characteristics and Host-Microbe Interactions ',\n",
       "  'summary': 'This text discusses the adhesive characteristics of four strains of bifidobacteria and their interactions with intestinal epithelial cells. The study found that Bifidobacterium bifidum showed significant upregulation of cell surface adhesins and glycosyl hydrolase proteins, while also matching with other proteins. The HT29-MTX cell line was used to evaluate adherence, as it is able to produce mucin and better mimics the mucus layer in the intestines. This study highlights the potential of B. bifidum as a probiotic for improving host-microbe interactions in the gut.'},\n",
       " {'title': '\"Adhesion of Probiotic Bacteria to Intestinal Epithelial Cells\" ',\n",
       "  'summary': 'This study evaluated the adherence of probiotic bacteria to intestinal epithelial cells using a human cell line that produces mucin. Results showed that B. infantis had higher levels of adhesion when exposed to HMOs, and bacterial genomes were found to have components that aid in adhesion, such as outer-membrane proteins and pili. Additionally, membrane-bound enzymes called \"sortases\" were found to play a critical role in facilitating communication between bacteria and host cells. This study highlights the importance of understanding host-microbe interactions in the development of probiotics.'},\n",
       " {'title': 'The Importance of Bifidobacterial Adherence to Intestinal Epithelial Cells ',\n",
       "  'summary': 'A recent study confirms that the adherence of bifidobacterial strains to intestinal epithelial cells is species- and strain-specific, highlighting the importance of this interaction in the gut microbiota. Moonlighting proteins and the mucus layer also play a role in this process, providing attachment sites and nutrients for these beneficial bacteria. This study sheds light on the specific strains of bifidobacteria that are most effective in adhering to intestinal cells, potentially leading to further research and development of probiotic treatments for gut health.'},\n",
       " {'title': '   The Role of HMO in Promoting Bifidobacteria-Intestinal Cell Interaction ',\n",
       "  'summary': 'HMO, a component of human breast milk, has been found to significantly improve the adhesion ability of bifidobacteria to intestinal cells, which is crucial for their colonization and potential health benefits to the host. This study also provides genome data for specific bifidobacterial strains and contributions from various authors. However, there may be a conflict of interest as some authors are employed by a company that has received costs for services related to the study.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Tools and Resources for Microbiome ResearchSummary '},\n",
       " {'title': 'Exploring Diversity in Gut Bacteria ',\n",
       "  'summary': 'This article discusses the findings of a comparative analysis of the genomes and methylomes of the gut bacteria Bifidobacterium breve. The study reveals a wide range of restriction/modification systems present in this commensal bacteria, which may play a role in its competitive population abundance. The article also touches upon the cross-feeding interactions between infant bifidobacteria and Eubacterium hallii, the immunomodulatory effects of sortase-deficient lactobacilli, and the binding of human plasminogen to Bifidobacterium. Overall, this research sheds light on the diversity and potential functions of these important gut bacteria.'},\n",
       " {'title': 'The Role of Bifidobacteria in Gut Health ',\n",
       "  'summary': 'This text discusses various studies on the interaction between Bifidobacteria and the human body, specifically focusing on the binding of human plasminogen to Bifidobacteria and its potential role in gut health. The studies also explore the effects of Bifidobacteria on intestinal epithelial function and the mechanisms by which they shape the gut microbiota. These findings highlight the potential benefits of Bifidobacteria in promoting a healthy gut and improving overall health.'},\n",
       " {'title': 'The Role of Bifidobacteria in Infant Gut Health ',\n",
       "  'summary': 'This article discusses the impact of Bifidobacteria, a type of probiotic, on infant gut health. The study found that Bifidobacteria isolated from infants and cultured on human milk oligosaccharides can affect intestinal epithelial function. Additionally, the article highlights the importance of autoinducer-2 in gut colonization and probiotic functionality of Bifidobacterium breve. Other studies mentioned in the article also explore the genomic characterization and transcriptional studies of Bifidobacterium adolescentis and the cross-feeding abilities of Bifidobacterium breve and Bifidobacterium bifidum. Overall, the research suggests that Bifidobacteria can play a crucial role in promoting a healthy gut microbiome in infants.'},\n",
       " {'title': 'The Role of Bifidobacterium in Gut Health ',\n",
       "  'summary': 'This text discusses the role of Bifidobacterium in promoting gut health through cross-feeding, immune modulation, and pathogen protection. It also explores the genetic and molecular characteristics of Bifidobacterium and its evolution as a gut symbiont. The study highlights the potential benefits of Bifidobacterium in maintaining a healthy gut microbiome.'},\n",
       " {'title': 'Probiotics and Gut Health ',\n",
       "  'summary': 'This text discusses various studies and research on the role of probiotics in maintaining gut health and treating conditions such as irritable bowel syndrome. It covers topics such as biofilm formation in gut symbionts, the use of probiotics in improving gut microbiota, and the definition and scope of prebiotics. The studies mentioned highlight the potential benefits of probiotics in promoting overall gut health and improving quality of life for individuals with gut-related issues.'},\n",
       " {'title': 'Expert Consensus on Prebiotics ',\n",
       "  'summary': 'This text discusses the definition and scope of prebiotics as agreed upon by the International Scientific Association for Probiotics and Prebiotics (ISAPP). It also includes insights into glycogen metabolism in Lactobacillus acidophilus, the transcriptional response of Bifidobacterium longum, the role of extracellular transaldolase from Bifidobacterium bifidum, a bile-inducible efflux transporter from Bifidobacterium longum, and the implication of an outer surface lipoprotein in adhesion of Bifidobacterium bifidum.'},\n",
       " {'title': 'The Role of Bifidobacterium in Gut Health ',\n",
       "  'summary': 'This text discusses various studies on the role of Bifidobacterium in gut health, including the identification of a bile-inducible efflux transporter in Bifidobacterium longum, the implication of an outer surface lipoprotein in adhesion of Bifidobacterium bifidum, and the critical role of a housekeeping sortase in probiotic Bifidobacterium bifidum. It also includes a comparative genomic analysis of Lactobacillus rhamnosus and the potential benefits of moonlighting proteins in multitasking. These studies shed light on the importance of Bifidobacterium in maintaining a healthy microbiome.'},\n",
       " {'title': 'Comparative Genomic Analysis of Probiotic Bacteria ',\n",
       "  'summary': 'This text discusses various studies on the genomic analysis of probiotic bacteria, specifically Lactobacillus rhamnosus and Bifidobacterium longum. These studies explore the functional roles of different genes and proteins in these bacteria, as well as their potential health benefits. The findings suggest that probiotic bacteria play a crucial role in maintaining a healthy gut microbiota, which has implications for overall health and well-being. These studies also highlight the importance of understanding the composition and activities of the infant gut microbiota, as it is the first microbial colonizer of the human gut.'},\n",
       " {'title': 'The Role of Gut Microbiota in Human Health ',\n",
       "  'summary': 'This text discusses the composition and activities of the infant gut microbiota, as well as the colonization of the human gut by health-promoting bacteria. It also explores the role of microbial lectins and a Bifidobacterial pilus-associated protein in promoting colonic epithelial proliferation. Additionally, it includes a functional genome analysis of Bifidobacterium breve and the relationship between mucus barrier, mucins, and gut microbiota. Overall, this text provides insights into the important role of gut microbiota in maintaining human health.'},\n",
       " {'title': 'Functional Genome Analysis of Bifidobacterium breve and its Role in Carbohydrate Metabolism and Gut Health ',\n",
       "  'summary': 'This study delves into the functional genome of Bifidobacterium breve, a beneficial gut bacteria, and its role in carbohydrate metabolism. The results reveal an on/off replication slippage switch and potential in vivo implications. Additionally, the study explores the relationship between B. breve and mucins, the slimy barrier in the gut, and how they work together to maintain gut health. The study also examines the microbial content of raw and pasteurized cow milk and its potential impact on gut health. Overall, this study sheds light on the important role of B. breve in maintaining a healthy gut microbiome.'},\n",
       " {'title': 'Mining Milk for Factors to Increase Adherence of Bifidobacterium longum ',\n",
       "  'summary': 'This text discusses various studies and research on Bifidobacterium longum, a probiotic commonly found in the gut. The studies focus on factors that can increase the adherence of this probiotic, such as mucin turnover in the small intestine, protein adhesin genes, and exopolysaccharide production. These factors can potentially improve the effectiveness of Bifidobacterium longum in promoting gut health. The text also mentions the potential benefits of Bifidobacterium longum in metabolizing bile salts and its role in nutrient absorption. Overall, the text highlights the importance of understanding and utilizing factors that can enhance the adherence of probiotics in promoting gut health.'},\n",
       " {'title': 'The Role of Bifidobacterium in Microbial Adhesion and Human Milk Oligosaccharide-Sharing ',\n",
       "  'summary': 'This article discusses the various ways in which Bifidobacterium, a type of beneficial bacteria found in the human gut, plays a role in microbial adhesion and the sharing of human milk oligosaccharides. It covers topics such as the use of bile salt hydrolase by Bifidobacterium longum, the role of sortase-dependent pili in Bifidobacterium bifidum, and the sharing of human milk oligosaccharides by a consortium of infant-derived Bifidobacterium. The article also mentions the Perseus computational platform used for comprehensive analysis of proteomics data. Overall, this article highlights the importance of Bifidobacterium in maintaining a healthy gut microbiome and its potential benefits for infants.'},\n",
       " {'title': 'The Role of Microbes in Controlling Adhesion and Sharing Human Milk Oligosaccharides ',\n",
       "  'summary': 'This text discusses the various ways in which microbes, specifically Bifidobacterium and Lactobacillus reuteri, use force and enzymes to control adhesion and share human milk oligosaccharides. These processes are important for the ecological performance and survival of these microbes, and understanding them can provide insights into the development of probiotics and prebiotics. The text also highlights the role of specific proteins, such as Lsp and MsrB, in the ecological performance of Lactobacillus reuteri, and the depletion of teichoic acids in this microbe. Overall, this text sheds light on the complex interactions between microbes and their environment.'},\n",
       " {'title': 'The Role of Bifidobacterium and Lactobacillus in Microbiota ',\n",
       "  'summary': \"This text discusses the role of specific strains of Bifidobacterium and Lactobacillus in the human microbiota. It includes proteomic profiling of Bifidobacterium bifidum and analysis of Bifidobacterium longum's adhesion to host tissue. It also examines the distribution and evolution of von Willebrand/integrin A domains, which play a role in cell adhesion. Additionally, the text evaluates the adhesion of Lactobacillus reuteri to host tissue and its potential benefits in promoting human health.\"},\n",
       " {'title': 'The Role of Probiotics in Maintaining Gut Health ',\n",
       "  'summary': \"This text discusses various studies on the effects of probiotics, specifically Lactobacillus reuteri and Bifidobacterium breve, on gut health. These studies explore the mechanisms by which these probiotics colonize the gut and their impact on intestinal secretory immunoglobulin A levels. The text also identifies surface-associated proteins of Bifidobacterium animalis lactis and the role of 2'-fucosyllactose in promoting Bifidobacterium bifidum. Overall, the text highlights the potential benefits of probiotics in maintaining a healthy gut microbiome.\"},\n",
       " {'title': 'Elucidating the Neuroimmunology of Traumatic Brain Injury ',\n",
       "  'summary': \"This article discusses various methodological approaches to understanding the intercellular communication and function involved in traumatic brain injury. It highlights the work of researchers from the Ann Romney Center for Neurologic Diseases and the Department of Neurosurgery at Brigham and Women's Hospital, as well as the David H. Koch Institute for Integrative Cancer Research at Massachusetts Institute of Technology. The article also mentions the contributions of reviewers from Qatar University and The University of Iowa. Overall, the article emphasizes the importance of studying the neuroimmunology of traumatic brain injury in order to develop effective treatments and interventions.\"},\n",
       " {'title': 'Exploring the Neuroimmunology of Traumatic Brain Injury ',\n",
       "  'summary': 'This article discusses the importance of understanding the neuroimmunology of traumatic brain injury (TBI) and outlines current methodologies that can aid in its exploration. These techniques include adoptive cell transfer, intra-CNS injection(s), selective cellular depletion, genetic manipulation, molecular neuroimaging, and in vitro models. The authors also highlight the crucial role of communication between resident neuroglial cells and recruited lymphocytes in the development of secondary brain injury. Financial support for this research was received from various sources, including the DFCI/Kiki Leptomeningeal Disease Research Fund and the US National Institutes of Health.'},\n",
       " {'title': 'Studying the Neuroimmunology of Traumatic Brain Injury ',\n",
       "  'summary': 'This text discusses the methods and approaches used to study the complex neuroimmunology underlying traumatic brain injury (TBI). These methods include adoptive cell transfer, intra-CNS injections, cell depletion, genetic manipulations, and molecular imaging. TBI is a heterogeneous condition with long-term disabling effects on cognition, sensorimotor function, and behavior. The research is supported by grants from DFCI/Kiki Leptomeningeal Disease Research Fund and the US National Institutes of Health, as well as a 2023 Stepping Strong Innovator Award. By understanding the neuroimmunology of TBI, researchers hope to develop better treatments and interventions for TBI survivors.'},\n",
       " {'title': 'Experimental Methods in Neuroimmunology of TBI ',\n",
       "  'summary': 'This text discusses various experimental methods used in the study of neuroimmunology of traumatic brain injury (TBI). These methods include adoptive cell transfer, intra-CNS injections, cell depletion, tissue- and temporal-specific genetic manipulations, molecular neuroimaging, and in vitro co-culture systems. The use of adoptive cell transfer, where specific cell populations are isolated and introduced into a recipient host, has been a commonly employed technique in immunology. This method has been used in animal models of disease and was first described in 1988 for the treatment of patients with metastatic melanoma. The text also includes a sample of TBI neuroimmunology studies categorized by methodological approach.'},\n",
       " {'title': 'Intervention Methods for Treating Traumatic Brain Injury ',\n",
       "  'summary': 'This text discusses various intervention methods for treating traumatic brain injury, including adoptive transfer of CD4+ and CD11b+ cells, as well as cell depletion using anti-CD8 antibodies. These methods were tested in different models of TBI and recipient species, with varying outcomes. While some interventions worsened cerebral injury and neuroinflammation, others showed promise in improving neurological recovery. These findings were reported in studies by Fee et al. (2003), Braun et al. (2017), and Abou-El-Hassan et al. (2023).'},\n",
       " {'title': 'The Effects of Depleting Immune Cells on Neurological Recovery after Traumatic Brain Injury ',\n",
       "  'summary': 'Various studies have investigated the effects of depleting different types of immune cells on neurological recovery after traumatic brain injury (TBI). Results have shown that depleting macrophages/microglia, Vγ1 and Vγ4 cells, Tregs, CD25, and Gr-1 cells can have varying effects on neuroinflammation, lesion volume, and sensorimotor deficits at different time points post-injury. These findings suggest that targeting specific immune cells may have potential therapeutic benefits for TBI patients.'},\n",
       " {'title': 'The Effects of CD11b Depletion on Brain Injuries ',\n",
       "  'summary': \"Studies have shown that depleting CD11b, a protein found on immune cells, can have varying effects on brain injuries. In one study, administering valganciclovir, a drug that depletes CD11b, before or after a traumatic brain injury (TBI) had no effect on brain damage. In another study, using diphtheria toxin to deplete CD11b after a TBI resulted in increased inflammation in the brain. Additionally, depleting macrophages and microglia, which also express CD11b, after a TBI in rat pups led to increased neurodegeneration and reactivity to brain injury markers. These findings suggest that CD11b may play a role in the body's response to brain injuries.\"},\n",
       " {'title': 'The Effects of Macrophage and B Cell Depletion on Neurodegeneration and Recovery after Traumatic Brain Injury ',\n",
       "  'summary': 'This text discusses four studies that investigate the effects of depleting macrophages and B cells on neurodegeneration and recovery after traumatic brain injury (TBI). The studies use different methods, such as clodronate liposomes and direct intra-CNS injection, and different animal models, such as Sprague-Dawley rat pups and C57BL/6J mice. Results show that depleting macrophages and B cells can have both positive and negative effects on neurodegeneration and recovery, depending on the timing and method of depletion. These findings have implications for potential therapeutic strategies for TBI.'},\n",
       " {'title': 'Potential Therapies for Traumatic Brain Injury ',\n",
       "  'summary': 'This text discusses various potential therapies for traumatic brain injury, including genetic engineering, intraventricular injections, and inducible deletions. These therapies have shown promising results in reducing brain edema, neuronal apoptosis, and astrocyte activation, as well as improving memory and promoting regeneration of oligodendrocytes. These therapies have been tested on rats and mice, and have shown positive effects up to 2 months post-injury. Further research and development of these therapies could potentially lead to improved outcomes for individuals with traumatic brain injury.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Investigating the Effects of Co-Culture Systems on Microglia and Myeloid CellsSummary '},\n",
       " {'title': 'The Role of T Cells in Modulating Neuroinflammation after Traumatic Brain Injury ',\n",
       "  'summary': 'T cells, specifically Tregs, have been shown to play a crucial role in reducing neuroinflammation after traumatic brain injury. Studies have found that Tregs can attenuate the pro-inflammatory response of microglia, the immune cells in the brain, through co-culture experiments. This highlights the potential for Tregs to be used as a therapeutic target for TBI.'},\n",
       " {'title': 'The Role of Adaptive Immune Cells in Traumatic Brain Injury ',\n",
       "  'summary': 'Recent studies have shown that Tregs and gamma-delta T cells play a significant role in modulating inflammation in traumatic brain injury (TBI). Further research using adaptive cell transfer (ACT) techniques, such as CAR T-cell therapy, may provide promising immunotherapy options for TBI. However, there are challenges in conducting these studies, such as regulatory hurdles and the complexity of the techniques. Direct intra-CNS injections of immune cells, known as CNS ACT, have not been widely used but could offer valuable insights into TBI neuroimmunology and potential treatments.'},\n",
       " {'title': 'The Advantages and Challenges of Direct CNS Injections for ACT Applications ',\n",
       "  'summary': 'Direct CNS injections offer a simplified method for delivering immune cells and pharmacological agents to the brain, bypassing the blood-brain barrier. However, their invasive nature and technical expertise required have limited their use in studying TBI neuroimmunology and as an immunotherapy. In preclinical experiments, these injections have shown promising results in improving motor coordination and memory in mice. Further investigations using advanced technology such as single-cell RNA-seq are needed to fully understand the crosstalk between the adaptive and innate immune responses.'},\n",
       " {'title': 'Exploring the Use of Intra-CNS Injections in Investigating TBI Neuroimmunology ',\n",
       "  'summary': 'Intra-CNS injections and other neuroimmunology assays can provide valuable insights into the dynamic nature of TBI neuroimmunology. While invasive, these injections can offer a more precise and controlled delivery of substances, allowing for temporal analysis of molecular and cellular changes in different phases of TBI. Additionally, the use of intra-CNS injections after TBI may have therapeutic potential, such as the potential for faster and more effective recovery with the use of regulatory cells of the adaptive immune system. However, challenges such as invasiveness, ethical concerns, and technical expertise must be considered. Cell depletion therapy, achieved through pharmacological agents, has also shown promise in treating autoimmune diseases and may be applicable in TBI research.'},\n",
       " {'title': 'The Role of Anti-CD19 Antibodies in Investigating TBI Neuroimmunology ',\n",
       "  'summary': 'Anti-CD19 antibodies have been used in numerous studies to deplete B cells and investigate the neuroimmunology of traumatic brain injury (TBI). These antibodies have been shown to be effective in reducing inflammation and improving outcomes in TBI. By targeting B cells, which play a key role in the immune response, researchers have gained valuable insights into the mechanisms of TBI and potential treatment options. This approach has been utilized in multiple studies and continues to be a valuable tool in understanding the complex immune response to TBI.'},\n",
       " {'title': 'The Role of Cell Depletion in Investigating Neuroimmunology after Traumatic Brain Injury ',\n",
       "  'summary': 'Cell depletion has been used in various studies to investigate the role of different immune cell populations in neuroinflammation after traumatic brain injury. This approach has shown that depletion of B cells can reduce neuroinflammation in spinal cord injury, while selective depletion of CD4 or CD8 T cells has been associated with reduced lesion volume in animal models of stroke. Myeloid cells, such as neutrophils, monocytes, and macrophages, have also been found to accumulate at sites of injury and can be targeted for investigation using cell depletion techniques. This method has potential applications in autoimmune and cancer treatments.'},\n",
       " {'title': 'Challenges and Approaches in Genetic Engineering for Therapeutic Use ',\n",
       "  'summary': 'This text discusses the challenges of using genetic engineering for therapeutic purposes, including off-target effects, immune suppression, complexity, and ethical considerations. It also mentions various approaches, such as the use of Cre-loxP technology and CRISPR/Cas9, and the potential use of short hairpin RNA for gene silencing. The text provides a summary of the different Cre promoters used in cells of the nervous and immune systems, as well as specific examples of genetic engineering approaches used in studies.'},\n",
       " {'title': 'The Role and Advantages/Disadvantages of Genetic Engineering ',\n",
       "  'summary': 'This text discusses the role of aquaporin-4 in brain edema after TBI and the benefits and drawbacks of genetic engineering. Genetic engineering allows for a better understanding of disease and the development of customized pharmaceuticals, but it also comes with technical expertise, unintended consequences, social acceptance, and regulatory challenges.'},\n",
       " {'title': 'The Advantages and Challenges of Molecular Imaging in Genetic Engineering ',\n",
       "  'summary': 'Molecular imaging has greatly advanced in the field of neuroglia research, but there are still challenges to overcome. These include technical expertise, unintended consequences, social acceptance, and regulatory hurdles. The use of molecular biomarkers, such as TSPO, in PET imaging has been successful in studying neurological diseases like stroke, but there are still limitations and potential risks. Two-photon in vivo imaging has also shown promise, but further research is needed. Overall, while molecular imaging has great potential in genetic engineering, careful consideration and regulation are necessary to ensure its safe and effective use.'},\n",
       " {'title': 'The Advantages and Disadvantages of Molecular Imaging and Co-Culture Systems ',\n",
       "  'summary': 'Molecular imaging offers a non-invasive approach for early detection and monitoring of diseases, but it also has limitations such as cost and complexity. Co-culture systems and organoids, on the other hand, allow for cellular crosstalk investigations, disease modeling, and drug screening. However, they also have limitations in replicating the natural environment. Few studies have utilized co-culture methods in TBI neuroimmunology, but they show potential for further research.'},\n",
       " {'title': 'Challenges and Future Directions in Studying Neuroimmunology of Traumatic Brain Injury ',\n",
       "  'summary': 'The complexity and heterogeneity of TBI, as well as ethical and technical challenges, make it difficult to fully understand and treat the neuroimmunological aspects of this condition. Animal models may not accurately reflect human TBI, and certain neurovascular compartments are often overlooked in research. However, recent advances in technology offer the potential for more detailed investigations at the cellular level. Ethical considerations and the need for sensitive diagnostic tools also pose challenges in clinical research. Moving forward, utilizing biotechnology and emerging tools may provide a more comprehensive understanding of TBI neuroimmunology.'},\n",
       " {'title': 'Understanding the Challenges and Potential of Neuroimmunology in TBI Research ',\n",
       "  'summary': 'This article discusses the ethical and practical challenges that limit our understanding of the chronic phase of TBI and the potential for using neuroimmunology methods to develop diagnostic tools and immunotherapies. The authors highlight the importance of utilizing advancements in biotechnology and emerging tools to investigate the inter and intra-cellular levels of neuroinflammation and neurotoxicity after TBI. They also discuss the potential for immunotherapies, such as depleting antibodies and viral therapy, to attenuate the inflammatory cascade and prevent chronic neurodegeneration. Overall, this article emphasizes the promising translational potential of neuroimmunology in TBI research.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Author Contributions and Conflict of Interest in Scientific ResearchSummary '},\n",
       " {'title': 'T-cell Receptor Gene Therapy for Melanoma ',\n",
       "  'summary': 'This article discusses a study on using T-cell receptor gene therapy to treat established tumors in a murine melanoma model. The results showed promising potential for this therapy in treating melanoma. Additionally, the article also mentions a separate study on identifying potential biomarkers and therapeutic targets for spinal cord injury. Another study explores the relationship between traumatic brain injury, diabetic neuropathy, and altered psychiatric health. Finally, the article discusses the role of Vγ1 and Vγ4 gamma-delta T cells in the immunopathology of traumatic brain injury in males.'},\n",
       " {'title': 'The Role of Gamma-Delta T Cells in Traumatic Brain Injury ',\n",
       "  'summary': 'A study found that Vγ1 and Vγ4 gamma-delta T cells have opposing effects on the immunopathology of traumatic brain injury in males. While Vγ1 cells contribute to inflammation and tissue damage, Vγ4 cells help reduce brain edema and preserve cognitive function. This research could lead to potential therapeutic targets for treating traumatic brain injury.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'PET Imaging for Neuroinflammation and Microglia FunctionSummary '},\n",
       " {'title': 'The Role of Microglia in Synaptic Function and Behavior ',\n",
       "  'summary': 'This article discusses the use of microglia depletion approaches to study the impact of microglia on synaptic function and behavior. It also highlights the development of in vitro models for traumatic brain injury and the potential use of biomaterial-based therapies. Additionally, it explores the role of microglia in axonal injury and the use of two-photon excitation microscopy for studying living cells and tissues. Finally, it discusses the potential of using polyclonal regulatory T cells for immunotherapy in type 1 diabetes.'},\n",
       " {'title': 'Advancements in Cell and Tissue Imaging Techniques ',\n",
       "  'summary': 'This text discusses various imaging techniques, such as two-photon excitation microscopy, that have been developed for studying living cells and tissues. It also explores the use of cell-based therapies for treating traumatic brain injury and the role of immune modulation in brain metabolism and T lymphocyte polarization. These techniques and therapies have the potential to greatly advance our understanding and treatment of various diseases and injuries.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': \"18F-FDG-PET Detects Changes in Brain Metabolism in Alzheimer's Disease ModelSummary \"},\n",
       " {'title': '\"New Treatment for Neuroinflammation after Traumatic Brain Injury\" ',\n",
       "  'summary': 'A recent study has found that a compound called ACT001 can reduce neuroinflammation after a traumatic brain injury by inhibiting the AKT/NFκB/NLRP3 pathway. This pathway is known to play a role in the activation of microglia, which contribute to neuroinflammation. This new treatment shows promise in reducing the harmful effects of neuroinflammation and could potentially improve outcomes for those with traumatic brain injuries. Other potential treatments for neuroinflammation, such as B-cell depletion and CRISPR/Cas9 systems, are also being explored. '},\n",
       " {'title': 'Neuroinflammation and Brain Injury ',\n",
       "  'summary': 'This text discusses various studies on neuroinflammation and brain injury, including the comparison of different models used to study traumatic brain injury in rats, the use of CRISPR/Cas9 systems to edit the central nervous system, imaging techniques to detect glial cell activation and white matter integrity in football players, and the role of activated CD8+ T cells in long-term neurological impairment after brain injury. It also reviews the use of radiopharmaceuticals for PET and SPECT imaging in the past decade.'},\n",
       " {'title': 'Activated CD8+ T cells and Long-Term Neurological Impairment After Traumatic Brain Injury ',\n",
       "  'summary': 'A study in mice found that activated CD8+ T cells can cause long-term neurological impairment after traumatic brain injury. This was demonstrated through PET imaging, which showed increased levels of microglia and cyclooxygenase 2 in the brain. The study also found that silencing IFN-γ binding/signaling in astrocytes and microglia had opposite effects on central nervous system autoimmunity. These findings have implications for understanding the role of immune cells in brain injury and potential treatments for long-term neurological damage.'},\n",
       " {'title': 'The Impact of B Cell Treatment on Central Nervous System Autoimmunity ',\n",
       "  'summary': 'B cell treatment has been shown to promote a neuroprotective environment and improve outcomes in patients with traumatic brain injury and rheumatoid arthritis. This treatment works by modulating the immune response and promoting a balance between B cells and peripheral myeloid cells. This approach has also shown promise in treating refractory metastatic melanoma, highlighting the potential for B cell therapy in various autoimmune and cancer conditions.'},\n",
       " {'title': 'The Efficacy of B-Cell-Targeted Therapy in Rheumatoid Arthritis and its Potential Role in Neuroinflammation ',\n",
       "  'summary': 'This article discusses the effectiveness of rituximab, a B-cell-targeted therapy, in treating rheumatoid arthritis. It also explores the potential role of B-cells in neuroinflammation, specifically in exacerbating acute damage in the central nervous system following traumatic injury. The article also mentions the use of nanomedicine in intrathecal drug delivery for treating neuroinflammation.'},\n",
       " {'title': 'Advancements in Intrathecal Drug Delivery for Traumatic Brain Injury ',\n",
       "  'summary': 'This text discusses the use of nanomedicine in intrathecal drug delivery for treating traumatic brain injury. It also explores the potential benefits of using this technique, such as preventing spasticity and reducing brain inflammation. Additionally, the text highlights the use of aquaporin-4 RNA interference to improve functional recovery after a brain injury. Finally, it mentions a study investigating sensorimotor impairments in individuals with mild traumatic brain injury, providing insight into the long-term effects of this type of injury.'},\n",
       " {'title': 'Investigating Sensorimotor Impairments in Mild Traumatic Brain Injury ',\n",
       "  'summary': 'This text discusses various studies and research on sensorimotor impairments in individuals 4 weeks to 6 months after experiencing a mild traumatic brain injury. It covers topics such as co-culture systems and technologies, the origin and dynamics of macrophages at central nervous system interfaces, and the role of TAK1 in CNS autoimmune inflammation. The text also explores the use of tetracycline-responsive promoters for tight control of gene expression in mammalian cells. Overall, the text provides valuable insights into the effects of mild traumatic brain injury on sensorimotor function and potential treatment options.'},\n",
       " {'title': 'New Insights into CNS Autoimmune Inflammation ',\n",
       "  'summary': 'A study using a new type of gene targeting in microglia has identified TAK1 as a key player in central nervous system autoimmune inflammation. This discovery could lead to new treatments for conditions such as multiple sclerosis. Additionally, the study also found that CD44 plays a role in regulating the differentiation of immune cells involved in autoimmune inflammation. Another study revealed the prevalence of medical and psychiatric comorbidities following traumatic brain injury, highlighting the need for comprehensive care for these patients. Finally, single-cell RNA sequencing of microglia throughout the lifespan and in injured brains showed complex changes in cell state, providing a better understanding of microglia function.'},\n",
       " {'title': 'Single-Cell RNA Sequencing and Imaging in Brain Injury ',\n",
       "  'summary': 'This text discusses recent studies using single-cell RNA sequencing and two-photon imaging techniques to examine changes in microglia and vascular function in the brain after injury. These studies reveal complex changes in cell state and provide insights into potential therapeutic targets for brain injury. Additionally, the text discusses the use of CRISPR-Cas9 technology to selectively activate specific cell types, providing a potential tool for further research in this area.'},\n",
       " {'title': 'Advances in Traumatic Brain Injury Research ',\n",
       "  'summary': 'This text discusses recent advances in research on traumatic brain injury, including the use of positron emission tomography imaging and the association between TBI and chronic disorders. It also explores the potential for cell-specific CRISPR-Cas9 activation and the role of neuroimmunology in TBI.'},\n",
       " {'title': 'The Association of Traumatic Brain Injury with Chronic Disorders ',\n",
       "  'summary': 'This article discusses the link between traumatic brain injury and the development of chronic cardiovascular, endocrine, neurological, and psychiatric disorders. It cites various studies and research, including the neuroimmunology of TBI and the use of rituximab for treatment. It also highlights the potential role of TH17 lymphocytes in promoting inflammation in the central nervous system. This information suggests the need for a paradigm shift in understanding and treating TBI, as well as the importance of early detection and intervention to prevent long-term health complications.'},\n",
       " {'title': 'Novel Monoclonal Antibody for Isolation of Astrocytes ',\n",
       "  'summary': 'A new monoclonal antibody, Anti-ACSA-2, has been identified for the isolation of living neonatal and adult astrocytes. This antibody has potential applications in studying central nervous system inflammation and traumatic brain injury. Additionally, the Cre-LoxP system and human organoids are discussed as model systems for understanding tissue-specific roles of target genes and for studying human biology and medicine. Depletion of regulatory T cells has been found to increase T cell brain infiltration, reactive astrogliosis, and interferon-γ gene expression in acute experimental traumatic brain injury.'},\n",
       " {'title': 'Human Organoids: Advancements in Modeling Human Biology and Medicine ',\n",
       "  'summary': 'This text discusses the use of human organoids as model systems for studying human biology and medicine. It highlights recent research on the depletion of regulatory T cells in traumatic brain injury, the TREM2-APOE pathway in neurodegenerative diseases, PET imaging of neuroinflammation, and B cell depletion therapies in autoimmune diseases. These studies demonstrate the potential of human organoids in advancing our understanding of various diseases and developing effective treatments.'},\n",
       " {'title': 'Advances in B Cell Depletion Therapies for Autoimmune Disease ',\n",
       "  'summary': 'This article discusses the latest developments and insights into B cell depletion therapies for autoimmune diseases. It covers the role of DNA methylation in T cell development and survival, the neuroprotective effects of aquaporin-4 gene silencing in traumatic brain injury, and the potential of adoptive regulatory T-cell therapy for cerebral ischemia. It also explores a novel method for delivering siRNA in traumatic brain injury and a 3D brain-like tissue culture model for studying controlled cortical impact injury. Overall, this article provides a comprehensive overview of the current state of B cell depletion therapies in treating autoimmune diseases.'},\n",
       " {'title': 'Advancements in Treating Traumatic Brain Injury ',\n",
       "  'summary': 'This text discusses various studies and methods for treating traumatic brain injury, including the use of siRNA delivery, 3D brain-like tissue cultures, regulatory T cells, and nerve growth factor gene therapy. These advancements show promise in improving cognitive function and reducing neuroinflammation after brain injury. Additionally, the use of diffusion tensor imaging has revealed white matter abnormalities in individuals with mild traumatic brain injury and cognitive disability, providing further insight into the pathophysiology of this condition. These advancements have the potential to greatly improve the treatment and outcomes of traumatic brain injury.'},\n",
       " {'title': 'Biomaterials and Brain Injury ',\n",
       "  'summary': 'This text discusses the use of biomaterials in studying brain injuries, specifically mild traumatic brain injuries (TBI) and their effects on cognitive disability. It also explores the potential of using PET probes to image cyclooxygenase-2 expression in brain injuries. Additionally, it delves into the immunopathology of B lymphocytes during stroke-induced injury and repair. The text highlights the importance of biomaterials in understanding and treating brain injuries, and the potential for further research in this area.'},\n",
       " {'title': 'The Role of B Lymphocytes in Stroke-Induced Injury and Repair ',\n",
       "  'summary': 'This article discusses the role of B lymphocytes in the immunopathology of stroke-induced injury and repair. It also explores potential therapeutic strategies, such as regulatory T-cell therapy, for reducing brain damage and inflammation after stroke. Additionally, the article highlights the use of tau positron emission tomography to diagnose chronic traumatic encephalopathy in a former football player and the impact of microglia depletion on brain injury in aged mice after acute ischemic stroke.'},\n",
       " {'title': 'The Impact of Microglia Depletion on Brain Injury After Stroke in Aged Mice ',\n",
       "  'summary': \"A study found that depleting microglia, a type of immune cell, in aged mice increased brain injury after a stroke. This was observed through PET imaging of brain inflammation and analysis of brain tissue. The study also explored the potential of using tissue-specific CRISPR technology for in vivo screening in Drosophila. Additionally, the role of adenosine A(2A) receptors in Parkinson's disease and the ability of cerebrospinal fluid to support cortical cell viability and proliferation were also investigated. These findings have implications for understanding and potentially treating brain injuries and diseases.\"},\n",
       " {'title': 'Title ',\n",
       "  'summary': \"Measuring Adenosine A(2A) Receptors in Parkinson's Disease PatientsSummary \"},\n",
       " {'title': 'The Role of Blood-Brain Barrier Cross-Talk in Neurological Disorders ',\n",
       "  'summary': \"This article discusses the potential implications of the Kallikrein-Kinin system in neurological disorders and the use of regulatory T cells to prevent allograft rejection. It also explores how brain-derived neurotrophic factor can affect cell migration in the developing mouse cerebral cortex and the use of [11C]PK11195 binding as a potential biomarker for Alzheimer's disease and progressive supranuclear palsy. The article also provides an evolutionary perspective on the use of mouse models in studying human diseases.\"},\n",
       " {'title': 'The Role of Microglia in Neurodegenerative Disorders ',\n",
       "  'summary': \"This text discusses the role of microglia, a type of immune cell in the brain, in neurodegenerative disorders such as Alzheimer's disease and progressive supranuclear palsy. It also explores the use of mouse models to study these diseases and the potential of regulatory B cells and γδ T cells in controlling inflammation and neurodegeneration. Additionally, it highlights the impact of brain injury on microglia and its association with inflammatory neurodegeneration.\"},\n",
       " {'title': 'The Role of Brain Injury in Accelerating Age-Related Inflammation and Neurodegeneration ',\n",
       "  'summary': 'This article discusses the impact of brain injury on the development of a reversible age-related microglial phenotype, which is associated with inflammatory neurodegeneration. It also explores the potential for using adoptive cell transfer as a clinical path for effective cancer immunotherapy. Additionally, the role of microglial control of astrocytes in response to microbial metabolites is examined, along with the advances and future challenges of single-cell RNA sequencing. The article also delves into the role of gut-licensed IFNγ+ NK cells in driving anti-inflammatory astrocytes and the use of site-specific DNA recombination in mammalian cells.'},\n",
       " {'title': 'The Role of Gut-Licensed IFNγ+ NK Cells in Driving Anti-Inflammatory Astrocytes ',\n",
       "  'summary': 'A study found that gut-licensed IFNγ+ NK cells play a crucial role in promoting the development of anti-inflammatory astrocytes, which can help reduce inflammation in the brain. This discovery could have implications for treating conditions such as stroke and brain trauma.'},\n",
       " {'title': 'The Role of Inflammation in Traumatic Brain Injury ',\n",
       "  'summary': 'This text discusses the role of inflammation in traumatic brain injury and potential treatments to reduce chronic microglial activation and improve long-term outcomes. Studies have shown that minocycline can reduce inflammation but may also increase neurodegeneration. Neutrophil elastase has also been identified as a mediator of acute pathogenesis and a determinant of long-term behavioral recovery. Additionally, cerebral interleukin-17-producing gammadeltaT cells have been found to play a pivotal role in delayed ischemic brain injury. Positron emission tomography and intraparenchymal application of mature B lymphocytes have also been explored as potential treatments for traumatic brain injury.'},\n",
       " {'title': 'Overview of Positron Emission Tomography and B Cell Therapy for Traumatic Brain Injury ',\n",
       "  'summary': 'This text provides an overview of positron emission tomography and its use in studying traumatic brain injury. It also discusses recent research on the potential therapeutic role of B cells in promoting tissue repair and functional recovery after brain injury. The authors highlight the importance of understanding the separate pathological processes of inflammatory leukocytic recruitment and neuronal degeneration in brain injury, as well as the potential for microglial-oligodendrocyte interactions to aid in myelination and neurological function recovery.'},\n",
       " {'title': 'Microglial-Oligodendrocyte Interactions in Myelination and Neurological Function Recovery after Traumatic Brain Injury ',\n",
       "  'summary': \"This article discusses the role of microglial-oligodendrocyte interactions in the process of myelination and recovery of neurological function after traumatic brain injury. It also explores the potential implications of depleting certain leukocytes on wound healing and neurological outcomes, as well as the role of human choroid plexus growth factors in Alzheimer's disease. Additionally, the article discusses the early source of IFN-γ in spinal cord injury and how heterozygosity for a mutation of the NF-κB innate immune response transcription factor relish can increase survival following traumatic brain injury in drosophila.\"},\n",
       " {'title': 'The Impact of Heterozygosity for NF-κB Mutation on Survival Following Traumatic Brain Injury in Drosophila ',\n",
       "  'summary': 'A study found that heterozygosity for a mutation of the NF-κB innate immune response transcription factor relish increased survival in drosophila with traumatic brain injury. This finding could potentially lead to new treatments for brain injuries in humans.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'The Role of Neuroinflammation in Stroke and Neurodegenerative DiseasesSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': \"Advancements in Neuroimaging for Parkinson's Disease and NeuroinflammationSummary \"},\n",
       " {'title': 'Title ',\n",
       "  'summary': \"Investigating Traumatic Brain Injury and Alzheimer's Disease in the ADNI CohortSummary \"},\n",
       " {'title': 'The Role of Neutrophils and Astrocytes in Traumatic Brain Injury ',\n",
       "  'summary': 'This text discusses the role of neutrophils and astrocytes in traumatic brain injury, specifically in relation to blood-brain barrier permeability and CNS inflammation. It also mentions potential drug targets and animal models for studying traumatic brain injury.'},\n",
       " {'title': 'Aquaporin-4 as a Potential Drug Target for Traumatic Brain Injury ',\n",
       "  'summary': 'This article discusses the potential of targeting aquaporin-4 as a treatment for traumatic brain injury, which can worsen brain edema. It also mentions various animal models used to study traumatic brain injury and the role of TCR signaling in maintaining and functioning of dendritic epidermal T cells. Additionally, it explores the use of B cell depletion therapies in autoimmune diseases and the development of autoantibodies in response to traumatic brain injury. The article also discusses the use of intravital imaging to study traumatic brain injury in mice.'},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'A Simple Method for Detecting Ectopic Cre Recombinase Expression in Mouse LinesSummary '},\n",
       " {'title': 'Title ',\n",
       "  'summary': 'Intricate 3D Architecture of a DNA Mimic of GFPSummary '}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_1_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the titles and summaries\n",
    "stage_1_summaries = [e['summary'] for e in stage_1_outputs]\n",
    "stage_1_titles = [e['title'] for e in stage_1_outputs]\n",
    "num_1_chunks = len(stage_1_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_1_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenAI to embed the summaries and titles. Size of _embeds: (num_chunks x 1536)\n",
    "openai_embed = OpenAIEmbeddings()\n",
    "\n",
    "summary_embeds = np.array(openai_embed.embed_documents(stage_1_summaries))\n",
    "title_embeds = np.array(openai_embed.embed_documents(stage_1_titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting similarity matrix of chunk summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get similarity matrix between the embeddings of the chunk summaries\n",
    "summary_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "summary_similarity_matrix[:] = np.nan\n",
    "\n",
    "for row in range(num_1_chunks):\n",
    "  for col in range(row, num_1_chunks):\n",
    "    # Calculate cosine similarity between the two vectors\n",
    "    similarity = 1- cosine(summary_embeds[row], summary_embeds[col])\n",
    "    summary_similarity_matrix[row, col] = similarity\n",
    "    summary_similarity_matrix[col, row] = similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc2b115a510>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw a heatmap with the summary_similarity_matrix\n",
    "plt.figure()\n",
    "# Color scheme blues\n",
    "plt.imshow(summary_similarity_matrix, cmap = 'Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community detection in summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm\n",
    "\n",
    "def get_topics(title_similarity, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "\n",
    "  proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "  for row in range(proximity_bonus_arr.shape[0]):\n",
    "    for col in range(proximity_bonus_arr.shape[1]):\n",
    "      if row == col:\n",
    "        proximity_bonus_arr[row, col] = 0\n",
    "      else:\n",
    "        proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "  title_similarity += proximity_bonus_arr\n",
    "\n",
    "  title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "  desired_num_topics = num_topics\n",
    "  # Store the accepted partitionings\n",
    "  topics_title_accepted = []\n",
    "\n",
    "  resolution = 0.85\n",
    "  resolution_step = 0.01\n",
    "  iterations = 40\n",
    "\n",
    "  # Find the resolution that gives the desired number of topics\n",
    "  topics_title = []\n",
    "  while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    resolution += resolution_step\n",
    "  topic_sizes = [len(c) for c in topics_title]\n",
    "  sizes_sd = np.std(topic_sizes)\n",
    "  modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "  lowest_sd_iteration = 0\n",
    "  # Set lowest sd to inf\n",
    "  lowest_sd = float('inf')\n",
    "\n",
    "  for i in range(iterations):\n",
    "    topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "    \n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "    print(f'Iteration {i}: topics_title (communities) {topics_title} modularity {modularity}')\n",
    "    \n",
    "    # Check SD\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    print('Communities size:', topic_sizes)\n",
    "    print('Standard deviation community size:', sizes_sd)\n",
    "    \n",
    "    topics_title_accepted.append(topics_title)\n",
    "    \n",
    "    if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "      lowest_sd_iteration = i\n",
    "      lowest_sd = sizes_sd\n",
    "      \n",
    "  # Set the chosen partitioning to be the one with highest modularity\n",
    "  topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "  print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}')\n",
    "  \n",
    "  topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "  # Arrange title_topics in order of topic_id_means\n",
    "  topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "  # Create an array denoting which topic each chunk belongs to\n",
    "  chunk_topics = [None] * title_similarity.shape[0]\n",
    "  for i, c in enumerate(topics_title):\n",
    "    for j in c:\n",
    "      chunk_topics[j] = i\n",
    "            \n",
    "  return {\n",
    "    'chunk_topics': chunk_topics,\n",
    "    'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {10}, {49}, {40}, {2, 35, 36, 38, 8, 9, 13, 15, 27}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {4, 6}, {51, 47, 55}, {32, 34, 37, 39, 7, 18, 29, 23, 59, 61, 30, 63}, {33}, {42, 43, 44, 28, 14}, {64, 3, 57, 45, 62}, {48, 46}, {52, 53, 54}, {56, 58}, {16}] modularity 0.024064370389019308\n",
      "Communities size: [7, 1, 1, 1, 9, 1, 9, 2, 3, 12, 1, 5, 5, 2, 3, 2, 1]\n",
      "Standard deviation community size: 3.3647881520176686\n",
      "Iteration 1: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {6}, {10}, {2, 35, 36, 4, 8, 9, 13, 15, 27}, {16}, {40, 18, 38, 39}, {17, 19, 20, 21, 22, 24, 25, 26, 60, 31}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {56, 51}, {33}, {55, 46, 47}, {42, 43, 44, 28, 14}, {49}, {52, 53, 54}, {57, 58, 62}, {64, 48, 3, 45}] modularity 0.024159854578607063\n",
      "Communities size: [7, 1, 1, 9, 1, 4, 10, 10, 2, 1, 3, 5, 1, 3, 3, 4]\n",
      "Standard deviation community size: 3.151760420780742\n",
      "Iteration 2: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {8, 9, 6}, {10}, {49}, {16}, {17, 19, 20, 21, 22, 24, 25, 26, 60, 31}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {2, 35, 4, 36, 27, 13, 15}, {47, 46, 55}, {33}, {40, 18, 38, 39}, {42, 43, 44, 28, 14}, {64, 48, 3, 45}, {56, 51}, {52, 53, 54}, {57, 58, 62}] modularity 0.024150545789677638\n",
      "Communities size: [7, 3, 1, 1, 1, 10, 10, 7, 3, 1, 4, 5, 4, 2, 3, 3]\n",
      "Standard deviation community size: 2.9040650388722358\n",
      "Iteration 3: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {49, 10}, {16, 6}, {17, 19, 20, 21, 22, 24, 25, 26, 60, 31}, {2, 35, 36, 4, 8, 9, 13, 15, 27}, {47, 46, 55}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {33}, {40, 18, 38, 39}, {42, 43, 44, 28, 14}, {64, 48, 3, 45}, {52, 53, 54}, {56, 51}, {57, 58, 62}] modularity 0.02415835619632962\n",
      "Communities size: [7, 2, 2, 10, 9, 3, 10, 1, 4, 5, 4, 3, 2, 3]\n",
      "Standard deviation community size: 2.9906316307797605\n",
      "Iteration 4: topics_title (communities) [{10}, {49}, {2, 35, 36, 4, 8, 9, 13, 15, 27}, {40, 18, 38, 39}, {17, 19, 20, 21, 22, 24, 25, 26, 60, 31}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {33}, {6}, {16}, {42, 43, 44, 28, 14}, {47, 46, 55}, {0, 1, 50, 5, 41, 11, 12}, {52, 53, 54}, {57, 58, 62}, {56, 51}, {64, 48, 3, 45}] modularity 0.024159854578607066\n",
      "Communities size: [1, 1, 9, 4, 10, 10, 1, 1, 1, 5, 3, 7, 3, 3, 2, 4]\n",
      "Standard deviation community size: 3.151760420780742\n",
      "Iteration 5: topics_title (communities) [{49}, {8, 9, 6}, {10}, {3, 31}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {33}, {2, 35, 4, 36, 27, 13, 15}, {40, 18, 38, 39}, {16}, {42, 43, 44, 28, 14}, {64, 57, 58, 45, 62}, {47, 46, 55}, {0, 1, 50, 5, 41, 11, 12}, {48}, {52, 53, 54}, {56, 51}] modularity 0.02414801596251237\n",
      "Communities size: [1, 3, 1, 2, 10, 9, 1, 7, 4, 1, 5, 5, 3, 7, 1, 3, 2]\n",
      "Standard deviation community size: 2.83331637148054\n",
      "Iteration 6: topics_title (communities) [{2, 35, 36, 4, 27, 13, 15}, {10}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {8, 9, 6}, {49}, {16}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {47, 46, 55}, {33}, {40, 18, 38, 39}, {42, 43, 44, 28, 14}, {57, 58, 62}, {0, 1, 50, 5, 41, 11, 12}, {52, 53, 54}, {56, 51}, {64, 48, 3, 45}] modularity 0.024158853379531334\n",
      "Communities size: [7, 1, 10, 3, 1, 1, 1, 9, 3, 1, 4, 5, 3, 7, 3, 2, 4]\n",
      "Standard deviation community size: 2.791484827695217\n",
      "Iteration 7: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {10}, {6}, {49}, {42, 43, 44, 28, 14}, {2, 35, 36, 4, 8, 9, 13, 15, 27}, {17, 19, 20, 21, 22, 24, 25, 26, 60, 31}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {33}, {40, 18, 38, 39}, {16}, {57, 58, 62}, {47, 46, 55}, {52, 53, 54}, {56, 51}, {64, 48, 3, 45}] modularity 0.024159854578607066\n",
      "Communities size: [7, 1, 1, 1, 5, 9, 10, 10, 1, 4, 1, 3, 3, 3, 2, 4]\n",
      "Standard deviation community size: 3.151760420780742\n",
      "Iteration 8: topics_title (communities) [{49}, {10}, {0, 1, 50, 5, 41, 11, 12}, {2, 35, 36, 4, 8, 9, 13, 15, 27}, {31}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {16}, {47, 46, 55}, {33}, {6}, {40, 18, 38, 39}, {42, 43, 44, 28, 14}, {52, 53, 54}, {56, 51}, {57, 58, 62}, {64, 48, 3, 45}] modularity 0.02416816216846076\n",
      "Communities size: [1, 1, 7, 9, 1, 10, 9, 1, 3, 1, 1, 4, 5, 3, 2, 3, 4]\n",
      "Standard deviation community size: 3.0338345784065\n",
      "Iteration 9: topics_title (communities) [{49, 10}, {6}, {16}, {40, 18, 38, 39}, {17, 19, 20, 21, 22, 24, 25, 26, 60, 31}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {33}, {2, 35, 36, 4, 8, 9, 13, 15, 27}, {42, 43, 44, 28, 14}, {64, 48, 3, 45}, {55, 46, 47}, {0, 1, 50, 5, 41, 11, 12}, {52, 53, 54}, {56, 51}, {57, 58, 62}] modularity 0.024159393070845758\n",
      "Communities size: [2, 1, 1, 4, 10, 10, 1, 9, 5, 4, 3, 7, 3, 2, 3]\n",
      "Standard deviation community size: 3.091206165165234\n",
      "Iteration 10: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {8, 9, 6}, {40, 18, 38, 39}, {16}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {31}, {2, 35, 4, 36, 27, 13, 15}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {33}, {14, 42, 43, 28, 44}, {55, 46, 47}, {49, 10}, {52, 53, 54}, {56, 51}, {57, 58, 62}, {64, 48, 3, 45}] modularity 0.024158391871770032\n",
      "Communities size: [7, 3, 4, 1, 9, 1, 7, 10, 1, 5, 3, 2, 3, 2, 3, 4]\n",
      "Standard deviation community size: 2.7264617638984046\n",
      "Iteration 11: topics_title (communities) [{49}, {2, 35, 36, 38, 8, 9, 13, 15, 27}, {10}, {32, 34, 37, 7, 39, 18, 29, 23, 59, 61, 30, 63}, {4, 6}, {0, 1, 50, 5, 41, 11, 12}, {16}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {33}, {40}, {42, 43, 44, 28, 14}, {64, 48, 3, 45}, {52, 53, 54}, {47, 46, 55}, {56, 51}, {57, 58, 62}] modularity 0.02407935462912729\n",
      "Communities size: [1, 9, 1, 12, 2, 7, 1, 1, 9, 1, 1, 5, 4, 3, 3, 2, 3]\n",
      "Standard deviation community size: 3.3296403995493895\n",
      "Iteration 12: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {2, 35, 36, 4, 27, 13, 15}, {8, 9, 6}, {10}, {49}, {16}, {3, 31}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {47, 46, 55}, {33}, {40, 18, 38, 39}, {42, 43, 44, 28, 14}, {64, 57, 58, 45, 62}, {48}, {52, 53, 54}, {56, 51}] modularity 0.024148015962512374\n",
      "Communities size: [7, 7, 3, 1, 1, 1, 2, 10, 9, 3, 1, 4, 5, 5, 1, 3, 2]\n",
      "Standard deviation community size: 2.83331637148054\n",
      "Iteration 13: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {10}, {8, 9, 6}, {49}, {44, 14}, {51, 28, 47}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {33}, {2, 35, 4, 36, 27, 13, 15}, {40, 18, 38, 39}, {16}, {64, 3, 42, 43, 45, 57, 58, 62}, {48, 46}, {55}, {52, 53, 54}, {56}] modularity 0.024102565345125934\n",
      "Communities size: [7, 1, 3, 1, 2, 3, 1, 9, 10, 1, 7, 4, 1, 8, 2, 1, 3, 1]\n",
      "Standard deviation community size: 3.039350970353318\n",
      "Iteration 14: topics_title (communities) [{49, 10}, {0, 1, 50, 5, 41, 11, 12}, {2, 35, 36, 4, 8, 9, 13, 15, 27}, {31}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {33}, {6}, {40, 18, 38, 39}, {16}, {42, 43, 44, 28, 14}, {47, 46, 55}, {56, 51}, {52, 53, 54}, {57, 58, 62}, {64, 48, 3, 45}] modularity 0.024167700660699447\n",
      "Communities size: [2, 7, 9, 1, 10, 9, 1, 1, 4, 1, 5, 3, 2, 3, 3, 4]\n",
      "Standard deviation community size: 2.9889118003045856\n",
      "Iteration 15: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {2, 35, 4, 36, 27, 13, 15}, {10}, {8, 9, 6}, {49}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {33}, {40, 18, 38, 39}, {16}, {42, 43, 44, 28, 14}, {64, 48, 3, 45}, {55, 46, 47}, {52, 53, 54}, {56, 51}, {57, 58, 62}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}] modularity 0.024158853379531338\n",
      "Communities size: [7, 7, 1, 3, 1, 1, 9, 1, 4, 1, 5, 4, 3, 3, 2, 3, 10]\n",
      "Standard deviation community size: 2.791484827695217\n",
      "Iteration 16: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {49, 10}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {8, 9, 6}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {56, 51}, {33}, {2, 35, 36, 4, 27, 13, 15}, {40, 18, 38, 39}, {16}, {42, 43, 44, 28, 14}, {64, 48, 3, 45}, {47, 46, 55}, {52, 53, 54}, {57, 58, 62}] modularity 0.024158391871770022\n",
      "Communities size: [7, 2, 10, 3, 1, 9, 2, 1, 7, 4, 1, 5, 4, 3, 3, 3]\n",
      "Standard deviation community size: 2.7264617638984046\n",
      "Iteration 17: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {10}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {8, 9, 6}, {49}, {40, 18, 38, 39}, {16}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {2, 35, 4, 36, 27, 13, 15}, {33}, {42, 43, 44, 28, 14}, {47, 46, 55}, {52, 53, 54}, {56, 51}, {57, 58, 62}, {64, 48, 3, 45}] modularity 0.024158853379531334\n",
      "Communities size: [7, 1, 10, 3, 1, 4, 1, 1, 9, 7, 1, 5, 3, 3, 2, 3, 4]\n",
      "Standard deviation community size: 2.791484827695217\n",
      "Iteration 18: topics_title (communities) [{56, 51}, {2, 35, 36, 4, 8, 9, 13, 15, 27}, {49, 10}, {6}, {17, 19, 20, 21, 22, 24, 25, 26, 60, 31}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {33}, {40, 18, 38, 39}, {16}, {42, 43, 44, 28, 14}, {47, 46, 55}, {0, 1, 50, 5, 41, 11, 12}, {52, 53, 54}, {57, 58, 62}, {64, 48, 3, 45}] modularity 0.02415939307084576\n",
      "Communities size: [2, 9, 2, 1, 10, 10, 1, 4, 1, 5, 3, 7, 3, 3, 4]\n",
      "Standard deviation community size: 3.091206165165234\n",
      "Iteration 19: topics_title (communities) [{32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {8, 9, 6}, {10}, {49}, {0, 1, 50, 5, 41, 11, 12}, {40, 18, 38, 39}, {42, 43, 44, 28, 14}, {17, 19, 20, 21, 22, 24, 25, 26, 60, 31}, {56, 51}, {47, 46, 55}, {33}, {2, 35, 4, 36, 27, 13, 15}, {16}, {64, 48, 3, 45}, {52, 53, 54}, {57, 58, 62}] modularity 0.02415054578967764\n",
      "Communities size: [10, 3, 1, 1, 7, 4, 5, 10, 2, 3, 1, 7, 1, 4, 3, 3]\n",
      "Standard deviation community size: 2.9040650388722358\n",
      "Iteration 20: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {49, 10}, {40, 18, 38, 39}, {2, 35, 36, 4, 8, 9, 13, 15, 27}, {16}, {31}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {51, 47, 55}, {33}, {6}, {42, 43, 44, 28, 14}, {64, 3, 57, 45, 62}, {48, 46}, {52, 53, 54}, {56, 58}] modularity 0.024152716420591476\n",
      "Communities size: [7, 2, 4, 9, 1, 1, 10, 9, 3, 1, 1, 5, 5, 2, 3, 2]\n",
      "Standard deviation community size: 3.0304444806001642\n",
      "Iteration 21: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {8, 9, 6}, {10}, {49}, {2, 35, 36, 4, 27, 13, 15}, {16}, {31}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {18, 28, 47}, {33}, {40, 38, 39}, {42, 43, 44, 14}, {48, 46}, {52, 53, 54}, {55}, {56, 51}, {57, 58, 62}, {64, 3, 45}] modularity 0.0241371978243493\n",
      "Communities size: [7, 3, 1, 1, 7, 1, 1, 10, 9, 3, 1, 3, 4, 2, 3, 1, 2, 3, 3]\n",
      "Standard deviation community size: 2.720599575435808\n",
      "Iteration 22: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {2, 35, 36, 4, 27, 13, 15}, {8, 9, 6}, {49, 10}, {17, 19, 20, 21, 22, 24, 25, 26, 60, 31}, {18, 28, 47}, {56, 51}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {33}, {40, 38, 39}, {16}, {44, 14}, {57, 58, 62}, {48, 46}, {55}, {64, 3, 42, 43, 45, 52, 53, 54}] modularity 0.024122055805282483\n",
      "Communities size: [7, 7, 3, 2, 10, 3, 2, 10, 1, 3, 1, 2, 3, 2, 1, 8]\n",
      "Standard deviation community size: 3.091697551507909\n",
      "Iteration 23: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {49, 10}, {6}, {48, 46}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {18, 28, 47}, {56, 51}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {33}, {2, 35, 4, 36, 8, 9, 13, 15, 27}, {40, 38, 39}, {16}, {44, 14}, {64, 3, 42, 43, 45, 52, 53, 54}, {55}, {57, 58, 62}] modularity 0.024139672184065607\n",
      "Communities size: [7, 2, 1, 2, 1, 9, 3, 2, 10, 1, 9, 3, 1, 2, 8, 1, 3]\n",
      "Standard deviation community size: 3.2035879539270824\n",
      "Iteration 24: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {4, 6}, {49, 10}, {2, 35, 36, 38, 8, 9, 13, 15, 27}, {16}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {47, 46, 55}, {32, 34, 37, 7, 39, 18, 61, 23, 59, 29, 30, 63}, {33}, {40}, {42, 43, 44, 28, 14}, {52, 53, 54}, {56, 51}, {57, 58, 62}, {64, 48, 3, 45}] modularity 0.02407889312136598\n",
      "Communities size: [7, 2, 2, 9, 1, 1, 9, 3, 12, 1, 1, 5, 3, 2, 3, 4]\n",
      "Standard deviation community size: 3.3065985166028247\n",
      "Iteration 25: topics_title (communities) [{10}, {8, 9, 6}, {0, 1, 50, 5, 41, 11, 12}, {40, 18, 38, 39}, {16}, {3, 31}, {56, 51}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {47, 46, 55}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {33}, {2, 35, 36, 4, 27, 13, 15}, {42, 43, 44, 28, 14}, {64, 57, 58, 45, 62}, {48}, {49}, {52, 53, 54}] modularity 0.02414801596251237\n",
      "Communities size: [1, 3, 7, 4, 1, 2, 2, 9, 3, 10, 1, 7, 5, 5, 1, 1, 3]\n",
      "Standard deviation community size: 2.83331637148054\n",
      "Iteration 26: topics_title (communities) [{49, 10}, {8, 9, 6}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {16}, {47, 46, 55}, {33}, {2, 35, 36, 4, 27, 13, 15}, {40, 18, 38, 39}, {42, 43, 44, 28, 14}, {0, 1, 50, 5, 41, 11, 12}, {56, 51}, {52, 53, 54}, {57, 58, 62}, {64, 48, 3, 45}] modularity 0.024158391871770026\n",
      "Communities size: [2, 3, 1, 9, 10, 1, 3, 1, 7, 4, 5, 7, 2, 3, 3, 4]\n",
      "Standard deviation community size: 2.7264617638984046\n",
      "Iteration 27: topics_title (communities) [{49}, {0, 1, 50, 5, 41, 11, 12}, {2, 35, 4, 36, 27, 13, 15}, {10}, {8, 9, 6}, {16}, {31}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {33}, {40, 18, 38, 39}, {42, 43, 44, 28, 14}, {47, 46, 55}, {52, 53, 54}, {56, 51}, {57, 58, 62}, {64, 48, 3, 45}] modularity 0.02415885337953133\n",
      "Communities size: [1, 7, 7, 1, 3, 1, 1, 10, 9, 1, 4, 5, 3, 3, 2, 3, 4]\n",
      "Standard deviation community size: 2.791484827695217\n",
      "Iteration 28: topics_title (communities) [{49, 10}, {8, 9, 6}, {56, 51}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {2, 35, 4, 36, 27, 13, 15}, {33}, {40, 18, 38, 39}, {16}, {42, 43, 44, 28, 14}, {47, 46, 55}, {0, 1, 50, 5, 41, 11, 12}, {52, 53, 54}, {57, 58, 62}, {64, 48, 3, 45}] modularity 0.024158391871770026\n",
      "Communities size: [2, 3, 2, 1, 9, 10, 7, 1, 4, 1, 5, 3, 7, 3, 3, 4]\n",
      "Standard deviation community size: 2.7264617638984046\n",
      "Iteration 29: topics_title (communities) [{32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {6}, {49, 10}, {0, 1, 50, 5, 41, 11, 12}, {2, 35, 36, 4, 8, 9, 13, 15, 27}, {17, 19, 20, 21, 22, 24, 25, 26, 60, 31}, {33}, {40, 18, 38, 39}, {16}, {42, 43, 44, 28, 14}, {47, 46, 55}, {52, 53, 54}, {56, 51}, {57, 58, 62}, {64, 48, 3, 45}] modularity 0.02415939307084575\n",
      "Communities size: [10, 1, 2, 7, 9, 10, 1, 4, 1, 5, 3, 3, 2, 3, 4]\n",
      "Standard deviation community size: 3.0912061651652345\n",
      "Iteration 30: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {10}, {8, 9, 6}, {49}, {40, 18, 38, 39}, {16}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {33}, {2, 35, 4, 36, 27, 13, 15}, {44, 42, 43, 28, 14}, {55, 46, 47}, {56, 51}, {52, 53, 54}, {57, 58, 62}, {64, 48, 3, 45}] modularity 0.024158853379531338\n",
      "Communities size: [7, 1, 3, 1, 4, 1, 1, 9, 10, 1, 7, 5, 3, 2, 3, 3, 4]\n",
      "Standard deviation community size: 2.7914848276952173\n",
      "Iteration 31: topics_title (communities) [{49, 10}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {0, 1, 50, 5, 41, 11, 12}, {44, 14}, {2, 35, 36, 4, 8, 9, 13, 15, 27}, {51, 28, 47}, {17, 19, 20, 21, 22, 24, 25, 26, 60, 31}, {33}, {6}, {40, 18, 38, 39}, {56}, {16}, {55}, {48, 46}, {57, 58, 62}, {64, 3, 42, 43, 45, 52, 53, 54}] modularity 0.02413163876470964\n",
      "Communities size: [2, 10, 7, 2, 9, 3, 10, 1, 1, 4, 1, 1, 1, 2, 3, 8]\n",
      "Standard deviation community size: 3.3628252630786513\n",
      "Iteration 32: topics_title (communities) [{49}, {8, 9, 6}, {10}, {40, 38, 39}, {16}, {17, 19, 20, 21, 22, 24, 25, 26, 60, 31}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {56, 51}, {33}, {2, 35, 4, 36, 27, 13, 15}, {42, 43, 44, 14}, {64, 3, 45}, {48, 46}, {18, 28, 47}, {0, 1, 50, 5, 41, 11, 12}, {52, 53, 54}, {55}, {57, 58, 62}] modularity 0.0241288902344956\n",
      "Communities size: [1, 3, 1, 3, 1, 10, 10, 2, 1, 7, 4, 3, 2, 3, 7, 3, 1, 3]\n",
      "Standard deviation community size: 2.850709246815\n",
      "Iteration 33: topics_title (communities) [{49, 10}, {8, 9, 6}, {0, 1, 50, 5, 41, 11, 12}, {16}, {31}, {56, 51}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {33}, {2, 35, 36, 4, 27, 13, 15}, {40, 18, 38, 39}, {42, 43, 44, 28, 14}, {64, 48, 3, 45}, {47, 46, 55}, {52, 53, 54}, {57, 58, 62}] modularity 0.024158391871770022\n",
      "Communities size: [2, 3, 7, 1, 1, 2, 9, 10, 1, 7, 4, 5, 4, 3, 3, 3]\n",
      "Standard deviation community size: 2.7264617638984046\n",
      "Iteration 34: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {10}, {8, 9, 6}, {49}, {2, 35, 36, 4, 27, 13, 15}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {47, 46, 55}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {33}, {40, 18, 38, 39}, {16}, {42, 43, 44, 28, 14}, {52, 53, 54}, {56, 51}, {57, 58, 62}, {64, 48, 3, 45}] modularity 0.024158853379531334\n",
      "Communities size: [7, 1, 3, 1, 7, 1, 9, 3, 10, 1, 4, 1, 5, 3, 2, 3, 4]\n",
      "Standard deviation community size: 2.7914848276952173\n",
      "Iteration 35: topics_title (communities) [{8, 9, 6}, {49, 10}, {0, 1, 50, 5, 41, 11, 12}, {48, 46}, {16}, {18, 28, 47}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {56, 51}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {33}, {2, 35, 4, 36, 27, 13, 15}, {40, 38, 39}, {42, 43, 44, 14}, {52, 53, 54}, {55}, {57, 58, 62}, {64, 3, 45}] modularity 0.024136736316587987\n",
      "Communities size: [3, 2, 7, 2, 1, 3, 1, 9, 2, 10, 1, 7, 3, 4, 3, 1, 3, 3]\n",
      "Standard deviation community size: 2.690288891734056\n",
      "Iteration 36: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {8, 9, 6}, {49, 10}, {47, 46, 55}, {40, 18, 38, 39}, {16}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {2, 35, 4, 36, 27, 13, 15}, {33}, {42, 43, 44, 28, 14}, {64, 48, 3, 45}, {52, 53, 54}, {56, 51}, {57, 58, 62}, {31}] modularity 0.02415839187177003\n",
      "Communities size: [7, 10, 3, 2, 3, 4, 1, 9, 7, 1, 5, 4, 3, 2, 3, 1]\n",
      "Standard deviation community size: 2.7264617638984046\n",
      "Iteration 37: topics_title (communities) [{49, 10}, {0, 1, 50, 5, 41, 11, 12}, {2, 35, 36, 4, 27, 13, 15}, {8, 9, 6}, {40, 18, 38, 39}, {16}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {47, 46, 55}, {32, 34, 37, 7, 61, 23, 59, 29, 30, 63}, {33}, {42, 43, 44, 28, 14}, {56, 51}, {52, 53, 54}, {57, 58, 62}, {64, 48, 3, 45}] modularity 0.024158391871770026\n",
      "Communities size: [2, 7, 7, 3, 4, 1, 1, 9, 3, 10, 1, 5, 2, 3, 3, 4]\n",
      "Standard deviation community size: 2.7264617638984046\n",
      "Iteration 38: topics_title (communities) [{0, 1, 50, 5, 41, 11, 12}, {8, 9, 6}, {31}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {33}, {2, 35, 4, 36, 27, 13, 15}, {40, 18, 38, 39}, {16}, {42, 43, 44, 28, 14}, {64, 48, 3, 45}, {47, 46, 55}, {49, 10}, {52, 53, 54}, {56, 51}, {57, 58, 62}] modularity 0.024158391871770026\n",
      "Communities size: [7, 3, 1, 9, 10, 1, 7, 4, 1, 5, 4, 3, 2, 3, 2, 3]\n",
      "Standard deviation community size: 2.7264617638984046\n",
      "Iteration 39: topics_title (communities) [{49}, {0, 1, 50, 5, 41, 11, 12}, {8, 9, 6}, {10}, {32, 34, 37, 7, 29, 23, 59, 61, 30, 63}, {17, 19, 20, 21, 22, 24, 25, 26, 60}, {2, 35, 36, 4, 27, 13, 15}, {33}, {40, 18, 38, 39}, {16}, {42, 43, 44, 28, 14}, {64, 57, 58, 45, 62}, {48}, {47, 46, 55}, {56, 51}, {52, 53, 54}, {3, 31}] modularity 0.024148015962512374\n",
      "Communities size: [1, 7, 3, 1, 10, 9, 7, 1, 4, 1, 5, 5, 1, 3, 2, 3, 2]\n",
      "Standard deviation community size: 2.83331637148054\n",
      "Best SD: inf, Best iteration: 0\n"
     ]
    }
   ],
   "source": [
    "# Set num_topics to be 1/4 of the number of chunks, or 8, which ever is smaller\n",
    "num_topics = min(int(num_1_chunks / 4), 8)\n",
    "topics_out = get_topics(summary_similarity_matrix, num_topics = num_topics, bonus_constant = 0.2)\n",
    "chunk_topics = topics_out['chunk_topics']\n",
    "topics = topics_out['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing each chunk title belonging to what community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3:The Louvain Method for Community Detection ',\n",
       " '3:Optimizing Modularity in Community Detection ',\n",
       " '4:Understanding Modularity in Weighted Graphs ',\n",
       " '11:   Community Detection in Graphs ',\n",
       " '0:   The Kronecker Delta Function and Modularity Calculation ',\n",
       " '3:  The Louvain Method for Maximizing Modularity ',\n",
       " '0:   Calculating Modularity Change in Network Communities ',\n",
       " '9:Title ',\n",
       " '4:   Modularity Optimization in Community Detection ',\n",
       " '4:   Modularity Optimization in Network Communities ',\n",
       " '1:Modularity Maximization Algorithm for Community Detection ',\n",
       " '3:The Limitations of Louvain Algorithm ',\n",
       " '3:Comparison of Modularity Optimization Methods for Non-Overlapping Community Detection ',\n",
       " '4:The Importance and Limitations of Modularity in Network Analysis ',\n",
       " '8:The Importance of Community Structure in Network Analysis ',\n",
       " '4:Understanding Modularity in Network Analysis ',\n",
       " '2:   Community Membership and Node Score ',\n",
       " '5:   Understanding Adjacency Matrices in Networks ',\n",
       " '9:Title ',\n",
       " '5: Expected Number of Edges Between Nodes ',\n",
       " '5:   The Connection Between Stubs in a Random Graph ',\n",
       " '5:   Random Connections in Network Nodes ',\n",
       " '5:   Counting Full Edges Between Two Vertices ',\n",
       " '9:Title ',\n",
       " '5:   Approximations for Random Networks ',\n",
       " '5:   Approximating Expected Number of Edges in Random Networks ',\n",
       " '5:   Approximating Edge Probability in Graphs ',\n",
       " '4:   Understanding Modularity in Network Communities ',\n",
       " '8:Title ',\n",
       " '9:  Understanding Set Notation ',\n",
       " '9:Title ',\n",
       " '6:   Community Detection and Edge Fraction ',\n",
       " '9:Title ',\n",
       " '7:   Community Detection in Undirected Networks ',\n",
       " '9:   The Importance of Delta and Q in Machine Learning ',\n",
       " '4:   Modularity Matrix and its Role in Network Analysis ',\n",
       " '4:  Understanding Modularity in Network Analysis ',\n",
       " '9:Title ',\n",
       " '4:The Limitations of Modularity Maximization in Detecting Community Structure ',\n",
       " '9:Title ',\n",
       " '10:The Limitations of Multiresolution Methods for Modularity Optimization ',\n",
       " '3:Software Tools for Graph Clustering ',\n",
       " '8:Understanding Community Structure in Complex Networks ',\n",
       " '8:Understanding Community Structure in Networks ',\n",
       " '8:The Importance of Community Structures in Networks ',\n",
       " '11:The Importance of Community Detection in Network Science ',\n",
       " '12:Algorithms for Finding Communities ',\n",
       " '14:Title ',\n",
       " '12:Approximations for Community Detection in Multidimensional Networks ',\n",
       " '13:The Girvan–Newman Algorithm: A Method for Community Detection ',\n",
       " '3:Maximizing Modularity in Network Clustering ',\n",
       " '14:Title ',\n",
       " '15:Community Detection in Networks ',\n",
       " '15:   Understanding Clique Graphs and Their Role in Community Detection ',\n",
       " '15:   Identifying Communities in Networks Using k-cliques ',\n",
       " '14:   Community Detection in Latent Feature Spaces ',\n",
       " '16:\"Community Detection Benchmarking: Evaluating Algorithms on Stochastic Block Models\" ',\n",
       " '11:The Impact of Community Structure on Detectability ',\n",
       " '16:   The Fundamental Limit of Detecting Communities in Networks ',\n",
       " '9:   Scaling in Sparse Cases ',\n",
       " '5:   Constant Average Degree in Graphs ',\n",
       " '9:Title ',\n",
       " '11: Detecting Communities in Networks ',\n",
       " '9:  The Importance of Maintaining a Balance Between Caffeine Intake and Caffeine Withdrawal ',\n",
       " '11:Community Detection in Graphs ']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_community = [f'{i}:{title}' for i, title in zip(chunk_topics, stage_1_titles)]\n",
    "title_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a heatmap of this array\n",
    "plt.figure(figsize = (10, 4))\n",
    "plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "# Draw vertical black lines for every 1 of the x-axis \n",
    "for i in range(1, len(chunk_topics)):\n",
    "  plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250):\n",
    "  print(f'Stage 2 start time {datetime.now()}')\n",
    "  \n",
    "  # Prompt that passes in all the titles of a topic, and asks for an overall title of the topic\n",
    "  title_prompt_template = \"\"\"Write an informative title that summarizes each of the following groups of titles. Make sure that the titles capture as much information as possible, \n",
    "  and are different from each other:\n",
    "  {text}\n",
    "  \n",
    "  Return your answer in a numbered list, with new line separating each title: \n",
    "  1. Title 1\n",
    "  2. Title 2\n",
    "  3. Title 3\n",
    "\n",
    "  TITLES:\n",
    "  \"\"\"\n",
    "\n",
    "  map_prompt_template = \"\"\"Wite a 75-100 word summary of the following text:\n",
    "    {text}\n",
    "\n",
    "    CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "  combine_prompt_template = 'Write a ' + str(summary_num_words) + \"\"\"-word summary of the following, removing irrelevant information. Finish your answer:\n",
    "  {text}\n",
    "  \"\"\" + str(summary_num_words) + \"\"\"-WORD SUMMARY:\"\"\"\n",
    "\n",
    "  title_prompt = PromptTemplate(template=title_prompt_template, input_variables=[\"text\"])\n",
    "  map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "  combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "  topics_data = []\n",
    "  for c in topics:\n",
    "    topic_data = {\n",
    "      'summaries': [stage_1_outputs[chunk_id]['summary'] for chunk_id in c],\n",
    "      'titles': [stage_1_outputs[chunk_id]['title'] for chunk_id in c]\n",
    "    }\n",
    "    topic_data['summaries_concat'] = ' '.join(topic_data['summaries'])\n",
    "    topic_data['titles_concat'] = ', '.join(topic_data['titles'])\n",
    "    topics_data.append(topic_data)\n",
    "    \n",
    "  # Get a list of each community's summaries (concatenated)\n",
    "  topics_summary_concat = [c['summaries_concat'] for c in topics_data]\n",
    "  topics_titles_concat = [c['titles_concat'] for c in topics_data]\n",
    "\n",
    "  # Concat into one long string to do the topic title creation\n",
    "  topics_titles_concat_all = ''''''\n",
    "  for i, c in enumerate(topics_titles_concat):\n",
    "    topics_titles_concat_all += f'''{i+1}. {c}\n",
    "    '''\n",
    "  \n",
    "  # print('topics_titles_concat_all', topics_titles_concat_all)\n",
    "\n",
    "  title_llm = OpenAI(temperature=0)\n",
    "  title_llm_chain = LLMChain(llm = title_llm, prompt = title_prompt)\n",
    "  title_llm_chain_input = [{'text': topics_titles_concat_all}]\n",
    "  title_llm_chain_results = title_llm_chain.apply(title_llm_chain_input)\n",
    "  \n",
    "  \n",
    "  # Split by new line\n",
    "  titles = title_llm_chain_results[0]['text'].split('\\n')\n",
    "  # Remove any empty titles\n",
    "  titles = [t for t in titles if t != '']\n",
    "  # Remove spaces at start or end of each title\n",
    "  titles = [t.strip() for t in titles]\n",
    "\n",
    "  map_llm = OpenAI(temperature=0)\n",
    "  reduce_llm = OpenAI(temperature=0)\n",
    "\n",
    "  # Run the map-reduce chain\n",
    "  docs = [Document(page_content=t) for t in topics_summary_concat]\n",
    "  chain = load_summarize_chain(chain_type=\"map_reduce\", map_prompt = map_prompt, combine_prompt = combine_prompt, return_intermediate_steps = True,\n",
    "                              llm = map_llm, reduce_llm = reduce_llm)\n",
    "\n",
    "  output = chain({\"input_documents\": docs}, return_only_outputs = True)\n",
    "  summaries = output['intermediate_steps']\n",
    "  stage_2_outputs = [{'title': t, 'summary': s} for t, s in zip(titles, summaries)]\n",
    "  final_summary = output['output_text']\n",
    "\n",
    "  # Return: stage_1_outputs (title and summary), stage_2_outputs (title and summary), final_summary, chunk_allocations\n",
    "  out = {\n",
    "    'stage_2_outputs': stage_2_outputs,\n",
    "    'final_summary': final_summary\n",
    "  }\n",
    "  print(f'Stage 2 done time {datetime.now()}')\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 start time 2024-01-10 09:01:13.446729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/ml-learning/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAI instead.\n",
      "  warn_deprecated(\n",
      "/workspaces/ml-learning/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAI instead.\n",
      "  warn_deprecated(\n",
      "/workspaces/ml-learning/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAI instead.\n",
      "  warn_deprecated(\n",
      "/workspaces/ml-learning/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/workspaces/ml-learning/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 done time 2024-01-10 09:01:25.488847\n"
     ]
    }
   ],
   "source": [
    "# Query GPT-3 to get a summarized title for each topic_data\n",
    "out = summarize_stage_2(stage_1_outputs, topics, summary_num_words = 250)\n",
    "stage_2_outputs = out['stage_2_outputs']\n",
    "stage_2_titles = [e['title'] for e in stage_2_outputs]\n",
    "stage_2_summaries = [e['summary'] for e in stage_2_outputs]\n",
    "final_summary = out['final_summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stage_2_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '1. Exploring Modularity and Delta Function in Network Analysis',\n",
       "  'summary': \" The Kronecker delta function is used to calculate the modularity of a community within a network. This measures how distinct and well-connected the community is. The formula involves subtracting the expected number of edges from the actual number of edges within the community, then squaring the result. This helps identify cohesive communities within a network. The process of moving a node from its own community to a neighbor's community is important in understanding network communities and is calculated using a similar equation.\"},\n",
       " {'title': '2. Maximizing Modularity: An Algorithm for Community Detection',\n",
       "  'summary': '\\nThe algorithm involves two phases: grouping nodes into communities based on connections and creating a new network with self-loops and weighted edges. It has been used to partition social networks, track dynamic communities in mobile phone networks, and detect species in network-based models.'},\n",
       " {'title': '3. Understanding Community Membership and Node Scores',\n",
       "  'summary': \"\\nThe text explains the significance of community membership and node score in a network. A node's score is determined by its belonging to a specific community, with a score of 1 for community 1 and -1 for community 2. This information is crucial in understanding the connections and relationships within a network.\"},\n",
       " {'title': '4. The Louvain Method: Optimizing Modularity in Network Clustering',\n",
       "  'summary': ' The Louvain Method is a popular algorithm for finding non-overlapping communities in large networks. It uses modularity optimization, which measures the density of edges within communities compared to edges between communities, to group nodes in a network. This method is efficient and has been shown to outperform other similar methods in terms of speed and modularity value. However, it has limitations such as a resolution limit for detecting smaller clusters. Other methods, such as the Leiden algorithm, have been developed to address these limitations. The text also discusses the importance of speed and modularity value when comparing different community detection methods. '},\n",
       " {'title': '5. Understanding Modularity in Weighted Graphs and its Limitations',\n",
       "  'summary': ' Modularity is a measure of how well a graph can be divided into communities. It takes into account the number of edges within a community compared to the expected number. This measure is used in community detection algorithms to identify distinct groups within a network. The modularity matrix is a key tool in network analysis, used to measure the strength of communities within a network. Modularity optimization involves calculating the difference between the sum of weights of links within a community and the expected sum of weights based on a null model. This measure helps identify communities within a network by maximizing the difference between these two values. However, modularity maximization is not statistically consistent and has a resolution limit, making it unable to detect small communities. Modularity is a useful concept for identifying and analyzing different communities within a network.'},\n",
       " {'title': '6. Analyzing Adjacency Matrices and Random Connections in Networks',\n",
       "  'summary': '\\nThis text explains how to represent a network using an adjacency matrix and discusses the expected number of edges between two nodes in a randomly rewired network. It also discusses the relationship between stubs in a random graph and the probability of connections between nodes. The text explains how to calculate the total number of edges between two vertices and discusses approximations for large random networks. It also discusses the concept of achieving a constant average degree in graphs, which can be useful in various applications.'},\n",
       " {'title': '7. Community Detection and Edge Fraction: A Comprehensive Analysis',\n",
       "  'summary': \" The formula calculates the fraction of edges connected to vertices in a specific community, using the total number of edges and the adjacency matrix. This helps identify the strength of connections within the community and understand the network's structure.\"},\n",
       " {'title': '8. Community Detection in Undirected Networks: Methods and Techniques',\n",
       "  'summary': ' The text explores community detection in a network with 10 nodes and 12 edges. It introduces a new way to measure the belonging of a vertex to a group, which is helpful for optimizing algorithms. The results of the optimal community partitions are shown in a figure.'},\n",
       " {'title': '9. The Importance of Community Structures in Complex Networks',\n",
       "  'summary': ' This text discusses the concept of community structure in complex networks, where nodes are grouped based on their dense connections. This is important in studying various networks and can be applied in community search. Community structure refers to groups of nodes that are more connected within themselves than with the rest of the network. Identifying these communities can provide insights into network function and topology. However, not all networks exhibit meaningful community structure, making it important to use advanced models. '},\n",
       " {'title': '10. Delta and Q: Key Factors in Machine Learning and Sparse Cases',\n",
       "  'summary': ' This text discusses the use of delta and Q in machine learning, as well as the importance of set notation in mathematics. It also explains the concept of expected value and its applications in various fields. Additionally, the text touches on the impact of social media on mental health and the effects of caffeine intake and withdrawal on the body.'},\n",
       " {'title': '11. The Challenges of Multiresolution Methods for Modularity Optimization',\n",
       "  'summary': ' Multiresolution methods attempt to overcome the resolution limit in modularity optimization by adding a resistance or null-case term parameter. However, these methods are not effective for highly heterogeneous communities with varying sizes.'},\n",
       " {'title': '12. Community Detection in Graphs: Impact and Importance',\n",
       "  'summary': ' This text introduces the concept of community detection in graphs and provides resources for implementing algorithms. It explains how communities are identified within a network and their importance in understanding network structure and relationships. The text also discusses the challenges of detecting communities when the difference between internal and external connections is small. Community detection has practical applications in fields such as social networks and can improve algorithms and predict missing or false links in network data. '},\n",
       " {'title': '13. Approximations and Algorithms for Community Detection in Multidimensional Networks',\n",
       "  'summary': ' This text explores different methods for identifying communities in multidimensional networks, including measuring edge density within and between clusters and considering alternative definitions of communities. It also discusses challenges in finding communities and two methods, minimum-cut and hierarchical clustering, for identifying them. While these methods have varying levels of success, they may not be ideal for all types of networks.'},\n",
       " {'title': '14. The Girvan-Newman Algorithm: A Powerful Tool for Community Detection',\n",
       "  'summary': ' The Girvan-Newman algorithm is a method for finding communities in a network by removing edges between them. However, it is slow and not suitable for large networks. Another method, modularity maximization, uses a benefit function to identify communities with high modularity.'},\n",
       " {'title': '15. Community Detection in Latent Feature Spaces: Techniques and Applications',\n",
       "  'summary': ' The text explores using representation learning and clustering methods to detect community structures in networks. It mentions techniques for projecting networks onto a latent space and using Euclidean and Hypergeometric spaces.'},\n",
       " {'title': '16. Identifying Communities in Networks Using Clique Graphs and k-cliques',\n",
       "  'summary': '\\nThis text discusses different methods for identifying communities in networks, with a focus on using cliques in graph analysis. It explains how nodes can belong to multiple cliques, leading to an overlapping community structure. The two main approaches are finding maximal cliques and using cliques of a fixed size. These methods are commonly used in social network analysis software. Clique graphs, which represent cliques and their overlap, can be used to determine community membership of nodes. The clique percolation method defines communities as percolation clusters of k-cliques. This text also explains how to identify communities by finding all k-cliques and defining them as adjacent if they share k-1 nodes. Communities are essentially the connected components in the clique graph.'},\n",
       " {'title': '17. Community Detection Benchmarking: Evaluating Algorithms on Stochastic Block Models and the Fundamental Limit of Detecting Communities in Networks',\n",
       "  'summary': \"\\nThe text discusses the use of benchmark graphs to evaluate community detection algorithms. These benchmarks start with a defined community structure and gradually make it more challenging for algorithms to detect. Measures such as normalized mutual information are used to compare the algorithm's solution to the original structure. The text also mentions a fundamental limit on our ability to detect communities in networks, regardless of the algorithm or resources available. In a stochastic block model, a network will have community structure if the connection probability within groups is greater than between groups, but this limit still applies in sparse cases.\"}]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_2_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "The Kronecker delta function is a useful tool for calculating the modularity of communities within a network. This measure helps identify cohesive and well-connected groups within a network by comparing the actual number of edges within a community to the expected number. The Louvain Method is a popular algorithm that uses modularity optimization to efficiently group nodes in a network. However, it has limitations such as a resolution limit for detecting smaller clusters. Other methods, such as the Leiden algorithm, have been developed to address these limitations. Modularity is also used in community detection to measure the strength of connections within a community. It involves calculating the difference between the sum of weights of links within a community and the expected sum of weights based on a null model. However, modularity maximization is not statistically consistent and has a resolution limit, making it unable to detect small communities. The concept of community structure in complex networks is important in understanding network function and topology. However, not all networks exhibit meaningful community structure, making it important to use advanced models. Community detection has practical applications in fields such as social networks and can improve algorithms and predict missing or false links in network data. Different methods, such as multiresolution methods and representation learning, have been developed to overcome the challenges of detecting communities"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(f'{final_summary}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Llamaindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state if the union ListIndex basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With llamaindex\n",
    "\n",
    "import openai\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "from llama_index import (\n",
    "  SimpleDirectoryReader,\n",
    "  ListIndex,\n",
    "  ServiceContext,\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=1024)\n",
    "\n",
    "documents = SimpleDirectoryReader('/workspaces/ml-learning/src/phages/data/test1').load_data()\n",
    "\n",
    "index = ListIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m WikipediaReader \u001b[38;5;241m=\u001b[39m download_loader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWikipediaReader\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m loader \u001b[38;5;241m=\u001b[39m WikipediaReader()\n\u001b[0;32m----> 9\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLouvain Method\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mModularity (networks)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCommunity structure\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# pipeline = IngestionPipeline(transformations=[TokenTextSplitter()])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# nodes = pipeline.run(documents=documents)\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/download/llamahub_modules/wikipedia/base.py:29\u001b[0m, in \u001b[0;36mWikipediaReader.load_data\u001b[0;34m(self, pages, lang, **load_kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pages:\n\u001b[1;32m     28\u001b[0m     wikipedia\u001b[38;5;241m.\u001b[39mset_lang(lang)\n\u001b[0;32m---> 29\u001b[0m     page_content \u001b[38;5;241m=\u001b[39m \u001b[43mwikipedia\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mload_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     30\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(Document(text\u001b[38;5;241m=\u001b[39mpage_content))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/wikipedia/wikipedia.py:270\u001b[0m, in \u001b[0;36mpage\u001b[0;34m(title, pageid, auto_suggest, redirect, preload)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m title \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m auto_suggest:\n\u001b[0;32m--> 270\u001b[0m     results, suggestion \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuggestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m       title \u001b[38;5;241m=\u001b[39m suggestion \u001b[38;5;129;01mor\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/wikipedia/util.py:28\u001b[0m, in \u001b[0;36mcache.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[key]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/wikipedia/wikipedia.py:103\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(query, results, suggestion)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m suggestion:\n\u001b[1;32m    101\u001b[0m   search_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrinfo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuggestion\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 103\u001b[0m raw_results \u001b[38;5;241m=\u001b[39m \u001b[43m_wiki_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m raw_results:\n\u001b[1;32m    106\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m raw_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP request timed out.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPool queue is full\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/wikipedia/wikipedia.py:737\u001b[0m, in \u001b[0;36m_wiki_request\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    734\u001b[0m   wait_time \u001b[38;5;241m=\u001b[39m (RATE_LIMIT_LAST_CALL \u001b[38;5;241m+\u001b[39m RATE_LIMIT_MIN_WAIT) \u001b[38;5;241m-\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    735\u001b[0m   time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mint\u001b[39m(wait_time\u001b[38;5;241m.\u001b[39mtotal_seconds()))\n\u001b[0;32m--> 737\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RATE_LIMIT:\n\u001b[1;32m    740\u001b[0m   RATE_LIMIT_LAST_CALL \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/requests/sessions.py:725\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 725\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/requests/sessions.py:725\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 725\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    727\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/requests/sessions.py:266\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, prepared_request, resp\u001b[38;5;241m.\u001b[39mraw)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "from llama_index.node_parser import TokenTextSplitter\n",
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "documents = loader.load_data(pages=['Louvain Method', 'Modularity (networks)', 'Community structure'])\n",
    "\n",
    "# pipeline = IngestionPipeline(transformations=[TokenTextSplitter()])\n",
    "\n",
    "# nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ListIndex.build_index_from_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom node parser\n",
    "\n",
    "# https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/root.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=OpenAIEmbedding(model_name='text-embedding-ada-002', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7fc2b551f610>, additional_kwargs={}, api_key='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', api_base='https://api.openai.com/v1', api_version='', max_retries=10, timeout=60.0, default_headers=None, reuse_client=True), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7fc2b551f610>, id_func=<function default_id_func at 0x7fc2b68e9ee0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.logger.base.LlamaLogger object at 0x7fc2b5082a90>, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7fc2b551f610>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_service_context': ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=OpenAIEmbedding(model_name='text-embedding-ada-002', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7fc2b5a674d0>, additional_kwargs={}, api_key='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', api_base='https://api.openai.com/v1', api_version='', max_retries=10, timeout=60.0, default_headers=None, reuse_client=True), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7fc2b5a674d0>, id_func=<function default_id_func at 0x7fc2b68e9ee0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.logger.base.LlamaLogger object at 0x7fc2b5a67750>, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7fc2b5a674d0>), '_storage_context': StorageContext(docstore=<llama_index.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x7fc2b5c1b510>, index_store=<llama_index.storage.index_store.simple_index_store.SimpleIndexStore object at 0x7fc2b5a66ed0>, vector_stores={'default': <llama_index.vector_stores.simple.SimpleVectorStore object at 0x7fc2b6191290>, 'image': <llama_index.vector_stores.simple.SimpleVectorStore object at 0x7fc2b5a67250>}, graph_store=<llama_index.graph_stores.simple.SimpleGraphStore object at 0x7fc2b5a66f10>), '_docstore': <llama_index.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x7fc2b5c1b510>, '_show_progress': False, '_vector_store': <llama_index.vector_stores.simple.SimpleVectorStore object at 0x7fc2b6191290>, '_graph_store': <llama_index.graph_stores.simple.SimpleGraphStore object at 0x7fc2b5a66f10>, '_index_struct': IndexList(index_id='b97932b7-2774-4835-a84a-b863930ccf4e', summary=None, nodes=['2d3205d1-022c-4ce4-80d4-5c569cc85eb0', '5bfccebb-2737-4e12-90e3-51dcc8001648', '26562ada-fd18-4eb2-b75a-0f66e7c1c53d', 'c696beaa-2515-459c-872b-4853f3c3baf0', 'f8be7847-6491-42c3-8134-c7608d52b0ac', '2233ee04-9cca-42dd-aa0b-04601f12818d', '125d003f-caee-4613-9750-1ae5526779d9', 'e9cd869f-6728-449d-8cbd-01eab4b48363', 'dce4caa0-7250-45f9-b196-b37da180465b', '06464a37-cc22-4936-a39c-72b7e2e89b50', 'b4bd2f3d-7172-4975-8d36-dacc224174c6'])}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>index._service_context</b>: ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=OpenAIEmbedding(model_name='text-embedding-ada-002', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7fc2b5a674d0>, additional_kwargs={}, api_key='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', api_base='https://api.openai.com/v1', api_version='', max_retries=10, timeout=60.0, default_headers=None, reuse_client=True), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7fc2b5a674d0>, id_func=<function default_id_func at 0x7fc2b68e9ee0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.logger.base.LlamaLogger object at 0x7fc2b5a67750>, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7fc2b5a674d0>)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>index._service_context.llm_predictor</b>: system_prompt=None query_wrapper_prompt=None pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>index._service_context.prompt_helper</b>: context_window=4096 num_output=256 chunk_overlap_ratio=0.1 chunk_size_limit=None separator=' '"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>index._service_context.embed_model</b>: model_name='text-embedding-ada-002' embed_batch_size=10 callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7fc2b5a674d0> additional_kwargs={} api_key='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' api_base='https://api.openai.com/v1' api_version='' max_retries=10 timeout=60.0 default_headers=None reuse_client=True"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>index._service_context.transformations</b>: [SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7fc2b5a674d0>, id_func=<function default_id_func at 0x7fc2b68e9ee0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>index._storage_context</b>: StorageContext(docstore=<llama_index.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x7fc2b5c1b510>, index_store=<llama_index.storage.index_store.simple_index_store.SimpleIndexStore object at 0x7fc2b5a66ed0>, vector_stores={'default': <llama_index.vector_stores.simple.SimpleVectorStore object at 0x7fc2b6191290>, 'image': <llama_index.vector_stores.simple.SimpleVectorStore object at 0x7fc2b5a67250>}, graph_store=<llama_index.graph_stores.simple.SimpleGraphStore object at 0x7fc2b5a66f10>)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>index._index_struct</b>: \"\"\"IndexList(index_id='b97932b7-2774-4835-a84a-b863930ccf4e', summary=None, nodes=['2d3205d1-022c-4ce4-80d4-5c569cc85eb0', '5bfccebb-2737-4e12-90e3-51dcc8001648', '26562ada-fd18-4eb2-b75a-0f66e7c1c53d', 'c696beaa-2515-459c-872b-4853f3c3baf0', 'f8be7847-6491-42c3-8134-c7608d52b0ac', '2233ee04-9cca-42dd-aa0b-04601f12818d', '125d003f-caee-4613-9750-1ae5526779d9', 'e9cd869f-6728-449d-8cbd-01eab4b48363', 'dce4caa0-7250-45f9-b196-b37da180465b', '06464a37-cc22-4936-a39c-72b7e2e89b50', 'b4bd2f3d-7172-4975-8d36-dacc224174c6'])\"\"\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(vars(index))\n",
    "\n",
    "display(Markdown(f'<b>index._service_context</b>: {index._service_context}'))\n",
    "display(Markdown(f'<b>index._service_context.llm_predictor</b>: {index._service_context.llm_predictor}'))\n",
    "display(Markdown(f'<b>index._service_context.prompt_helper</b>: {index._service_context.prompt_helper}'))\n",
    "display(Markdown(f'<b>index._service_context.embed_model</b>: {index._service_context.embed_model}'))\n",
    "display(Markdown(f'<b>index._service_context.transformations</b>: {index._service_context.transformations}'))\n",
    "\n",
    "display(Markdown(f'<b>index._storage_context</b>: {index._storage_context}'))\n",
    "display(Markdown(f'<b>index._index_struct</b>: \"\"\"{index._index_struct}\"\"\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "doc.id_: 99420c3f-a95d-4755-87ea-0ec07bd1b737"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "doc.embedding: None"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "doc.metadata: {'file_path': '/workspaces/ml-learning/src/phages/data/test1/stateoftheunion.txt', 'file_name': 'stateoftheunion.txt', 'file_type': 'text/plain', 'file_size': 43412, 'creation_date': '2024-01-07', 'last_modified_date': '2024-01-07', 'last_accessed_date': '2024-01-07'}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "start_char_idx: None"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "end_char_idx: None"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "text_template: \"\"\"{metadata_str}\n",
       "\n",
       "{content}\"\"\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f'doc.id_: {doc.id_}'))\n",
    "display(Markdown(f'doc.embedding: {doc.embedding}'))\n",
    "display(Markdown(f'doc.metadata: {doc.metadata}'))\n",
    "display(Markdown(f'start_char_idx: {doc.start_char_idx}'))\n",
    "display(Markdown(f'end_char_idx: {doc.end_char_idx}'))\n",
    "display(Markdown(f'text_template: \"\"\"{doc.text_template}\"\"\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Summarize, don't use vague references such as The Speaker, instead use names…\"\n",
    "response = index.as_query_engine(response_mode=\"tree_summarize\").query(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the given context, Paul Pelosi addresses various individuals and congratulates them, including Kevin McCarthy as the new Speaker of the House, Hakeem Jeffries as the first Black House Minority Leader, Mitch McConnell as the longest-serving Senate Leader, and Chuck Schumer as the Senate Majority Leader. Paul Pelosi emphasizes the progress and resilience of America, highlighting the creation of 12 million new jobs, the control of COVID-19, and the strength of democracy. He also mentions bipartisan cooperation in passing laws and emphasizes the importance of the middle class and American manufacturing. Paul Pelosi discusses infrastructure investments, including the Bipartisan Infrastructure Law, and mentions specific projects like the Brent Spence bridge. He also addresses healthcare costs, particularly the cost of insulin, and the need to lower prescription drug prices. Paul Pelosi emphasizes the importance of tackling the climate crisis and investing in clean energy. He mentions the need for the wealthiest and biggest corporations to pay their fair share of taxes.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Markdown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display(\u001b[43mMarkdown\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Markdown' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "display(Markdown(f'{response}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llamaindex with custom tree summarize prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=20)\n",
    "service_context = ServiceContext.from_defaults(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(Markdown(documents[0].get_content()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 3/3 [00:00<00:00, 27.39it/s]\n"
     ]
    }
   ],
   "source": [
    "nodes = text_splitter.get_nodes_from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SummaryIndex, GPTDocumentSummaryIndex\n",
    "from llama_index.response_synthesizers.type import ResponseMode\n",
    "from llama_index.prompts.base import PromptTemplate\n",
    "from llama_index.prompts.prompt_type import PromptType\n",
    "\n",
    "list_index = ListIndex(nodes, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SummaryIndex, GPTDocumentSummaryIndex\n",
    "from llama_index.response_synthesizers.type import ResponseMode\n",
    "from llama_index.prompts.base import PromptTemplate\n",
    "from llama_index.prompts.prompt_type import PromptType\n",
    "\n",
    "# index_summary = SummaryIndex.from_documents(documents, show_progress=True)\n",
    "\n",
    "CUSTOM_TREE_SUMMARIZE_PROMPT_TMPL = (\n",
    "    \"Write a 250-word summary of the following, removing irrelevant information. Finish your answer: \\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"\\n\"\n",
    "    \"\\n\"\n",
    "    '250-WORD SUMMARY:\"\"\"\\n'\n",
    ")\n",
    "\n",
    "CUSTOM_TREE_SUMMARIZE_PROMPT = PromptTemplate(\n",
    "   CUSTOM_TREE_SUMMARIZE_PROMPT_TMPL, prompt_type=PromptType.SUMMARY\n",
    ")\n",
    "\n",
    "title_acc_prompt_template = \"\"\"Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\n",
    "  {context_str}\n",
    "\n",
    "  Return your answer in the following format:\n",
    "  Title | Summary...\n",
    "  e.g. \n",
    "  Why Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\n",
    "\n",
    "  TITLE AND CONCISE SUMMARY:\"\"\"\n",
    "\n",
    "title_acc_prompt = PromptTemplate(\n",
    "   title_acc_prompt_template, prompt_type=PromptType.SUMMARY\n",
    ")\n",
    "\n",
    "\n",
    "prompt = \"Summarize…\"\n",
    "# summary_query_engine_tree = index_summary.as_query_engine(response_mode=\"tree_summarize\")\n",
    "# summary_query_engine_compact_acc = index_summary.as_query_engine(response_mode=ResponseMode.COMPACT_ACCUMULATE)\n",
    "# summary_query_engine_refine = index_summary.as_query_engine(response_mode=ResponseMode.REFINE)\n",
    "# summary_query_engine_simple = index_summary.as_query_engine(response_mode=ResponseMode.SIMPLE_SUMMARIZE)\n",
    "\n",
    "# response_compact_acc = summary_query_engine_compact_acc.query(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextNode' object has no attribute 'node'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse_synthesizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     ResponseMode,\n\u001b[1;32m      4\u001b[0m     get_response_synthesizer,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m response_synthesizer \u001b[38;5;241m=\u001b[39m get_response_synthesizer(\n\u001b[1;32m      8\u001b[0m     response_mode\u001b[38;5;241m=\u001b[39mResponseMode\u001b[38;5;241m.\u001b[39mACCUMULATE\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mresponse_synthesizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummarize...\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/base.py:148\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    144\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    145\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[1;32m    146\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[1;32m    147\u001b[0m         query_str\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[0;32m--> 148\u001b[0m         text_chunks\u001b[38;5;241m=\u001b[39m\u001b[43m[\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetadataMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[1;32m    152\u001b[0m     )\n\u001b[1;32m    154\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    155\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/llama_index/response_synthesizers/base.py:149\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    144\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    145\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[1;32m    146\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[1;32m    147\u001b[0m         query_str\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[1;32m    148\u001b[0m         text_chunks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m--> 149\u001b[0m             \u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mLLM) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[1;32m    150\u001b[0m         ],\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[1;32m    152\u001b[0m     )\n\u001b[1;32m    154\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    155\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TextNode' object has no attribute 'node'"
     ]
    }
   ],
   "source": [
    "from llama_index.schema import Node\n",
    "from llama_index.response_synthesizers import (\n",
    "    ResponseMode,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=ResponseMode.ACCUMULATE\n",
    ")\n",
    "\n",
    "response = response_synthesizer.synthesize(\n",
    "    \"summarize...\", nodes=nodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_acc = list_index.as_query_engine(response_mode=ResponseMode.ACCUMULATE, text_qa_template = title_acc_prompt).query('Summarize...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_tree = index_summary.as_query_engine(response_mode=\"tree_summarize\",summarize_template =  CUSTOM_TREE_SUMMARIZE_PROMPT).query('Summarize...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_compact_acc = index_summary.as_query_engine(response_mode=ResponseMode.COMPACT_ACCUMULATE).query('Summarize...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9005d22c-718a-48c2-8f30-730159c85576',\n",
       " '47dbaa28-236a-417c-8b0f-5e685c62d996',\n",
       " '1a5fe5e4-7307-423a-a811-7cbe551e5784',\n",
       " '4172d2bb-5ef6-4e06-818a-55bff2631bb5',\n",
       " 'bc113ff8-5048-4844-96ee-354e92b85ec3',\n",
       " '95fbebda-656b-4065-87bb-68f4b81a13fb',\n",
       " 'd8a37425-d847-4ce8-a0a8-a73ae15a6711',\n",
       " '2088cedb-8879-4de0-8e04-7a1f8a7b1c71',\n",
       " 'ea90a275-4076-4a48-b2d7-ac1095ff09eb',\n",
       " '02428601-eb8c-411f-aa7e-ba9aa51358d1',\n",
       " '6bb1fbf7-1a6c-4be9-a701-c2cb7b9054ed']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_summary._index_struct.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Response 1: The speaker begins by congratulating members of Congress and acknowledging various leaders. They emphasize the progress and resilience of America, highlighting the economic growth, the control over COVID-19, and the strength of democracy. The speaker emphasizes the importance of working together and mentions bipartisan achievements in areas such as defense, infrastructure, and veterans' rights. They discuss the need to rebuild the middle class, create manufacturing jobs, and invest in American products. The speaker also addresses issues such as inflation, healthcare costs, and prescription drug prices. They highlight the importance of infrastructure and the passage of the Bipartisan Infrastructure Law. The speaker mentions specific projects and the impact they will have on communities. They discuss the need to address climate change and invest in clean energy. The speaker emphasizes the importance of fair taxation and making the wealthiest and biggest corporations pay their fair share. They assure that individuals earning less than $400,000 a\n",
       "---------------------\n",
       "Response 2: The speaker discusses various initiatives and proposals aimed at addressing climate change, tax fairness, corporate accountability, consumer protection, worker rights, education, and public safety. They emphasize the need to invest in clean energy, rebuild infrastructure, and create jobs. They also advocate for tax reforms to ensure that the wealthy and big corporations pay their fair share. The speaker highlights the importance of competition in capitalism and proposes measures to strengthen antitrust enforcement. They address issues such as hidden fees, non-compete agreements, and fraud prevention. The speaker emphasizes the need for affordable education, including preschool and community college, and supports measures to improve public school teacher salaries. They also discuss the progress made in fighting the COVID-19 pandemic and the importance of public safety and trust in law enforcement.\n",
       "---------------------\n",
       "Response 3: The speaker emphasizes the importance of creating neighborhoods free of violence and building trust between law enforcement and the community. They acknowledge the challenges faced by police officers and the need for them to be supported and trained adequately. The speaker also highlights the need for additional resources to address mental health and substance abuse issues, as well as to reduce violent and gun crimes. Overall, the speaker calls for collective action to ensure the safety and well-being of everyone in society.\n",
       "---------------------\n",
       "Response 4: The speaker addresses various issues in their speech, including the need for trust between law enforcement and the community, the desire for safe neighborhoods and the protection of children, the importance of equal protection under the law, and the recognition of the sacrifices made by police officers. They also discuss the need for police reform, addressing gun violence, immigration, reproductive rights, LGBTQ rights, and the United States' role in the world. The speaker emphasizes the importance of democracy and the need to protect it, and concludes by calling for unity and optimism in facing the challenges ahead.\n",
       "---------------------\n",
       "Response 5: The speaker addresses the recent political violence and emphasizes the need to speak out against it. They stress the importance of protecting the right to vote, upholding the rule of law, and restoring trust in democratic institutions. The speaker calls for unity, hope, and optimism, urging Americans to see each other as fellow citizens. They express confidence in the strength and future of the nation, emphasizing the capacity to overcome challenges together. The speech concludes with a blessing for all and a prayer for the protection of troops."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "display(Markdown(f'{response_compact_acc}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_acc = index_summary.as_query_engine(response_mode=ResponseMode.ACCUMULATE, text_qa_template = title_acc_prompt).query('Summarize...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='66798390-546f-479f-9add-79641b81fc92', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='32ee0945-5f2a-41e2-bbdf-0dd1ddc4fab6', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='e2d4fb35246a6751413cd39467fc1541f5fc31f2cffb06e9baa10056d23cee22'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f32d7231-c5f0-4937-a3a0-e9b9462cfbb3', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='737819098cb269fa43d07308799544cb4bd9bdbe3c9fa92af2dc1ab31fbdc7fc')}, hash='1536d10cd91f74ac363ba6006421eabb84c0a4cbc30269c14570e3c0ddf6ab17', text=\"The Louvain method for community detection is a method to extract non-overlapping communities from large networks created by Blondel et al. from the University of Louvain (the source of this method's name). The method is a greedy optimization method that appears to run in time \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        ⋅\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\cdot \\\\log n)}\\n   where \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is the number of nodes in the network.\\n\\n\\n== Modularity optimization ==\\nThe inspiration for this method of community detection is the optimization of modularity as the algorithm progresses. Modularity is a scale value between −0.5 (non-modular clustering) and 1 (fully modular clustering) that measures the relative density of edges inside communities with respect to edges outside communities. Optimizing this value theoretically results in the best possible grouping of the nodes of a given network. But because going through all possible iterations of the nodes into groups is impractical, heuristic algorithms are used.\\nIn the Louvain Method of community detection, first small communities are found by optimizing modularity locally on all nodes, then each small community is grouped into one node and the first step is repeated. The method is similar to the earlier method by Clauset, Newman and Moore that connects communities whose amalgamation produces the largest increase in modularity.\\n\\n\\n== Algorithm ==\\nThe value to be optimized is modularity, defined as a value in the range \\n  \\n    \\n      \\n        [\\n        −\\n        1\\n        \\n          /\\n        \\n        2\\n        ,\\n        1\\n        ]\\n      \\n    \\n    {\\\\displaystyle [-1/2,1]}\\n   that measures the density of links inside communities compared to links between communities.\", start_char_idx=0, end_char_idx=1836, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='f32d7231-c5f0-4937-a3a0-e9b9462cfbb3', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='32ee0945-5f2a-41e2-bbdf-0dd1ddc4fab6', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='e2d4fb35246a6751413cd39467fc1541f5fc31f2cffb06e9baa10056d23cee22'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='66798390-546f-479f-9add-79641b81fc92', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='1536d10cd91f74ac363ba6006421eabb84c0a4cbc30269c14570e3c0ddf6ab17'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d097fd2d-2b68-4bbd-b384-5e9e14fb80a0', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='cd2f812338a390bec7e080dc4e13c50fb28dbc2101c70d44af2f05128d92e41a')}, hash='737819098cb269fa43d07308799544cb4bd9bdbe3c9fa92af2dc1ab31fbdc7fc', text='== Algorithm ==\\nThe value to be optimized is modularity, defined as a value in the range \\n  \\n    \\n      \\n        [\\n        −\\n        1\\n        \\n          /\\n        \\n        2\\n        ,\\n        1\\n        ]\\n      \\n    \\n    {\\\\displaystyle [-1/2,1]}\\n   that measures the density of links inside communities compared to links between communities. For a weighted graph, modularity is defined as:\\n\\n  \\n    \\n      \\n        Q\\n        =\\n        \\n          \\n            1\\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            j\\n          \\n        \\n        \\n          \\n            [\\n          \\n        \\n        \\n          A\\n          \\n            i\\n            j\\n          \\n        \\n        −\\n        \\n          \\n            \\n              \\n                k\\n                \\n                  i\\n                \\n              \\n              \\n                k\\n                \\n                  j\\n                \\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          \\n            ]\\n          \\n        \\n        δ\\n        (\\n        \\n          c\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          c\\n          \\n            j\\n          \\n        \\n        )\\n        ,\\n      \\n    \\n    {\\\\displaystyle Q={\\\\frac {1}{2m}}\\\\sum \\\\limits _{ij}{\\\\bigg [}A_{ij}-{\\\\frac {k_{i}k_{j}}{2m}}{\\\\bigg ]}\\\\delta (c_{i},c_{j}),}\\n  \\nwhere\\n\\n  \\n    \\n      \\n        \\n          A\\n          \\n            i\\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle A_{ij}}\\n   represents the edge weight between nodes \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   and \\n  \\n    \\n      \\n        j\\n      \\n    \\n    {\\\\displaystyle j}\\n  ;\\n\\n  \\n    \\n      \\n        \\n          k\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{i}}\\n   and \\n  \\n    \\n      \\n        \\n          k\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{j}}\\n   are the sum of the weights of the edges attached to nodes \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   and \\n  \\n    \\n      \\n        j\\n      \\n    \\n    {\\\\displaystyle j}\\n  , respectively;\\n\\n  \\n    \\n      \\n        m\\n      \\n    \\n    {\\\\displaystyle m}\\n   is the sum of all of the edge weights in the graph;\\n\\n  \\n    \\n      \\n        \\n          c\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle c_{i}}\\n   and \\n  \\n    \\n      \\n        \\n          c\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle c_{j}}\\n   are the communities of the nodes; and\\n\\n  \\n    \\n      \\n        δ\\n      \\n    \\n    {\\\\displaystyle \\\\delta }\\n   is Kronecker delta function (\\n  \\n    \\n      \\n        δ\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle \\\\delta (x,y)=1}\\n   if \\n  \\n    \\n      \\n        x\\n        =\\n        y\\n      \\n    \\n    {\\\\displaystyle x=y}\\n  , \\n  \\n    \\n      \\n        0\\n      \\n    \\n    {\\\\displaystyle 0}\\n   otherwise).Based on the above equation, the modularity of a community \\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n   can be calculated as:\\n\\n  \\n    \\n      \\n        \\n          Q\\n          \\n            c\\n          \\n        \\n        =\\n        \\n          \\n            \\n              Σ\\n              \\n                i\\n                n\\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        −\\n        (\\n        \\n          \\n            \\n              Σ\\n              \\n                t\\n                o\\n                t\\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          )\\n          \\n            2\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle Q_{c}={\\\\frac {\\\\Sigma _{in}}{2m}}-({\\\\frac {\\\\Sigma _{tot}}{2m}})^{2},}\\n  \\nwhere\\n\\n  \\n    \\n      \\n        \\n          Σ\\n          \\n            i\\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Sigma _{in}}\\n   is the sum of edge weights between nodes within the community \\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n   (each edge is considered twice); and\\n\\n  \\n    \\n      \\n        \\n          Σ\\n          \\n            t\\n            o\\n            t\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Sigma _{tot}}\\n   is the sum of all edge weights for nodes within the community (including edges which link to other communities).In order to maximize modularity efficiently, the Louvain Method has two phases that are repeated iteratively.\\nFirst, each node in the network is assigned to its own community. Then for each node \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  , the change in modularity is calculated for removing \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   from its own community and moving it into the community of each neighbor \\n  \\n    \\n      \\n        j\\n      \\n    \\n    {\\\\displaystyle j}\\n   of \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  .', start_char_idx=1495, end_char_idx=6565, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='d097fd2d-2b68-4bbd-b384-5e9e14fb80a0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='32ee0945-5f2a-41e2-bbdf-0dd1ddc4fab6', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='e2d4fb35246a6751413cd39467fc1541f5fc31f2cffb06e9baa10056d23cee22'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f32d7231-c5f0-4937-a3a0-e9b9462cfbb3', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='737819098cb269fa43d07308799544cb4bd9bdbe3c9fa92af2dc1ab31fbdc7fc'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='0dcd049c-1bff-4452-acff-bc1abd043e2f', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='75345c548edb35243f5756bee05da5f34aec67cc546b1e75f56f4acbd4fb8715')}, hash='cd2f812338a390bec7e080dc4e13c50fb28dbc2101c70d44af2f05128d92e41a', text='First, each node in the network is assigned to its own community. Then for each node \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  , the change in modularity is calculated for removing \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   from its own community and moving it into the community of each neighbor \\n  \\n    \\n      \\n        j\\n      \\n    \\n    {\\\\displaystyle j}\\n   of \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  . This value is easily calculated by two steps: (1) removing \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   from its original community, and (2) inserting \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   to the community of \\n  \\n    \\n      \\n        j\\n      \\n    \\n    {\\\\displaystyle j}\\n  . The two equations are quite similar, and the equation for step (2) is:\\n  \\n    \\n      \\n        Δ\\n        Q\\n        =\\n        \\n          \\n            [\\n          \\n        \\n        \\n          \\n            \\n              \\n                Σ\\n                \\n                  i\\n                  n\\n                \\n              \\n              +\\n              2\\n              \\n                k\\n                \\n                  i\\n                  ,\\n                  i\\n                  n\\n                \\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        −\\n        \\n          \\n            (\\n          \\n        \\n        \\n          \\n            \\n              \\n                Σ\\n                \\n                  t\\n                  o\\n                  t\\n                \\n              \\n              +\\n              \\n                k\\n                \\n                  i\\n                \\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          \\n            \\n              )\\n            \\n          \\n          \\n            2\\n          \\n        \\n        \\n          \\n            ]\\n          \\n        \\n        −\\n        \\n          \\n            [\\n          \\n        \\n        \\n          \\n            \\n              Σ\\n              \\n                i\\n                n\\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        −\\n        \\n          \\n            (\\n          \\n        \\n        \\n          \\n            \\n              Σ\\n              \\n                t\\n                o\\n                t\\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          \\n            \\n              )\\n            \\n          \\n          \\n            2\\n          \\n        \\n        −\\n        \\n          \\n            (\\n          \\n        \\n        \\n          \\n            \\n              k\\n              \\n                i\\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          \\n            \\n              )\\n            \\n          \\n          \\n            2\\n          \\n        \\n        \\n          \\n            ]\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Delta Q={\\\\bigg [}{\\\\frac {\\\\Sigma _{in}+2k_{i,in}}{2m}}-{\\\\bigg (}{\\\\frac {\\\\Sigma _{tot}+k_{i}}{2m}}{\\\\bigg )}^{2}{\\\\bigg ]}-{\\\\bigg [}{\\\\frac {\\\\Sigma _{in}}{2m}}-{\\\\bigg (}{\\\\frac {\\\\Sigma _{tot}}{2m}}{\\\\bigg )}^{2}-{\\\\bigg (}{\\\\frac {k_{i}}{2m}}{\\\\bigg )}^{2}{\\\\bigg ]}}\\n  \\nWhere \\n  \\n    \\n      \\n        \\n          Σ\\n          \\n            i\\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Sigma _{in}}\\n   is sum of all the weights of the links inside the community \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   is moving into, \\n  \\n    \\n      \\n        \\n          Σ\\n          \\n            t\\n            o\\n            t\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Sigma _{tot}}\\n   is the sum of all the weights of the links to nodes in the community \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   is moving into, \\n  \\n    \\n      \\n        \\n          k\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{i}}\\n   is the weighted degree of \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  , \\n  \\n    \\n      \\n        \\n          k\\n          \\n            i\\n            ,\\n            i\\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{i,in}}\\n   is the sum of the weights of the links between \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   and other nodes in the community that \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   is moving into, and \\n  \\n    \\n      \\n        m\\n      \\n    \\n    {\\\\displaystyle m}\\n   is the sum of the weights of all links in the network. Then, once this value is calculated for all communities \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   is connected to, \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   is placed into the community that resulted in the greatest modularity increase. If no increase is possible, \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   remains in its original community. This process is applied repeatedly and sequentially to all nodes until no modularity increase can occur. Once this local maximum of modularity is hit, the first phase has ended.', start_char_idx=6099, end_char_idx=11412, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='0dcd049c-1bff-4452-acff-bc1abd043e2f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='32ee0945-5f2a-41e2-bbdf-0dd1ddc4fab6', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='e2d4fb35246a6751413cd39467fc1541f5fc31f2cffb06e9baa10056d23cee22'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='d097fd2d-2b68-4bbd-b384-5e9e14fb80a0', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='cd2f812338a390bec7e080dc4e13c50fb28dbc2101c70d44af2f05128d92e41a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='8f6308d5-0311-46d3-9af2-f03d910e2c42', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5f45bc9c8639211ebb656ace1dedfd4de7cd2188fbba638c2ec1d57160c846a0')}, hash='75345c548edb35243f5756bee05da5f34aec67cc546b1e75f56f4acbd4fb8715', text='Then, once this value is calculated for all communities \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   is connected to, \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   is placed into the community that resulted in the greatest modularity increase. If no increase is possible, \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   remains in its original community. This process is applied repeatedly and sequentially to all nodes until no modularity increase can occur. Once this local maximum of modularity is hit, the first phase has ended.\\nIn the second phase of the algorithm, it groups all of the nodes in the same community and builds a new network where nodes are the communities from the previous phase. Any links between nodes of the same community are now represented by self-loops on the new community node and links from multiple nodes in the same community to a node in a different community are represented by weighted edges between communities. Once the new network is created, the second phase has ended and the first phase can be re-applied to the new network.\\n\\n\\n== Previous uses ==\\nTwitter social Network (2.4 Million nodes, 38 million links) by Josep Pujol, Vijay Erramilli, and Pablo Rodriguez: The authors explore the problem of partitioning Online Social Networks onto different machines.\\nMobile phone Network (4 Million nodes, 100 Million links) by Derek Greene, Donal Doyle, and Padraig Cunningham: Community-tracking strategies for identifying dynamic communities of different dynamic social networks.\\nDetecting species in network-based dynamical model.\\n\\n\\n== Disadvantages ==\\nIt is important to emphasize that Louvain produces only non-overlapping communities, which means that each node can belong to at most one community. This is highly unrealistic in many real-world applications. For example, in social networks, most people belong to multiple communities: their family, their friends, their co-workers, old school buddies, etc. In biological networks, most genes or proteins belong to more than one pathway or complex. Furthermore, Louvain has been shown to sometimes produce arbitrarily badly connected communities, and has been effectively superseded (at least in the non-overlapping case) by the Leiden algorithm.\\n\\n\\n== Comparison to other methods of non-overlapping community detection ==\\nWhen comparing modularity optimization methods, the two measures of importance are the speed and the resulting modularity value. A higher speed is better as it shows a method is more efficient than others and a higher modularity value is desirable as it points to having better-defined communities.\\nThe compared methods are, the algorithm of Clauset, Newman, and Moore, Pons and Latapy, and Wakita and Tsurumi.\\n-/- in the table refers to a method that took over 24hrs to run. This table (from) shows that the Louvain method outperforms many similar modularity optimization methods in both the modularity and the time categories.\\n\\n\\n== See also ==\\nModularity (networks)\\nCommunity structure\\nNetwork science\\nK-means clustering\\n\\n\\n== References ==\\n\\n\"The Louvain method for community detection in large networks\" Vincent Blondel http://perso.uclouvain.be/vincent.blondel/research/louvain.html', start_char_idx=10830, end_char_idx=14078, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='8f6308d5-0311-46d3-9af2-f03d910e2c42', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6859ef38-2941-4a08-a1f0-9a3e7a430898', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='735a591b325be2f57da7c8756161c4cb8071593f110f388a3f18fba441884387'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0dcd049c-1bff-4452-acff-bc1abd043e2f', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='75345c548edb35243f5756bee05da5f34aec67cc546b1e75f56f4acbd4fb8715'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c1b544e2-2990-468a-8a35-c210a20bf36d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5494deb6493cfd07a1eb0dc824819ba3107da6f6b5298e4b1028613576e6c5cc')}, hash='5f45bc9c8639211ebb656ace1dedfd4de7cd2188fbba638c2ec1d57160c846a0', text='Modularity is a measure of the structure of networks or graphs which measures the strength of division of a network into modules (also called groups, clusters or communities). Networks with high modularity have dense connections between the nodes within modules but sparse connections between nodes in different modules. Modularity is often used in optimization methods for detecting community structure in networks.  Biological networks, including animal brains, exhibit a high degree of modularity. However, modularity maximization is not statistically consistent, and finds communities in its own null model, i.e. fully random graphs, and therefore it cannot be used to find statistically significant community structures in empirical networks. Furthermore, it has been shown that modularity suffers a resolution limit and, therefore, it is unable to detect small communities.\\n\\n\\n== Motivation ==\\nMany scientifically important problems can be represented and empirically studied using networks. For example, biological and social patterns, the World Wide Web, metabolic networks, food webs, neural networks and pathological networks are real world problems that can be mathematically represented and topologically studied to reveal some unexpected structural features. Most of these networks possess a certain community structure that has substantial importance in building an understanding regarding the dynamics of the network. For instance, a closely connected social community will imply a faster rate of transmission of information or rumor among them than a loosely connected community. Thus, if a network is represented by a number of individual nodes connected by links which signify a certain degree of interaction between the nodes, communities are defined as groups of densely interconnected nodes that are only sparsely connected with the rest of the network. Hence, it may be imperative to identify the communities in networks since the communities may have quite different properties such as node degree, clustering coefficient, betweenness, centrality, etc., from that of the average network. Modularity is one such measure, which when maximized, leads to the appearance of communities in a given network.', start_char_idx=0, end_char_idx=2222, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='c1b544e2-2990-468a-8a35-c210a20bf36d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6859ef38-2941-4a08-a1f0-9a3e7a430898', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='735a591b325be2f57da7c8756161c4cb8071593f110f388a3f18fba441884387'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8f6308d5-0311-46d3-9af2-f03d910e2c42', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5f45bc9c8639211ebb656ace1dedfd4de7cd2188fbba638c2ec1d57160c846a0'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c950ee50-c6ea-4fb9-b661-85e4af94730d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f756c9de205234e9d1af3e169669621adecbea315932bc0f970cc3a177c6fd3b')}, hash='5494deb6493cfd07a1eb0dc824819ba3107da6f6b5298e4b1028613576e6c5cc', text=\"== Definition ==\\nModularity is the fraction of the edges that fall within the given groups minus the expected fraction if edges were distributed at random. The value of the modularity for unweighted and undirected graphs lies in the range \\n  \\n    \\n      \\n        [\\n        −\\n        1\\n        \\n          /\\n        \\n        2\\n        ,\\n        1\\n        ]\\n      \\n    \\n    {\\\\displaystyle [-1/2,1]}\\n  .  It is positive if the number of edges within groups exceeds the number expected on the basis of chance. For a given division of the network's vertices into some modules, modularity reflects the concentration of edges within modules compared with random distribution of links between all nodes regardless of modules.\\nThere are different methods for calculating modularity. In the most common version of the concept, the randomization of the edges is done so as to preserve the degree of each vertex. Consider a graph with \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   nodes and \\n  \\n    \\n      \\n        m\\n      \\n    \\n    {\\\\displaystyle m}\\n   links (edges) such that the graph can be partitioned into two communities using a membership variable \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n  . If a node \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   belongs to community 1, \\n  \\n    \\n      \\n        \\n          s\\n          \\n            v\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle s_{v}=1}\\n  , or if \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   belongs to community 2, \\n  \\n    \\n      \\n        \\n          s\\n          \\n            v\\n          \\n        \\n        =\\n        −\\n        1\\n      \\n    \\n    {\\\\displaystyle s_{v}=-1}\\n  . Let the adjacency matrix for the network be represented by \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n  , where \\n  \\n    \\n      \\n        \\n          A\\n          \\n            v\\n            w\\n          \\n        \\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle A_{vw}=0}\\n   means there's no edge (no interaction) between nodes \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   and \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   and \\n  \\n    \\n      \\n        \\n          A\\n          \\n            v\\n            w\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle A_{vw}=1}\\n   means there is an edge between the two. Also for simplicity we consider an undirected network. Thus \\n  \\n    \\n      \\n        \\n          A\\n          \\n            v\\n            w\\n          \\n        \\n        =\\n        \\n          A\\n          \\n            w\\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle A_{vw}=A_{wv}}\\n  . (It is important to note that multiple edges may exist between two nodes, but here we assess the simplest case).\\nModularity \\n  \\n    \\n      \\n        Q\\n      \\n    \\n    {\\\\displaystyle Q}\\n   is then defined as the fraction of edges that fall within group 1 or 2, minus the expected number of edges within groups 1 and 2 for a random graph with the same node degree distribution as the given network.\\nThe expected number of edges shall be computed using the concept of a configuration model. The configuration model is a randomized realization of a particular network. Given a network with \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   nodes, where each node \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   has a node degree \\n  \\n    \\n      \\n        \\n          k\\n          \\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{v}}\\n  , the configuration model cuts each edge into two halves,  and then each half edge, called a stub, is rewired randomly with any other stub in the network, even allowing self-loops (which occur when a stub is rewired to another stub from the same node) and multiple-edges between the same two nodes. Thus, even though the node degree distribution of the graph remains intact, the configuration model results in a completely random network.\\n\\n\\n== Expected Number of Edges Between Nodes ==\\nNow consider two nodes \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   and \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n  , with node degrees \\n  \\n    \\n      \\n        \\n          k\\n          \\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{v}}\\n   and \\n  \\n    \\n      \\n        \\n          k\\n          \\n            w\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{w}}\\n   respectively, from a randomly rewired network as described above. We calculate the expected number of full edges between these nodes.\", start_char_idx=2225, end_char_idx=6829, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='c950ee50-c6ea-4fb9-b661-85e4af94730d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6859ef38-2941-4a08-a1f0-9a3e7a430898', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='735a591b325be2f57da7c8756161c4cb8071593f110f388a3f18fba441884387'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c1b544e2-2990-468a-8a35-c210a20bf36d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5494deb6493cfd07a1eb0dc824819ba3107da6f6b5298e4b1028613576e6c5cc'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='3ddf81a2-6d52-4d6c-b2ac-f294fd654791', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4be6dbc0ddf0c74a47f8631eaa6f6d3365c9680dbbf3d6fddbe08ef5e5262888')}, hash='f756c9de205234e9d1af3e169669621adecbea315932bc0f970cc3a177c6fd3b', text='== Expected Number of Edges Between Nodes ==\\nNow consider two nodes \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   and \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n  , with node degrees \\n  \\n    \\n      \\n        \\n          k\\n          \\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{v}}\\n   and \\n  \\n    \\n      \\n        \\n          k\\n          \\n            w\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{w}}\\n   respectively, from a randomly rewired network as described above. We calculate the expected number of full edges between these nodes.\\nLet us consider each of the \\n  \\n    \\n      \\n        \\n          k\\n          \\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{v}}\\n   stubs of node \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   and create associated indicator variables \\n  \\n    \\n      \\n        \\n          I\\n          \\n            i\\n          \\n          \\n            (\\n            v\\n            ,\\n            w\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle I_{i}^{(v,w)}}\\n   for them, \\n  \\n    \\n      \\n        i\\n        =\\n        1\\n        ,\\n        …\\n        ,\\n        \\n          k\\n          \\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle i=1,\\\\ldots ,k_{v}}\\n  , with \\n  \\n    \\n      \\n        \\n          I\\n          \\n            i\\n          \\n          \\n            (\\n            v\\n            ,\\n            w\\n            )\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle I_{i}^{(v,w)}=1}\\n   if the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  -th stub happens to connect to one of the \\n  \\n    \\n      \\n        \\n          k\\n          \\n            w\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{w}}\\n   stubs of node \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   in this particular random graph. If it does not, then \\n  \\n    \\n      \\n        \\n          I\\n          \\n            i\\n          \\n          \\n            (\\n            v\\n            ,\\n            w\\n            )\\n          \\n        \\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle I_{i}^{(v,w)}=0}\\n  .', start_char_idx=6229, end_char_idx=8389, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='3ddf81a2-6d52-4d6c-b2ac-f294fd654791', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6859ef38-2941-4a08-a1f0-9a3e7a430898', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='735a591b325be2f57da7c8756161c4cb8071593f110f388a3f18fba441884387'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c950ee50-c6ea-4fb9-b661-85e4af94730d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f756c9de205234e9d1af3e169669621adecbea315932bc0f970cc3a177c6fd3b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='57d7ff03-9dd8-42a5-9494-3319f961ded9', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='98a179372ab837a752b1acd6a877f94262c7e9711e16c99fe69e81e5f62384c0')}, hash='4be6dbc0ddf0c74a47f8631eaa6f6d3365c9680dbbf3d6fddbe08ef5e5262888', text='If it does not, then \\n  \\n    \\n      \\n        \\n          I\\n          \\n            i\\n          \\n          \\n            (\\n            v\\n            ,\\n            w\\n            )\\n          \\n        \\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle I_{i}^{(v,w)}=0}\\n  . Since the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  -th stub of node \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   can connect to any of the \\n  \\n    \\n      \\n        2\\n        m\\n        −\\n        1\\n      \\n    \\n    {\\\\displaystyle 2m-1}\\n   remaining stubs with equal probability, and since there are \\n  \\n    \\n      \\n        \\n          k\\n          \\n            w\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{w}}\\n   stubs it can connect to associated with node \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n  , evidently\\n\\n  \\n    \\n      \\n        p\\n        (\\n        \\n          I\\n          \\n            i\\n          \\n          \\n            (\\n            v\\n            ,\\n            w\\n            )\\n          \\n        \\n        =\\n        1\\n        )\\n        =\\n        E\\n        [\\n        \\n          I\\n          \\n            i\\n          \\n          \\n            (\\n            v\\n            ,\\n            w\\n            )\\n          \\n        \\n        ]\\n        =\\n        \\n          \\n            \\n              k\\n              \\n                w\\n              \\n            \\n            \\n              2\\n              m\\n              −\\n              1\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p(I_{i}^{(v,w)}=1)=E[I_{i}^{(v,w)}]={\\\\frac {k_{w}}{2m-1}}}\\n  The total number of full edges \\n  \\n    \\n      \\n        \\n          J\\n          \\n            v\\n            w\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle J_{vw}}\\n   between \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   and \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   is just \\n  \\n    \\n      \\n        \\n          J\\n          \\n            v\\n            w\\n          \\n        \\n        =\\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            \\n              k\\n              \\n                v\\n              \\n            \\n          \\n        \\n        \\n          I\\n          \\n            i\\n          \\n          \\n            (\\n            v\\n            ,\\n            w\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle J_{vw}=\\\\sum _{i=1}^{k_{v}}I_{i}^{(v,w)}}\\n  , so the expected value of this quantity is\\n\\n  \\n    \\n      \\n        E\\n        [\\n        \\n          J\\n          \\n            v\\n            w\\n          \\n        \\n        ]\\n        =\\n        E\\n        \\n          [\\n          \\n            \\n              ∑\\n              \\n                i\\n                =\\n                1\\n              \\n              \\n                \\n                  k\\n                  \\n                    v\\n                  \\n                \\n              \\n            \\n            \\n              I\\n              \\n                i\\n              \\n              \\n                (\\n                v\\n                ,\\n                w\\n                )\\n              \\n            \\n          \\n          ]\\n        \\n        =\\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            \\n              k\\n              \\n                v\\n              \\n            \\n          \\n        \\n        E\\n        [\\n        \\n          I\\n          \\n            i\\n          \\n          \\n            (\\n            v\\n            ,\\n            w\\n            )\\n          \\n        \\n        ]\\n        =\\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            \\n              k\\n              \\n                v\\n              \\n            \\n          \\n        \\n        \\n          \\n            \\n              k\\n              \\n                w\\n              \\n            \\n            \\n              2\\n              m\\n              −\\n              1\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                k\\n                \\n                  v\\n                \\n              \\n              \\n                k\\n                \\n                  w\\n                \\n              \\n            \\n            \\n              2\\n              m\\n              −\\n              1\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle E[J_{vw}]=E\\\\left[\\\\sum _{i=1}^{k_{v}}I_{i}^{(v,w)}\\\\right]=\\\\sum _{i=1}^{k_{v}}E[I_{i}^{(v,w)}]=\\\\sum _{i=1}^{k_{v}}{\\\\frac {k_{w}}{2m-1}}={\\\\frac {k_{v}k_{w}}{2m-1}}}\\n  Many texts then make the following approximations, for random networks with a large number of edges. When \\n  \\n    \\n      \\n        m\\n      \\n    \\n    {\\\\displaystyle m}\\n   is large, they drop the subtraction of \\n  \\n    \\n      \\n        1\\n      \\n    \\n    {\\\\displaystyle 1}\\n   in the denominator above and simply use the approximate expression \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                k\\n                \\n                  v\\n                \\n              \\n              \\n                k\\n                \\n                  w\\n                \\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {k_{v}k_{w}}{2m}}}\\n   for the expected number of edges between two nodes.', start_char_idx=8123, end_char_idx=13501, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='57d7ff03-9dd8-42a5-9494-3319f961ded9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6859ef38-2941-4a08-a1f0-9a3e7a430898', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='735a591b325be2f57da7c8756161c4cb8071593f110f388a3f18fba441884387'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='3ddf81a2-6d52-4d6c-b2ac-f294fd654791', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4be6dbc0ddf0c74a47f8631eaa6f6d3365c9680dbbf3d6fddbe08ef5e5262888'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='bc244995-8d11-4ae1-95ed-512742e3ab91', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='ece86016382d4aec30b71287c270f5b62a668ca9fe9078e31f93230477fe6e50')}, hash='98a179372ab837a752b1acd6a877f94262c7e9711e16c99fe69e81e5f62384c0', text='When \\n  \\n    \\n      \\n        m\\n      \\n    \\n    {\\\\displaystyle m}\\n   is large, they drop the subtraction of \\n  \\n    \\n      \\n        1\\n      \\n    \\n    {\\\\displaystyle 1}\\n   in the denominator above and simply use the approximate expression \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                k\\n                \\n                  v\\n                \\n              \\n              \\n                k\\n                \\n                  w\\n                \\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {k_{v}k_{w}}{2m}}}\\n   for the expected number of edges between two nodes. Additionally, in a large random network, the number of self-loops and multi-edges is vanishingly small. Ignoring self-loops and multi-edges allows one to assume that there is at most one edge between any two nodes. In that case, \\n  \\n    \\n      \\n        \\n          J\\n          \\n            v\\n            w\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle J_{vw}}\\n   becomes a binary indicator variable, so its expected value is also the probability that it equals \\n  \\n    \\n      \\n        1\\n      \\n    \\n    {\\\\displaystyle 1}\\n  , which means one can approximate the probability of an edge existing between nodes \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   and \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   as \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                k\\n                \\n                  v\\n                \\n              \\n              \\n                k\\n                \\n                  w\\n                \\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {k_{v}k_{w}}{2m}}}\\n  .\\n\\n\\n== Modularity ==\\nHence, the difference between the actual number of edges between node \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   and \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   and the expected number of edges between them is\\n\\n  \\n    \\n      \\n        \\n          A\\n          \\n            v\\n            w\\n          \\n        \\n        −\\n        \\n          \\n            \\n              \\n                k\\n                \\n                  v\\n                \\n              \\n              \\n                k\\n                \\n                  w\\n                \\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle A_{vw}-{\\\\frac {k_{v}k_{w}}{2m}}}\\n  \\nSumming over all node pairs gives the equation for modularity, \\n  \\n    \\n      \\n        Q\\n      \\n    \\n    {\\\\displaystyle Q}\\n  .\\n\\nIt is important to note that Eq. 3 holds good for partitioning into two communities only. Hierarchical partitioning (i.e. partitioning into two communities, then the two sub-communities further partitioned into two smaller sub communities only to maximize Q) is a possible approach to identify multiple communities in a network. Additionally, (3) can be generalized for partitioning a network into c communities.\\n\\nwhere eij is the fraction of edges with one end vertices in community i and the other in community j:\\n\\n  \\n    \\n      \\n        \\n          e\\n          \\n            i\\n            j\\n          \\n        \\n        =\\n        \\n          ∑\\n          \\n            v\\n            w\\n          \\n        \\n        \\n          \\n            \\n              A\\n              \\n                v\\n                w\\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          1\\n          \\n            v\\n            ∈\\n            \\n              c\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          1\\n          \\n            w\\n            ∈\\n            \\n              c\\n              \\n                j\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle e_{ij}=\\\\sum _{vw}{\\\\frac {A_{vw}}{2m}}1_{v\\\\in c_{i}}1_{w\\\\in c_{j}}}\\n  and ai is the fraction of ends of edges that are attached to vertices in community i:\\n\\n  \\n    \\n      \\n        \\n          a\\n          \\n            i\\n          \\n        \\n        =\\n        \\n          \\n            \\n              k\\n              \\n                i\\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        =\\n        \\n          ∑\\n          \\n            j\\n          \\n        \\n        \\n          e\\n          \\n            i\\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{i}={\\\\frac {k_{i}}{2m}}=\\\\sum _{j}e_{ij}}\\n  \\n\\n\\n== Example of multiple community detection ==\\nWe consider an undirected network with 10 nodes and 12 edges and the following adjacency matrix.\\n\\nThe communities in the graph are represented by the red, green and blue node clusters in Fig 1. The optimal community partitions are depicted in Fig 2.\\n\\n\\n== Matrix formulation ==\\nAn alternative formulation of the modularity, useful particularly in spectral optimization algorithms, is as follows.', start_char_idx=12809, end_char_idx=17937, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='bc244995-8d11-4ae1-95ed-512742e3ab91', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6859ef38-2941-4a08-a1f0-9a3e7a430898', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='735a591b325be2f57da7c8756161c4cb8071593f110f388a3f18fba441884387'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='57d7ff03-9dd8-42a5-9494-3319f961ded9', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='98a179372ab837a752b1acd6a877f94262c7e9711e16c99fe69e81e5f62384c0'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b3e4d526-d9f5-47ae-a810-5d5b7d1d9e16', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f8be77d8e35867b288b0f133f29b3bd70f3d248ce3ce792e0d6541b62935b268')}, hash='ece86016382d4aec30b71287c270f5b62a668ca9fe9078e31f93230477fe6e50', text='== Example of multiple community detection ==\\nWe consider an undirected network with 10 nodes and 12 edges and the following adjacency matrix.\\n\\nThe communities in the graph are represented by the red, green and blue node clusters in Fig 1. The optimal community partitions are depicted in Fig 2.\\n\\n\\n== Matrix formulation ==\\nAn alternative formulation of the modularity, useful particularly in spectral optimization algorithms, is as follows.  Define \\n  \\n    \\n      \\n        \\n          S\\n          \\n            v\\n            r\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{vr}}\\n   to be \\n  \\n    \\n      \\n        1\\n      \\n    \\n    {\\\\displaystyle 1}\\n   if vertex \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   belongs to group \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n   and \\n  \\n    \\n      \\n        0\\n      \\n    \\n    {\\\\displaystyle 0}\\n   otherwise.  Then\\n\\n  \\n    \\n      \\n        δ\\n        (\\n        \\n          c\\n          \\n            v\\n          \\n        \\n        ,\\n        \\n          c\\n          \\n            w\\n          \\n        \\n        )\\n        =\\n        \\n          ∑\\n          \\n            r\\n          \\n        \\n        \\n          S\\n          \\n            v\\n            r\\n          \\n        \\n        \\n          S\\n          \\n            w\\n            r\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\delta (c_{v},c_{w})=\\\\sum _{r}S_{vr}S_{wr}}\\n  and hence\\n\\n  \\n    \\n      \\n        Q\\n        =\\n        \\n          \\n            1\\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            v\\n            w\\n          \\n        \\n        \\n          ∑\\n          \\n            r\\n          \\n        \\n        \\n          [\\n          \\n            \\n              A\\n              \\n                v\\n                w\\n              \\n            \\n            −\\n            \\n              \\n                \\n                  \\n                    k\\n                    \\n                      v\\n                    \\n                  \\n                  \\n                    k\\n                    \\n                      w\\n                    \\n                  \\n                \\n                \\n                  2\\n                  m\\n                \\n              \\n            \\n          \\n          ]\\n        \\n        \\n          S\\n          \\n            v\\n            r\\n          \\n        \\n        \\n          S\\n          \\n            w\\n            r\\n          \\n        \\n        =\\n        \\n          \\n            1\\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          T\\n          r\\n        \\n        (\\n        \\n          \\n            S\\n          \\n          \\n            \\n              T\\n            \\n          \\n        \\n        \\n          B\\n          S\\n        \\n        )\\n        ,\\n      \\n    \\n    {\\\\displaystyle Q={\\\\frac {1}{2m}}\\\\sum _{vw}\\\\sum _{r}\\\\left[A_{vw}-{\\\\frac {k_{v}k_{w}}{2m}}\\\\right]S_{vr}S_{wr}={\\\\frac {1}{2m}}\\\\mathrm {Tr} (\\\\mathbf {S} ^{\\\\mathrm {T} }\\\\mathbf {BS} ),}\\n  where \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   is the (non-square) matrix having elements \\n  \\n    \\n      \\n        \\n          S\\n          \\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{v}}\\n   and \\n  \\n    \\n      \\n        B\\n      \\n    \\n    {\\\\displaystyle B}\\n   is the so-called modularity matrix, which has elements\\n\\n  \\n    \\n      \\n        \\n          B\\n          \\n            v\\n            w\\n          \\n        \\n        =\\n        \\n          A\\n          \\n            v\\n            w\\n          \\n        \\n        −\\n        \\n          \\n            \\n              \\n                k\\n                \\n                  v\\n                \\n              \\n              \\n                k\\n                \\n                  w\\n                \\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle B_{vw}=A_{vw}-{\\\\frac {k_{v}k_{w}}{2m}}.}\\n  All rows and columns of the modularity matrix sum to zero, which means that the modularity of an undivided network is also always \\n  \\n    \\n      \\n        0\\n      \\n    \\n    {\\\\displaystyle 0}\\n  .', start_char_idx=17497, end_char_idx=21668, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='b3e4d526-d9f5-47ae-a810-5d5b7d1d9e16', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6859ef38-2941-4a08-a1f0-9a3e7a430898', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='735a591b325be2f57da7c8756161c4cb8071593f110f388a3f18fba441884387'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='bc244995-8d11-4ae1-95ed-512742e3ab91', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='ece86016382d4aec30b71287c270f5b62a668ca9fe9078e31f93230477fe6e50'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='22b05407-47c9-4726-b2a7-c9f7eda56505', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b9bb7e8c50702da84fd1e9e8267771d8585701c1c2effbdb7caf39eff8ad9a7e')}, hash='f8be77d8e35867b288b0f133f29b3bd70f3d248ce3ce792e0d6541b62935b268', text=\"{\\\\displaystyle B_{vw}=A_{vw}-{\\\\frac {k_{v}k_{w}}{2m}}.}\\n  All rows and columns of the modularity matrix sum to zero, which means that the modularity of an undivided network is also always \\n  \\n    \\n      \\n        0\\n      \\n    \\n    {\\\\displaystyle 0}\\n  .\\nFor networks divided into just two communities, one can alternatively define \\n  \\n    \\n      \\n        \\n          s\\n          \\n            v\\n          \\n        \\n        =\\n        ±\\n        1\\n      \\n    \\n    {\\\\displaystyle s_{v}=\\\\pm 1}\\n   to indicate the community to which node \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   belongs, which then leads to\\n\\n  \\n    \\n      \\n        Q\\n        =\\n        \\n          \\n            1\\n            \\n              4\\n              m\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            v\\n            w\\n          \\n        \\n        \\n          B\\n          \\n            v\\n            w\\n          \\n        \\n        \\n          s\\n          \\n            v\\n          \\n        \\n        \\n          s\\n          \\n            w\\n          \\n        \\n        =\\n        \\n          \\n            1\\n            \\n              4\\n              m\\n            \\n          \\n        \\n        \\n          \\n            s\\n          \\n          \\n            \\n              T\\n            \\n          \\n        \\n        \\n          B\\n          s\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle Q={1 \\\\over 4m}\\\\sum _{vw}B_{vw}s_{v}s_{w}={1 \\\\over 4m}\\\\mathbf {s} ^{\\\\mathrm {T} }\\\\mathbf {Bs} ,}\\n  where \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n   is the column vector with elements \\n  \\n    \\n      \\n        \\n          s\\n          \\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle s_{v}}\\n  .This function has the same form as the Hamiltonian of an Ising spin glass, a connection that has been exploited to create simple computer algorithms, for instance using simulated annealing, to maximize the modularity.  The general form of the modularity for arbitrary numbers of communities is equivalent to a Potts spin glass and similar algorithms can be developed for this case also.\\n\\n\\n== Overfitting ==\\nAlthough the method of modularity maximization is motivated by computing a deviation from a null model, this deviation is not computed in a statistically consistent manner. Because of this, the method notoriously finds high-scoring communities in its own null model\\n\\n(the configuration model), which by definition cannot be statistically significant. Because of this, the method cannot be used to reliably obtain statistically significant community structure in empirical networks.\\n\\n\\n== Resolution limit ==\\nModularity compares the number of edges inside a cluster with the expected number of edges that\\none would find in the cluster if the network were a random network with the same number of nodes and where\\neach node keeps its degree, but edges are otherwise randomly attached. This random null model implicitly assumes that each node can get attached to any other node of the network. This assumption is however unreasonable if the network is very large, as the horizon of a node includes a small part of the network, ignoring most of it.\\nMoreover, this implies that the expected number of edges between two groups of nodes decreases if the size of the network increases. So, if a network is large enough, the expected number of edges between two groups of nodes in modularity's null model may be smaller than one. If this happens, a single edge between the two clusters would be interpreted by modularity as a sign of a strong correlation between the two clusters, and optimizing modularity would lead to the merging of the two clusters, independently of the clusters' features. So, even weakly interconnected complete graphs, which have the highest possible density of internal edges, and represent the best identifiable communities, would be merged by modularity optimization if the network were sufficiently large.\\nFor this reason, optimizing modularity in large networks would fail to resolve small communities, even when they are well defined. This bias\\nis inevitable for methods like modularity optimization, which rely on a global null model.\\n\\n\\n== Multiresolution methods ==\\nThere are two main approaches which try to solve the resolution limit within the modularity context: the addition of a resistance r to every node, in the form of a self-loop, which increases (r>0) or decreases (r<0) the aversion of nodes to form communities; or the addition of a parameter γ>0 in front of the null-case term in the definition of modularity, which controls the relative importance between internal links of the communities and the null model. Optimizing modularity for values of these parameters in their respective appropriate ranges, it is possible to recover the whole mesoscale of the network, from the macroscale in which all nodes belong to the same community, to the microscale in which every node forms its own community, hence the name multiresolution methods. However, it has been shown that these methods have limitations when communities are very heterogeneous in size.\", start_char_idx=21417, end_char_idx=26516, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='22b05407-47c9-4726-b2a7-c9f7eda56505', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6859ef38-2941-4a08-a1f0-9a3e7a430898', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='735a591b325be2f57da7c8756161c4cb8071593f110f388a3f18fba441884387'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='b3e4d526-d9f5-47ae-a810-5d5b7d1d9e16', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f8be77d8e35867b288b0f133f29b3bd70f3d248ce3ce792e0d6541b62935b268'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='1a33ce25-10ab-4263-b874-ca76f03ee8ad', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='d7c57507d122dc39f93ddb0ac7bc221b89b84a0a8fbddc3bb6c678573a817e7e')}, hash='b9bb7e8c50702da84fd1e9e8267771d8585701c1c2effbdb7caf39eff8ad9a7e', text='== Multiresolution methods ==\\nThere are two main approaches which try to solve the resolution limit within the modularity context: the addition of a resistance r to every node, in the form of a self-loop, which increases (r>0) or decreases (r<0) the aversion of nodes to form communities; or the addition of a parameter γ>0 in front of the null-case term in the definition of modularity, which controls the relative importance between internal links of the communities and the null model. Optimizing modularity for values of these parameters in their respective appropriate ranges, it is possible to recover the whole mesoscale of the network, from the macroscale in which all nodes belong to the same community, to the microscale in which every node forms its own community, hence the name multiresolution methods. However, it has been shown that these methods have limitations when communities are very heterogeneous in size.\\n\\n\\n== Software Tools ==\\nThere are a couple of software tools available that are able to compute clusterings in graphs with good modularity.\\nOriginal implementation of the multi-level Louvain method.The Leiden algorithm which additionally avoids unconnected communities.The Vienna Graph Clustering (VieClus) algorithm, a parallel memetic algorithm.\\n\\n\\n== See also ==\\nComplex network\\nCommunity structure\\nNull model\\nPercolation theory\\n\\n\\n== References ==', start_char_idx=25589, end_char_idx=26965, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='1a33ce25-10ab-4263-b874-ca76f03ee8ad', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='10ce2e27-9666-471f-8d66-5b4af242079c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='f4605aac6375f764ce1d16f4e8bfc20dbe7891701bb380555f0a8e33de75b1c1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='22b05407-47c9-4726-b2a7-c9f7eda56505', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='b9bb7e8c50702da84fd1e9e8267771d8585701c1c2effbdb7caf39eff8ad9a7e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='4293944d-9502-4e9e-bf02-1fe908c8ed8d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='0f4de134de85b56ebaec21a08dd1fa1d41b62c46f7c837b8a958df6a5bc62e89')}, hash='d7c57507d122dc39f93ddb0ac7bc221b89b84a0a8fbddc3bb6c678573a817e7e', text='In the study of complex networks, a network is said to have community structure if the nodes of the network can be easily grouped into (potentially overlapping) sets of nodes such that each set of nodes is densely connected internally. In the particular case of non-overlapping community finding, this implies that the network divides naturally into groups of nodes with dense connections internally and sparser connections between groups.  But overlapping communities are also allowed. The more general definition is based on the principle that pairs of nodes are more likely to be connected if they are both members of the same community(ies), and less likely to be connected if they do not share communities. A related but different problem is community search, where the goal is to find a community that a certain vertex belongs to.\\n\\n\\n== Properties ==\\nIn the study of networks, such as computer and information networks, social networks and biological networks, a number of different characteristics have been found to occur commonly, including the small-world property, heavy-tailed degree distributions, and clustering, among others. Another common characteristic is community structure.\\nIn the context of networks, community structure refers to the occurrence of groups of nodes in a network that are more densely connected internally than with the rest of the network, as shown in the example image to the right.  This inhomogeneity of connections suggests that the network has certain natural divisions within it.\\nCommunities are often defined in terms of the partition of the set of vertices, that is each node is put into one and only one community, just as in the figure. This is a useful simplification and most community detection methods find this type of community structure. However, in some cases a better representation could be one where vertices are in more than one community.  This might happen in a social network where each vertex represents a person, and the communities represent the different groups of friends: one community for family, another community for co-workers, one for friends in the same sports club, and so on. The use of cliques for community detection discussed below is just one example of how such overlapping community structure can be found.\\nSome networks may not have any meaningful community structure.  Many basic network models, for example, such as the random graph and the Barabási–Albert model, do not display community structure.\\n\\n\\n== Importance ==\\nCommunity structures are quite common in real networks. Social networks include community groups (the origin of the term, in fact) based on common location, interests, occupation, etc.Finding an underlying community structure in a network, if it exists, is important for a number of reasons. Communities allow us to create a large scale map of a network since individual communities act like meta-nodes in the network which makes its study easier.Individual communities also shed light on the function of the system represented by the network since communities often correspond to functional units of the system. In metabolic networks, such functional groups correspond to cycles or pathways whereas in the protein interaction network, communities correspond to proteins with similar functionality inside a biological cell. Similarly, citation networks form communities by research topic. Being able to identify these sub-structures within a network can provide insight into how network function and topology affect each other. Such insight can be useful in improving some algorithms on graphs such as spectral clustering.Importantly, communities often have very different properties than the average properties of the networks. Thus, only concentrating on the average properties usually misses many important and interesting features inside the networks. For example, in a given social network, both gregarious and reticent groups might exists simultaneously.Existence of communities also generally affects various processes like rumour spreading or epidemic spreading happening on a network. Hence to properly understand such processes, it is important to detect communities and also to study how they affect the spreading processes in various settings.\\nFinally, an important application that community detection has found in network science is the prediction of missing links and the identification of false links in the network. During the measurement process, some links may not get observed for a number of reasons. Similarly, some links could falsely enter into the data because of the errors in the measurement. Both these cases are well handled by community detection algorithm since it allows one to assign the probability of existence of an edge between a given pair of nodes.\\n\\n\\n== Algorithms for finding communities ==\\nFinding communities within an arbitrary network can be a computationally difficult task.  The number of communities, if any, within the network is typically unknown and the communities are often of unequal size and/or density.  Despite these difficulties, however, several methods for community finding have been developed and employed with varying levels of success.', start_char_idx=0, end_char_idx=5202, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='4293944d-9502-4e9e-bf02-1fe908c8ed8d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='10ce2e27-9666-471f-8d66-5b4af242079c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='f4605aac6375f764ce1d16f4e8bfc20dbe7891701bb380555f0a8e33de75b1c1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1a33ce25-10ab-4263-b874-ca76f03ee8ad', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='d7c57507d122dc39f93ddb0ac7bc221b89b84a0a8fbddc3bb6c678573a817e7e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='891de726-5373-4f22-9e48-f12f983c2e5d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='35c7edcb9c32ec4b8af6bd897d48325698bea0f370a576964c20d676dc7ffa82')}, hash='0f4de134de85b56ebaec21a08dd1fa1d41b62c46f7c837b8a958df6a5bc62e89', text='== Algorithms for finding communities ==\\nFinding communities within an arbitrary network can be a computationally difficult task.  The number of communities, if any, within the network is typically unknown and the communities are often of unequal size and/or density.  Despite these difficulties, however, several methods for community finding have been developed and employed with varying levels of success.\\n\\n\\n=== Minimum-cut method ===\\nOne of the oldest algorithms for dividing networks into parts is the minimum cut method (and variants such as ratio cut and normalized cut).  This method sees use, for example, in load balancing for parallel computing in order to minimize communication between processor nodes.\\nIn the minimum-cut method, the network is divided into a predetermined number of parts, usually of approximately the same size, chosen such that the number of edges between groups is minimized.  The method works well in many of the applications for which it was originally intended but is less than ideal for finding community structure in general networks since it will find communities regardless of whether they are implicit in the structure, and it will find only a fixed number of them.\\n\\n\\n=== Hierarchical clustering ===\\nAnother method for finding community structures in networks is hierarchical clustering.  In this method one defines a similarity measure quantifying some (usually topological) type of similarity between node pairs.  Commonly used measures include the cosine similarity, the Jaccard index, and the Hamming distance between rows of the adjacency matrix.  Then one groups similar nodes into communities according to this measure.  There are several common schemes for performing the grouping, the two simplest being single-linkage clustering, in which two groups are considered separate communities if and only if all pairs of nodes in different groups have similarity lower than a given threshold, and complete linkage clustering, in which all nodes within every group have similarity greater than a threshold. An important step is how to determine the threshold to stop the agglomerative clustering, indicating a near-to-optimal community structure. A common strategy consist to build one or several metrics monitoring global properties of the network, which peak at given step of the clustering. An interesting approach in this direction is the use of various similarity or dissimilarity measures, combined through convex sums,. Another approximation is the computation of a quantity monitoring the density of edges within clusters with respect to the density between clusters, such as the partition density, which has been proposed when the similarity metric is defined between edges (which permits the definition of overlapping communities), and extended when the similarity is defined between nodes, which allows to consider alternative definitions of communities such as guilds (i.e. groups of nodes sharing a similar number of links with respect to the same neighbours but not necessarily connected themselves). These methods can be extended to consider multidimensional networks, for instance when we are dealing with networks having nodes with different types of links.\\n\\n\\n=== Girvan–Newman algorithm ===\\nAnother commonly used algorithm for finding communities is the Girvan–Newman algorithm.  This algorithm identifies edges in a network that lie between communities and then removes them, leaving behind just the communities themselves.  The identification is performed by employing the graph-theoretic measure betweenness centrality, which assigns a number to each edge which is large if the edge lies \"between\" many pairs of nodes.\\nThe Girvan–Newman algorithm returns results of reasonable quality and is popular because it has been implemented in a number of standard software packages.  But it also runs slowly, taking time O(m2n) on a network of n vertices and m edges, making it impractical for networks of more than a few thousand nodes.\\n\\n\\n=== Modularity maximization ===\\nIn spite of its known drawbacks, one of the most widely used methods for community detection is modularity maximization.  Modularity is a benefit function that measures the quality of a particular division of a network into communities.  The modularity maximization method detects communities by searching over possible divisions of a network for one or more that have particularly high modularity.  Since exhaustive search over all possible divisions is usually intractable, practical algorithms are based on approximate optimization methods such as greedy algorithms, simulated annealing, or spectral optimization, with different approaches offering different balances between speed and accuracy.\\nA popular modularity maximization approach is the Louvain method, which iteratively optimizes local communities until global modularity can no longer be improved given perturbations to the current community state.\\nAn algorithm that utilizes the RenEEL scheme, which is an example of the Extremal Ensemble Learning (EEL) paradigm, is currently the best modularity maximizing algorithm.The usefulness of modularity optimization is questionable, as it has been shown that modularity optimization often fails to detect clusters smaller than some scale, depending on the size of the network (resolution limit); on the other hand the landscape of modularity values is characterized by a huge degeneracy of partitions with high modularity, close to the absolute maximum, which may be very different from each other.', start_char_idx=4794, end_char_idx=10330, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='891de726-5373-4f22-9e48-f12f983c2e5d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='10ce2e27-9666-471f-8d66-5b4af242079c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='f4605aac6375f764ce1d16f4e8bfc20dbe7891701bb380555f0a8e33de75b1c1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4293944d-9502-4e9e-bf02-1fe908c8ed8d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='0f4de134de85b56ebaec21a08dd1fa1d41b62c46f7c837b8a958df6a5bc62e89'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='0b9ab24a-11a9-474c-89d9-75fba3587393', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='0d80173700fd137520c5f1dbf53869a397991cef2fec2765adc3e8c101277040')}, hash='35c7edcb9c32ec4b8af6bd897d48325698bea0f370a576964c20d676dc7ffa82', text='=== Statistical inference ===\\nMethods based on statistical inference attempt to fit a generative model to the network data, which encodes the community structure. The overall advantage of this approach compared to the alternatives is its more principled nature, and the capacity to inherently address issues of statistical significance. Most methods in the literature are based on the stochastic block model as well as variants including mixed membership,\\ndegree-correction, and hierarchical structures.Model selection can be performed using principled approaches such as minimum description length (or equivalently, Bayesian model selection) and likelihood-ratio test. Currently many algorithms exist to perform efficient inference of stochastic block models, including belief propagation\\nand agglomerative Monte Carlo.In contrast to approaches that attempt to cluster a network given an objective function, this class of methods is based on generative models, which not only serve as a description of the large-scale structure of the network, but also can be used to generalize the data and predict the occurrence of missing or spurious links in the network.\\n\\n\\n=== Clique-based methods ===\\nCliques are subgraphs in which every node is connected to every other node in the clique.  As nodes can not be more tightly connected than this, it is not surprising that there are many approaches to community detection in networks based on the detection of cliques in a graph and the analysis of how these overlap.  Note that as a node can be a member of more than one clique, a node can be a member of more than one community in these methods giving an \"overlapping community structure\".\\nOne approach is to find the \"maximal cliques\". That is to find the cliques which are not the subgraph of any other clique.  The classic algorithm to find these is the Bron–Kerbosch algorithm. The overlap of these can be used to define communities in several ways.  The simplest is to consider only  maximal cliques bigger than a minimum size (number of nodes).  The union of these cliques then defines a subgraph whose components (disconnected parts) then define communities. Such approaches are often implemented in social network analysis software such as UCInet.\\nThe alternative approach is to use cliques of fixed size \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  . The overlap of these can be used to define a type of \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -regular hypergraph or a structure which is a generalisation of the line graph (the case when \\n  \\n    \\n      \\n        k\\n        =\\n        2\\n      \\n    \\n    {\\\\displaystyle k=2}\\n  ) known as a \"Clique graph\".  The clique graphs have vertices which represent the cliques in the original graph while the edges of the clique graph record the overlap of the clique in the original graph.  Applying any of the previous community detection methods (which assign each node to a community) to the clique graph then assigns each clique to a community.  This can then be used to determine community membership of nodes in the cliques.  Again as a node may be in several cliques, it can be a member of several communities.\\nFor instance the  clique percolation method defines communities as percolation clusters of \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -cliques. To do this it\\nfinds all \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -cliques in a network, that is all the complete sub-graphs of \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -nodes.\\nIt then defines two \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -cliques to be adjacent if they share \\n  \\n    \\n      \\n        k\\n        −\\n        1\\n      \\n    \\n    {\\\\displaystyle k-1}\\n   nodes, that is this is used to define edges in a clique graph. A community is then defined to be the maximal union of \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -cliques in which we can reach any \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -clique from any other \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -clique through series of \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -clique adjacencies. That is communities are just the connected components in the clique graph. Since a node can belong to several different \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -clique percolation clusters at the same time, the communities can overlap with each other.', start_char_idx=10333, end_char_idx=14857, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='0b9ab24a-11a9-474c-89d9-75fba3587393', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='10ce2e27-9666-471f-8d66-5b4af242079c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='f4605aac6375f764ce1d16f4e8bfc20dbe7891701bb380555f0a8e33de75b1c1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='891de726-5373-4f22-9e48-f12f983c2e5d', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='35c7edcb9c32ec4b8af6bd897d48325698bea0f370a576964c20d676dc7ffa82'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2ed941f3-6cd6-4b9a-aba4-2eb46311bbc2', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='2eef574898492658036e80cc4b11b2f753835c68f657c6ea4dc016c4646563c4')}, hash='0d80173700fd137520c5f1dbf53869a397991cef2fec2765adc3e8c101277040', text='=== Community detection in latent feature spaces ===\\nA network can be represented or projected onto a latent space via representation learning methods to efficiently represent a system. Then, various clustering methods can be employed to detect community structures. For Euclidean spaces, methods like embedding-based Silhouette community detection can be utilized. For Hypergeometric latent spaces, critical gap method or modified density-based, hierarchical, or partitioning-based clustering methods can be utilized.\\n\\n\\n== Testing methods of finding communities algorithms ==\\nThe evaluation of algorithms, to detect which are better at detecting community structure, is still an open question. It must be based on analyses of networks of known structure. A typical example is the \"four groups\" test, in which a network is divided into four equally-sized groups (usually of 32 nodes each) and the probabilities of connection within and between groups varied to create more or less challenging structures for the detection algorithm. Such benchmark graphs are a special case of the planted l-partition model\\nof Condon and Karp, or more generally of \"stochastic block models\", a general class of random network models containing community structure. Other more flexible benchmarks have been proposed that allow for varying group sizes and nontrivial degree distributions, such as LFR benchmark\\nwhich is an extension of the four groups benchmark that includes heterogeneous distributions of node degree and community size, making it a more severe test of community detection methods.Commonly used computer-generated benchmarks start with a network of well-defined communities. Then, this structure is degraded by rewiring or removing links and it gets harder and harder for the algorithms to detect the original partition. At the end, the network reaches a point where it is essentially random. This kind of benchmark may be called \"open\". The performance on these benchmarks is evaluated by measures such as normalized mutual information or variation of information. They compare the solution obtained by an algorithm  with the original community structure, evaluating the similarity of both partitions.', start_char_idx=14860, end_char_idx=17061, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None),\n",
       " NodeWithScore(node=TextNode(id_='2ed941f3-6cd6-4b9a-aba4-2eb46311bbc2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='10ce2e27-9666-471f-8d66-5b4af242079c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='f4605aac6375f764ce1d16f4e8bfc20dbe7891701bb380555f0a8e33de75b1c1'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0b9ab24a-11a9-474c-89d9-75fba3587393', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='0d80173700fd137520c5f1dbf53869a397991cef2fec2765adc3e8c101277040')}, hash='2eef574898492658036e80cc4b11b2f753835c68f657c6ea4dc016c4646563c4', text='== Detectability ==\\nDuring recent years, a rather surprising result has been obtained by various groups which shows that a phase transition exists in the community detection problem, showing that as the density of connections inside communities and between communities become more and more equal or both become smaller (equivalently, as the community structure becomes too weak or the network becomes too sparse), suddenly the communities become undetectable. In a sense, the communities themselves still exist, since the presence and absence of edges is still correlated with the community memberships of their endpoints; but it becomes information-theoretically impossible to label the nodes better than chance, or even distinguish the graph from one generated by a null model such as the Erdos–Renyi model without community structure. This transition is independent of the type of algorithm being used to detect communities, implying that there exists a fundamental limit on our ability to detect communities in networks, even with optimal Bayesian inference (i.e., regardless of our computational resources).Consider a stochastic block model with total \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   nodes, \\n  \\n    \\n      \\n        q\\n        =\\n        2\\n      \\n    \\n    {\\\\displaystyle q=2}\\n   groups of equal size, and let \\n  \\n    \\n      \\n        \\n          p\\n          \\n            in\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{in}}}\\n   and \\n  \\n    \\n      \\n        \\n          p\\n          \\n            out\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{out}}}\\n   be the connection probabilities inside and between the groups respectively. If \\n  \\n    \\n      \\n        \\n          p\\n          \\n            in\\n          \\n        \\n        >\\n        \\n          p\\n          \\n            out\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{in}}>p_{\\\\text{out}}}\\n  , the network would possess community structure since the link density inside the groups would be more than the density of links between the groups.  In the sparse case, \\n  \\n    \\n      \\n        \\n          p\\n          \\n            in\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{in}}}\\n   and \\n  \\n    \\n      \\n        \\n          p\\n          \\n            out\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{out}}}\\n   scale as \\n  \\n    \\n      \\n        O\\n        (\\n        1\\n        \\n          /\\n        \\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(1/n)}\\n   so that the average degree is constant:\\n\\n  \\n    \\n      \\n        \\n          p\\n          \\n            in\\n          \\n        \\n        =\\n        \\n          c\\n          \\n            in\\n          \\n        \\n        \\n          /\\n        \\n        n\\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{in}}=c_{\\\\text{in}}/n}\\n   and \\n  \\n    \\n      \\n        \\n          p\\n          \\n            out\\n          \\n        \\n        =\\n        \\n          c\\n          \\n            out\\n          \\n        \\n        \\n          /\\n        \\n        n\\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{out}}=c_{\\\\text{out}}/n}\\n  Then it becomes impossible to detect the communities when:\\n\\n  \\n    \\n      \\n        \\n          c\\n          \\n            in\\n          \\n        \\n        −\\n        \\n          c\\n          \\n            out\\n          \\n        \\n        =\\n        \\n          \\n            2\\n            (\\n            \\n              c\\n              \\n                in\\n              \\n            \\n            +\\n            \\n              c\\n              \\n                out\\n              \\n            \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle c_{\\\\text{in}}-c_{\\\\text{out}}={\\\\sqrt {2(c_{\\\\text{in}}+c_{\\\\text{out}})}}}\\n  \\n\\n\\n== See also ==\\nComplex network\\nHierarchy\\nNetwork theory\\nPercolation theory\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nCommunity detection in graphs – an introduction\\nAre there implementations of algorithms for community detection in graphs? – Stack Overflow\\nWhat are the differences between community detection algorithms in igraph? – Stack Overflow', start_char_idx=17064, end_char_idx=21108, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_acc.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Response 1: The Louvain Method for Community Detection | The Louvain method is a greedy optimization method developed by Blondel et al. from the University of Louvain for extracting non-overlapping communities from large networks. The method aims to optimize modularity, a measure of the relative density of edges within communities compared to edges between communities. By iteratively optimizing modularity locally on all nodes and grouping small communities into one node, the Louvain method produces the best possible grouping of nodes in a network. The method has a time complexity of O(n*log(n)), where n is the number of nodes in the network.\\n---------------------\\nResponse 2: Algorithm for Optimizing Modularity in Networks | This text explains the algorithm for optimizing modularity in networks. Modularity is a measure of the density of links within communities compared to links between communities. The algorithm uses the Louvain Method, which has two phases. In the first phase, each node is assigned to its own community. In the second phase, the change in modularity is calculated for moving a node from its own community to the community of each of its neighbors. This process is repeated iteratively to maximize modularity efficiently.\\n---------------------\\nResponse 3: Modularity Optimization Algorithm for Community Detection | The text describes a modularity optimization algorithm for community detection in a network. The algorithm assigns each node to its own community and then calculates the change in modularity for moving the node into the community of each of its neighbors. The algorithm uses equations to calculate the modularity change and places the node into the community that results in the greatest increase in modularity. This process is repeated for all nodes until no further increase in modularity is possible. The algorithm aims to find the local maximum of modularity, indicating the optimal community structure in the network.\\n---------------------\\nResponse 4: Louvain Method for Community Detection in Large Networks | The Louvain method is an algorithm used for community detection in large networks. It involves two phases: in the first phase, nodes are iteratively moved to communities that result in the greatest increase in modularity. In the second phase, the nodes are grouped into communities and a new network is created. The Louvain method has been used in various applications, such as partitioning online social networks and identifying dynamic communities in mobile phone networks. However, it has limitations, such as producing only non-overlapping communities and sometimes creating poorly connected communities. Nonetheless, it has been shown to outperform other modularity optimization methods in terms of both speed and resulting modularity value.\\n---------------------\\nResponse 5: The Importance of Modularity in Network Analysis | Modularity is a measure of the structure of networks that determines the strength of division into modules or communities. It is commonly used in optimization methods for detecting community structure in networks. However, modularity maximization is not statistically consistent and cannot be used to find statistically significant community structures in real-world networks. Additionally, modularity suffers from a resolution limit and is unable to detect small communities. Despite these limitations, modularity is still an important measure in network analysis as it helps reveal unexpected structural features and understand the dynamics of various networks.\\n---------------------\\nResponse 6: Modularity in Graph Theory | Modularity is a measure of the concentration of edges within groups compared to a random distribution of links between all nodes. It is calculated by subtracting the expected fraction of edges within groups from the actual fraction. The value of modularity for unweighted and undirected graphs ranges from -1/2 to 1, with positive values indicating a higher number of edges within groups than expected by chance. Different methods can be used to calculate modularity, with the most common version preserving the degree of each vertex. The expected number of edges is computed using a configuration model, which randomizes the network while maintaining the node degree distribution.\\n---------------------\\nResponse 7: Expected Number of Edges Between Nodes | This text discusses the calculation of the expected number of edges between two nodes in a randomly rewired network. It explains the use of indicator variables to determine if a stub of one node connects to a stub of the other node. The indicator variables are set to 1 if a connection is made and 0 if not. The text provides a mathematical approach to calculating the expected number of full edges between the nodes.\\n---------------------\\nResponse 8: Approximations for Expected Number of Edges in Random Networks | This text discusses the approximations made for the expected number of edges between two nodes in random networks with a large number of edges. It explains that when the number of edges is large, the subtraction of 1 in the denominator is dropped, and the approximate expression k_v * k_w / 2m is used instead.\\n---------------------\\nResponse 9: Modularity and Community Detection in Networks | This text explains the concept of modularity in network analysis and its application in community detection. It discusses the approximation of the expected number of edges between nodes in a large random network and how it can be used to calculate the probability of an edge existing between two nodes. The text also mentions the equation for modularity and its use in partitioning a network into communities. It concludes with an example of multiple community detection and an alternative formulation of modularity.\\n---------------------\\nResponse 10: Matrix Formulation of Modularity for Community Detection | This text explains the matrix formulation of modularity, which is a measure used in community detection algorithms. The modularity matrix is defined using the adjacency matrix of a network and represents the optimal community partitions. The formula for modularity involves the sum of elements in the modularity matrix and the transpose of the matrix. The modularity of an undivided network is always zero.\\n---------------------\\nResponse 11: Limitations of Modularity Maximization for Community Detection | Modularity maximization, a method commonly used for community detection in networks, has several limitations. Firstly, it is prone to overfitting and often finds high-scoring communities in its own null model, which are not statistically significant. Secondly, it suffers from a resolution limit, where it fails to detect small communities in large networks. This is because the null model assumes that each node can be connected to any other node, which is unrealistic for large networks. To address these limitations, multiresolution methods have been proposed, but they also have limitations when communities are highly heterogeneous in size.\\n---------------------\\nResponse 12: Multiresolution Methods for Community Detection in Networks | This text discusses two main approaches for solving the resolution limit in community detection within the modularity context. One approach involves adding a resistance parameter to each node, either increasing or decreasing their aversion to form communities. The other approach involves adding a parameter that controls the relative importance between internal links of the communities and the null model. These multiresolution methods allow for the recovery of the entire mesoscale of a network, from a macro-scale where all nodes belong to the same community, to a micro-scale where each node forms its own community. However, these methods have limitations when communities vary greatly in size. The text also mentions several software tools available for computing clusterings with good modularity.\\n---------------------\\nResponse 13: The Importance of Community Structure in Networks | Community structure refers to the occurrence of groups of nodes in a network that are more densely connected internally than with the rest of the network. This structure is common in various types of networks, including social networks, biological networks, and information networks. Identifying community structure is important for understanding network function and topology, as communities often correspond to functional units of the system. Additionally, communities can affect processes such as rumor spreading and epidemic spreading on a network. Various algorithms have been developed to find communities within networks, despite the computational challenges posed by the unknown number and unequal size/density of communities.\\n---------------------\\nResponse 14: Algorithms for Finding Communities in Networks | This text discusses various algorithms for finding communities within networks. The minimum-cut method divides the network into parts by minimizing the number of edges between groups. Hierarchical clustering groups similar nodes into communities based on a similarity measure. The Girvan-Newman algorithm identifies edges between communities and removes them. Modularity maximization searches for divisions of the network with high modularity, with the Louvain method being a popular approach. The RenEEL scheme is currently the best modularity maximizing algorithm. However, modularity optimization has limitations in detecting small clusters and has a degeneracy of partitions with high modularity.\\n---------------------\\nResponse 15: Methods for Community Detection in Networks | This text discusses two approaches for community detection in networks: statistical inference and clique-based methods. Statistical inference involves fitting a generative model to the network data, allowing for the identification of community structures and addressing issues of statistical significance. Clique-based methods, on the other hand, focus on detecting cliques in the graph and analyzing their overlap to define communities. These methods can be implemented using algorithms such as the Bron-Kerbosch algorithm or by creating clique graphs. Both approaches offer different ways to identify communities in networks and can be used to predict missing or spurious links.\\n---------------------\\nResponse 16: Testing Methods for Community Detection Algorithms | This text discusses the evaluation of algorithms for detecting community structures in networks. It highlights the importance of analyzing networks of known structure to determine the effectiveness of different algorithms. The \"four groups\" test is commonly used, where a network is divided into four equally-sized groups and the probabilities of connection within and between groups are varied. Other benchmarks, such as the LFR benchmark, allow for varying group sizes and nontrivial degree distributions. These benchmarks help assess the performance of algorithms by comparing the obtained solution with the original community structure using measures like normalized mutual information or variation of information.\\n---------------------\\nResponse 17: Detectability of Communities in Networks | This text discusses the concept of detectability in community detection problems in networks. It explains that there is a phase transition in detectability, where communities become undetectable as the density of connections within and between communities becomes more equal or smaller. While the communities still exist, it becomes impossible to label nodes or distinguish the graph from a null model without community structure. This transition is independent of the algorithm used for community detection, suggesting a fundamental limit on our ability to detect communities in networks. The text also mentions the conditions under which communities become undetectable in a stochastic block model.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f'{response_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_summary._index_struct.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response_acc.source_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Response 1: The Louvain Method for Community Detection | The Louvain method is a greedy optimization method developed by Blondel et al. from the University of Louvain for extracting non-overlapping communities from large networks. The method aims to optimize modularity, a measure of the relative density of edges within communities compared to edges between communities. By iteratively optimizing modularity locally on all nodes and grouping small communities into one node, the Louvain method produces the best possible grouping of nodes in a network. The method has a time complexity of O(n*log(n)), where n is the number of nodes in the network.\n",
       "---------------------\n",
       "Response 2: Algorithm for Optimizing Modularity in Networks | This text explains the algorithm for optimizing modularity in networks. Modularity is a measure of the density of links within communities compared to links between communities. The algorithm uses the Louvain Method, which has two phases. In the first phase, each node is assigned to its own community. In the second phase, the change in modularity is calculated for moving a node from its own community to the community of each of its neighbors. This process is repeated iteratively to maximize modularity efficiently.\n",
       "---------------------\n",
       "Response 3: Modularity Optimization Algorithm for Community Detection | The text describes a modularity optimization algorithm for community detection in a network. The algorithm assigns each node to its own community and then calculates the change in modularity for moving the node into the community of each of its neighbors. The algorithm uses equations to calculate the modularity change and places the node into the community that results in the greatest increase in modularity. This process is repeated for all nodes until no further increase in modularity is possible. The algorithm aims to find the local maximum of modularity, indicating the optimal community structure in the network.\n",
       "---------------------\n",
       "Response 4: Louvain Method for Community Detection in Large Networks | The Louvain method is an algorithm used for community detection in large networks. It involves two phases: in the first phase, nodes are iteratively moved to communities that result in the greatest increase in modularity. In the second phase, the nodes are grouped into communities and a new network is created. The Louvain method has been used in various applications, such as partitioning online social networks and identifying dynamic communities in mobile phone networks. However, it has limitations, such as producing only non-overlapping communities and sometimes creating poorly connected communities. Nonetheless, it has been shown to outperform other modularity optimization methods in terms of both speed and resulting modularity value.\n",
       "---------------------\n",
       "Response 5: The Importance of Modularity in Network Analysis | Modularity is a measure of the structure of networks that determines the strength of division into modules or communities. It is commonly used in optimization methods for detecting community structure in networks. However, modularity maximization is not statistically consistent and cannot be used to find statistically significant community structures in real-world networks. Additionally, modularity suffers from a resolution limit and is unable to detect small communities. Despite these limitations, modularity is still an important measure in network analysis as it helps reveal unexpected structural features and understand the dynamics of various networks.\n",
       "---------------------\n",
       "Response 6: Modularity in Graph Theory | Modularity is a measure of the concentration of edges within groups compared to a random distribution of links between all nodes. It is calculated by subtracting the expected fraction of edges within groups from the actual fraction. The value of modularity for unweighted and undirected graphs ranges from -1/2 to 1, with positive values indicating a higher number of edges within groups than expected by chance. Different methods can be used to calculate modularity, with the most common version preserving the degree of each vertex. The expected number of edges is computed using a configuration model, which randomizes the network while maintaining the node degree distribution.\n",
       "---------------------\n",
       "Response 7: Expected Number of Edges Between Nodes | This text discusses the calculation of the expected number of edges between two nodes in a randomly rewired network. It explains the use of indicator variables to determine if a stub of one node connects to a stub of the other node. The indicator variables are set to 1 if a connection is made and 0 if not. The text provides a mathematical approach to calculating the expected number of full edges between the nodes.\n",
       "---------------------\n",
       "Response 8: Approximations for Expected Number of Edges in Random Networks | This text discusses the approximations made for the expected number of edges between two nodes in random networks with a large number of edges. It explains that when the number of edges is large, the subtraction of 1 in the denominator is dropped, and the approximate expression k_v * k_w / 2m is used instead.\n",
       "---------------------\n",
       "Response 9: Modularity and Community Detection in Networks | This text explains the concept of modularity in network analysis and its application in community detection. It discusses the approximation of the expected number of edges between nodes in a large random network and how it can be used to calculate the probability of an edge existing between two nodes. The text also mentions the equation for modularity and its use in partitioning a network into communities. It concludes with an example of multiple community detection and an alternative formulation of modularity.\n",
       "---------------------\n",
       "Response 10: Matrix Formulation of Modularity for Community Detection | This text explains the matrix formulation of modularity, which is a measure used in community detection algorithms. The modularity matrix is defined using the adjacency matrix of a network and represents the optimal community partitions. The formula for modularity involves the sum of elements in the modularity matrix and the transpose of the matrix. The modularity of an undivided network is always zero.\n",
       "---------------------\n",
       "Response 11: Limitations of Modularity Maximization for Community Detection | Modularity maximization, a method commonly used for community detection in networks, has several limitations. Firstly, it is prone to overfitting and often finds high-scoring communities in its own null model, which are not statistically significant. Secondly, it suffers from a resolution limit, where it fails to detect small communities in large networks. This is because the null model assumes that each node can be connected to any other node, which is unrealistic for large networks. To address these limitations, multiresolution methods have been proposed, but they also have limitations when communities are highly heterogeneous in size.\n",
       "---------------------\n",
       "Response 12: Multiresolution Methods for Community Detection in Networks | This text discusses two main approaches for solving the resolution limit in community detection within the modularity context. One approach involves adding a resistance parameter to each node, either increasing or decreasing their aversion to form communities. The other approach involves adding a parameter that controls the relative importance between internal links of the communities and the null model. These multiresolution methods allow for the recovery of the entire mesoscale of a network, from a macro-scale where all nodes belong to the same community, to a micro-scale where each node forms its own community. However, these methods have limitations when communities vary greatly in size. The text also mentions several software tools available for computing clusterings with good modularity.\n",
       "---------------------\n",
       "Response 13: The Importance of Community Structure in Networks | Community structure refers to the occurrence of groups of nodes in a network that are more densely connected internally than with the rest of the network. This structure is common in various types of networks, including social networks, biological networks, and information networks. Identifying community structure is important for understanding network function and topology, as communities often correspond to functional units of the system. Additionally, communities can affect processes such as rumor spreading and epidemic spreading on a network. Various algorithms have been developed to find communities within networks, despite the computational challenges posed by the unknown number and unequal size/density of communities.\n",
       "---------------------\n",
       "Response 14: Algorithms for Finding Communities in Networks | This text discusses various algorithms for finding communities within networks. The minimum-cut method divides the network into parts by minimizing the number of edges between groups. Hierarchical clustering groups similar nodes into communities based on a similarity measure. The Girvan-Newman algorithm identifies edges between communities and removes them. Modularity maximization searches for divisions of the network with high modularity, with the Louvain method being a popular approach. The RenEEL scheme is currently the best modularity maximizing algorithm. However, modularity optimization has limitations in detecting small clusters and has a degeneracy of partitions with high modularity.\n",
       "---------------------\n",
       "Response 15: Methods for Community Detection in Networks | This text discusses two approaches for community detection in networks: statistical inference and clique-based methods. Statistical inference involves fitting a generative model to the network data, allowing for the identification of community structures and addressing issues of statistical significance. Clique-based methods, on the other hand, focus on detecting cliques in the graph and analyzing their overlap to define communities. These methods can be implemented using algorithms such as the Bron-Kerbosch algorithm or by creating clique graphs. Both approaches offer different ways to identify communities in networks and can be used to predict missing or spurious links.\n",
       "---------------------\n",
       "Response 16: Testing Methods for Community Detection Algorithms | This text discusses the evaluation of algorithms for detecting community structures in networks. It highlights the importance of analyzing networks of known structure to determine the effectiveness of different algorithms. The \"four groups\" test is commonly used, where a network is divided into four equally-sized groups and the probabilities of connection within and between groups are varied. Other benchmarks, such as the LFR benchmark, allow for varying group sizes and nontrivial degree distributions. These benchmarks help assess the performance of algorithms by comparing the obtained solution with the original community structure using measures like normalized mutual information or variation of information.\n",
       "---------------------\n",
       "Response 17: Detectability of Communities in Networks | This text discusses the concept of detectability in community detection problems in networks. It explains that there is a phase transition in detectability, where communities become undetectable as the density of connections within and between communities becomes more equal or smaller. While the communities still exist, it becomes impossible to label nodes or distinguish the graph from a null model without community structure. This transition is independent of the algorithm used for community detection, suggesting a fundamental limit on our ability to detect communities in networks. The text also mentions the conditions under which communities become undetectable in a stochastic block model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f'{response_acc}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Response 1: The speaker begins by addressing various individuals, including members of Congress, the Cabinet, military leaders, and the Supreme Court. They congratulate the new Speaker of the House, Kevin McCarthy, and acknowledge other political leaders. The speaker then highlights the progress and resilience of America, stating that the country has emerged stronger from every crisis. They mention the economic growth, job creation, and the handling of the COVID-19 pandemic. The speaker emphasizes the importance of unity and bipartisanship, citing examples of Democrats and Republicans working together on various issues. They express their vision to restore the soul of the nation, rebuild the middle class, and create an economy that benefits everyone. The speaker highlights the achievements in job creation, low unemployment rates, and the revitalization of the manufacturing sector. They conclude by emphasizing the importance of exporting American products and creating American jobs.\n",
       "---------------------\n",
       "Response 2: The context discusses various topics such as the importance of a strong middle class, job creation, low unemployment rates, inflation, small businesses, the semiconductor industry, and infrastructure. It highlights the need to prioritize American manufacturing, particularly in the semiconductor sector, to avoid supply chain disruptions. The passage also mentions the bipartisan CHIPS and Science Act, which aims to strengthen the domestic supply chain for semiconductors. Additionally, it emphasizes the significance of investing in infrastructure to rebuild highways, bridges, railroads, tunnels, ports, airports, and improve access to clean water and high-speed internet across America.\n",
       "---------------------\n",
       "Response 3: The speaker discusses various infrastructure projects that have been funded, including the rebuilding of highways, bridges, railroads, tunnels, ports, and airports across America. They express gratitude to Republican friends who voted for a law and assure those who voted against it that their projects will still be funded. The speaker also mentions specific projects, such as the Brent Spence bridge, and emphasizes the importance of unity and pride in the country. They highlight efforts to replace lead pipes, provide access to affordable high-speed internet, and enforce the Buy American policy for federal infrastructure projects. The speaker acknowledges the struggles faced by many Americans and promises to invest in places and people that have been forgotten. They mention the Inflation Reduction Act, which aims to lower healthcare costs, particularly for prescription drugs like insulin. The law caps the cost of insulin at $35 a month for seniors on Medicare and proposes extending this cap to all Americans who need it. Additionally, the law sets a maximum out-of-pocket drug cost for seniors on Medicare and requires drug companies to pay Medicare back if prices rise faster than inflation.\n",
       "---------------------\n",
       "Response 4: The speaker discusses the issue of high insulin costs and proposes capping the cost at $35 a month for all Americans who need it. They also mention capping out-of-pocket drug costs for seniors on Medicare and giving Medicare the power to negotiate drug prices. The speaker emphasizes the importance of bringing down prescription drug costs to save seniors money and reduce the federal deficit. They express their commitment to veto any attempts to raise the cost of prescription drugs. The speaker also highlights the achievements of the Affordable Care Act in increasing health insurance coverage and saving people money on premiums. They call for making these savings permanent and expanding coverage to those not covered by Medicaid. The speaker addresses the climate crisis and the need for significant investments to tackle it, including building electric vehicle charging stations and promoting clean energy. They stress the urgency of confronting the climate crisis for the sake of future generations. The speaker advocates for making the wealthiest and biggest corporations pay their fair share of taxes and criticizes the current tax system for allowing billion-dollar companies to pay zero federal income taxes. They highlight the implementation of a minimum tax for billion-dollar companies and emphasize that individuals earning less than $400,000 a year will not see an increase in taxes. The speaker also discusses the need to address the actions of Big Oil, such as quadrupling the tax on corporate stock buybacks and closing loopholes that allow the wealthy to avoid paying taxes.\n",
       "---------------------\n",
       "Response 5: The speaker in the given context is proposing various measures to address economic inequality and ensure that the wealthy and corporations pay their fair share of taxes. They advocate for a billionaire minimum tax, higher taxes on corporate stock buybacks, closing tax loopholes for the wealthy, and cracking down on tax cheats. The speaker also emphasizes the importance of protecting Social Security and Medicare benefits for seniors and opposes any cuts to these programs. They express a commitment to reducing the deficit, extending the Medicare Trust Fund, and not raising taxes on those making under $400,000 a year. Additionally, the speaker highlights efforts to protect consumers from exploitation, promote competition, and eliminate hidden fees in various industries.\n",
       "---------------------\n",
       "Response 6: The speaker discusses various measures aimed at protecting consumers and workers from unfair practices. They mention cracking down on nursing homes that commit fraud, allowing over-the-counter hearing aids, strengthening antitrust enforcement, and addressing hidden fees charged by businesses. The speaker also emphasizes the importance of restoring the dignity of work by banning non-compete agreements, supporting workers' rights to organize, and ensuring a living wage. Additionally, they highlight the need for affordable housing, home care services for seniors and people with disabilities, and accessible education, including preschool for 3- and 4-year-olds.\n",
       "---------------------\n",
       "Response 7: The speaker discusses the importance of restoring the dignity of work and making education affordable. They highlight the need for access to preschool, raising teacher salaries, reducing student debt, and providing career opportunities for students. The speaker also acknowledges the progress made in fighting the COVID-19 pandemic but emphasizes the need to remain vigilant. They address the issue of fraud related to relief money and express the intention to prosecute those responsible. The speaker also mentions the increase in violent crime and the importance of public safety and trust in law enforcement. They share the story of Tyre Nichols, a victim of police violence, and emphasize the need for equal protection under the law.\n",
       "---------------------\n",
       "Response 8: The speaker addresses various issues in their speech, including the need for safer neighborhoods, trust between law enforcement and communities, support for police officers, addressing mental health and substance abuse challenges, reducing violent crime and gun crime, holding law enforcement accountable, passing gun safety laws, banning assault weapons, addressing immigration, protecting reproductive rights, and ensuring equality for LGBTQ Americans. The speaker emphasizes the importance of taking action and working together to make positive changes.\n",
       "---------------------\n",
       "Response 9: The speaker discusses various topics in their speech, including pathways to citizenship for certain groups, protecting rights and freedoms, the need to codify Roe v. Wade, the importance of access to reproductive health care, the push for the Equality Act, and the need to stand against aggression and defend democracy. They also mention the United States' support for Ukraine, the country's approach to China, the strengthening of democracies, and the progress made in areas such as opioid addiction treatment and gun safety. The speaker shares a personal story about a father who lost his daughter to addiction.\n",
       "---------------------\n",
       "Response 10: The speaker highlights the progress made in passing laws related to opioid addiction, gun safety, and healthcare. They also mention the launch of ARPA-H to drive breakthroughs in fighting diseases like cancer and Alzheimer's. The speaker shares a personal story about a family affected by addiction and emphasizes the need to address the fentanyl crisis and improve mental health care, especially for children. They also discuss the importance of supporting veterans and their families and re-igniting efforts to combat cancer. The speaker concludes by emphasizing the importance of democracy in achieving these goals.\n",
       "---------------------\n",
       "Response 11: The speaker emphasizes the importance of hope and the resilience of the American people. They highlight the success of past efforts in fighting diseases like HIV/AIDS and express the belief that cancer can also be conquered. The speaker acknowledges the threats faced by democracy in recent years, including the attack on the Capitol, and calls for unity and the protection of democratic values. They stress the need to reject hate and extremism and emphasize that democracy should be a non-partisan issue. The speaker sees the current moment as a critical one that will shape the future of the nation and the world. They express optimism about the state of the union and the potential of the United States when its people come together. The speech concludes with a blessing for the audience and a wish for the protection of the military."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f'{response_acc}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Wikipedia articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "documents = loader.load_data(pages=['Louvain Method', 'Modularity (networks)', 'Community structure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='32ee0945-5f2a-41e2-bbdf-0dd1ddc4fab6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='e2d4fb35246a6751413cd39467fc1541f5fc31f2cffb06e9baa10056d23cee22', text='The Louvain method for community detection is a method to extract non-overlapping communities from large networks created by Blondel et al. from the University of Louvain (the source of this method\\'s name). The method is a greedy optimization method that appears to run in time \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        ⋅\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\cdot \\\\log n)}\\n   where \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is the number of nodes in the network.\\n\\n\\n== Modularity optimization ==\\nThe inspiration for this method of community detection is the optimization of modularity as the algorithm progresses. Modularity is a scale value between −0.5 (non-modular clustering) and 1 (fully modular clustering) that measures the relative density of edges inside communities with respect to edges outside communities. Optimizing this value theoretically results in the best possible grouping of the nodes of a given network. But because going through all possible iterations of the nodes into groups is impractical, heuristic algorithms are used.\\nIn the Louvain Method of community detection, first small communities are found by optimizing modularity locally on all nodes, then each small community is grouped into one node and the first step is repeated. The method is similar to the earlier method by Clauset, Newman and Moore that connects communities whose amalgamation produces the largest increase in modularity.\\n\\n\\n== Algorithm ==\\nThe value to be optimized is modularity, defined as a value in the range \\n  \\n    \\n      \\n        [\\n        −\\n        1\\n        \\n          /\\n        \\n        2\\n        ,\\n        1\\n        ]\\n      \\n    \\n    {\\\\displaystyle [-1/2,1]}\\n   that measures the density of links inside communities compared to links between communities. For a weighted graph, modularity is defined as:\\n\\n  \\n    \\n      \\n        Q\\n        =\\n        \\n          \\n            1\\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            i\\n            j\\n          \\n        \\n        \\n          \\n            [\\n          \\n        \\n        \\n          A\\n          \\n            i\\n            j\\n          \\n        \\n        −\\n        \\n          \\n            \\n              \\n                k\\n                \\n                  i\\n                \\n              \\n              \\n                k\\n                \\n                  j\\n                \\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          \\n            ]\\n          \\n        \\n        δ\\n        (\\n        \\n          c\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          c\\n          \\n            j\\n          \\n        \\n        )\\n        ,\\n      \\n    \\n    {\\\\displaystyle Q={\\\\frac {1}{2m}}\\\\sum \\\\limits _{ij}{\\\\bigg [}A_{ij}-{\\\\frac {k_{i}k_{j}}{2m}}{\\\\bigg ]}\\\\delta (c_{i},c_{j}),}\\n  \\nwhere\\n\\n  \\n    \\n      \\n        \\n          A\\n          \\n            i\\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle A_{ij}}\\n   represents the edge weight between nodes \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   and \\n  \\n    \\n      \\n        j\\n      \\n    \\n    {\\\\displaystyle j}\\n  ;\\n\\n  \\n    \\n      \\n        \\n          k\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{i}}\\n   and \\n  \\n    \\n      \\n        \\n          k\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{j}}\\n   are the sum of the weights of the edges attached to nodes \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   and \\n  \\n    \\n      \\n        j\\n      \\n    \\n    {\\\\displaystyle j}\\n  , respectively;\\n\\n  \\n    \\n      \\n        m\\n      \\n    \\n    {\\\\displaystyle m}\\n   is the sum of all of the edge weights in the graph;\\n\\n  \\n    \\n      \\n        \\n          c\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle c_{i}}\\n   and \\n  \\n    \\n      \\n        \\n          c\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle c_{j}}\\n   are the communities of the nodes; and\\n\\n  \\n    \\n      \\n        δ\\n      \\n    \\n    {\\\\displaystyle \\\\delta }\\n   is Kronecker delta function (\\n  \\n    \\n      \\n        δ\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle \\\\delta (x,y)=1}\\n   if \\n  \\n    \\n      \\n        x\\n        =\\n        y\\n      \\n    \\n    {\\\\displaystyle x=y}\\n  , \\n  \\n    \\n      \\n        0\\n      \\n    \\n    {\\\\displaystyle 0}\\n   otherwise).Based on the above equation, the modularity of a community \\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n   can be calculated as:\\n\\n  \\n    \\n      \\n        \\n          Q\\n          \\n            c\\n          \\n        \\n        =\\n        \\n          \\n            \\n              Σ\\n              \\n                i\\n                n\\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        −\\n        (\\n        \\n          \\n            \\n              Σ\\n              \\n                t\\n                o\\n                t\\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          )\\n          \\n            2\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle Q_{c}={\\\\frac {\\\\Sigma _{in}}{2m}}-({\\\\frac {\\\\Sigma _{tot}}{2m}})^{2},}\\n  \\nwhere\\n\\n  \\n    \\n      \\n        \\n          Σ\\n          \\n            i\\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Sigma _{in}}\\n   is the sum of edge weights between nodes within the community \\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n   (each edge is considered twice); and\\n\\n  \\n    \\n      \\n        \\n          Σ\\n          \\n            t\\n            o\\n            t\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Sigma _{tot}}\\n   is the sum of all edge weights for nodes within the community (including edges which link to other communities).In order to maximize modularity efficiently, the Louvain Method has two phases that are repeated iteratively.\\nFirst, each node in the network is assigned to its own community. Then for each node \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  , the change in modularity is calculated for removing \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   from its own community and moving it into the community of each neighbor \\n  \\n    \\n      \\n        j\\n      \\n    \\n    {\\\\displaystyle j}\\n   of \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  . This value is easily calculated by two steps: (1) removing \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   from its original community, and (2) inserting \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   to the community of \\n  \\n    \\n      \\n        j\\n      \\n    \\n    {\\\\displaystyle j}\\n  . The two equations are quite similar, and the equation for step (2) is:\\n  \\n    \\n      \\n        Δ\\n        Q\\n        =\\n        \\n          \\n            [\\n          \\n        \\n        \\n          \\n            \\n              \\n                Σ\\n                \\n                  i\\n                  n\\n                \\n              \\n              +\\n              2\\n              \\n                k\\n                \\n                  i\\n                  ,\\n                  i\\n                  n\\n                \\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        −\\n        \\n          \\n            (\\n          \\n        \\n        \\n          \\n            \\n              \\n                Σ\\n                \\n                  t\\n                  o\\n                  t\\n                \\n              \\n              +\\n              \\n                k\\n                \\n                  i\\n                \\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          \\n            \\n              )\\n            \\n          \\n          \\n            2\\n          \\n        \\n        \\n          \\n            ]\\n          \\n        \\n        −\\n        \\n          \\n            [\\n          \\n        \\n        \\n          \\n            \\n              Σ\\n              \\n                i\\n                n\\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        −\\n        \\n          \\n            (\\n          \\n        \\n        \\n          \\n            \\n              Σ\\n              \\n                t\\n                o\\n                t\\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          \\n            \\n              )\\n            \\n          \\n          \\n            2\\n          \\n        \\n        −\\n        \\n          \\n            (\\n          \\n        \\n        \\n          \\n            \\n              k\\n              \\n                i\\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          \\n            \\n              )\\n            \\n          \\n          \\n            2\\n          \\n        \\n        \\n          \\n            ]\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Delta Q={\\\\bigg [}{\\\\frac {\\\\Sigma _{in}+2k_{i,in}}{2m}}-{\\\\bigg (}{\\\\frac {\\\\Sigma _{tot}+k_{i}}{2m}}{\\\\bigg )}^{2}{\\\\bigg ]}-{\\\\bigg [}{\\\\frac {\\\\Sigma _{in}}{2m}}-{\\\\bigg (}{\\\\frac {\\\\Sigma _{tot}}{2m}}{\\\\bigg )}^{2}-{\\\\bigg (}{\\\\frac {k_{i}}{2m}}{\\\\bigg )}^{2}{\\\\bigg ]}}\\n  \\nWhere \\n  \\n    \\n      \\n        \\n          Σ\\n          \\n            i\\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Sigma _{in}}\\n   is sum of all the weights of the links inside the community \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   is moving into, \\n  \\n    \\n      \\n        \\n          Σ\\n          \\n            t\\n            o\\n            t\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Sigma _{tot}}\\n   is the sum of all the weights of the links to nodes in the community \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   is moving into, \\n  \\n    \\n      \\n        \\n          k\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{i}}\\n   is the weighted degree of \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  , \\n  \\n    \\n      \\n        \\n          k\\n          \\n            i\\n            ,\\n            i\\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{i,in}}\\n   is the sum of the weights of the links between \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   and other nodes in the community that \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   is moving into, and \\n  \\n    \\n      \\n        m\\n      \\n    \\n    {\\\\displaystyle m}\\n   is the sum of the weights of all links in the network. Then, once this value is calculated for all communities \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   is connected to, \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   is placed into the community that resulted in the greatest modularity increase. If no increase is possible, \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   remains in its original community. This process is applied repeatedly and sequentially to all nodes until no modularity increase can occur. Once this local maximum of modularity is hit, the first phase has ended.\\nIn the second phase of the algorithm, it groups all of the nodes in the same community and builds a new network where nodes are the communities from the previous phase. Any links between nodes of the same community are now represented by self-loops on the new community node and links from multiple nodes in the same community to a node in a different community are represented by weighted edges between communities. Once the new network is created, the second phase has ended and the first phase can be re-applied to the new network.\\n\\n\\n== Previous uses ==\\nTwitter social Network (2.4 Million nodes, 38 million links) by Josep Pujol, Vijay Erramilli, and Pablo Rodriguez: The authors explore the problem of partitioning Online Social Networks onto different machines.\\nMobile phone Network (4 Million nodes, 100 Million links) by Derek Greene, Donal Doyle, and Padraig Cunningham: Community-tracking strategies for identifying dynamic communities of different dynamic social networks.\\nDetecting species in network-based dynamical model.\\n\\n\\n== Disadvantages ==\\nIt is important to emphasize that Louvain produces only non-overlapping communities, which means that each node can belong to at most one community. This is highly unrealistic in many real-world applications. For example, in social networks, most people belong to multiple communities: their family, their friends, their co-workers, old school buddies, etc. In biological networks, most genes or proteins belong to more than one pathway or complex. Furthermore, Louvain has been shown to sometimes produce arbitrarily badly connected communities, and has been effectively superseded (at least in the non-overlapping case) by the Leiden algorithm.\\n\\n\\n== Comparison to other methods of non-overlapping community detection ==\\nWhen comparing modularity optimization methods, the two measures of importance are the speed and the resulting modularity value. A higher speed is better as it shows a method is more efficient than others and a higher modularity value is desirable as it points to having better-defined communities.\\nThe compared methods are, the algorithm of Clauset, Newman, and Moore, Pons and Latapy, and Wakita and Tsurumi.\\n-/- in the table refers to a method that took over 24hrs to run. This table (from) shows that the Louvain method outperforms many similar modularity optimization methods in both the modularity and the time categories.\\n\\n\\n== See also ==\\nModularity (networks)\\nCommunity structure\\nNetwork science\\nK-means clustering\\n\\n\\n== References ==\\n\\n\"The Louvain method for community detection in large networks\" Vincent Blondel http://perso.uclouvain.be/vincent.blondel/research/louvain.html', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6859ef38-2941-4a08-a1f0-9a3e7a430898', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='735a591b325be2f57da7c8756161c4cb8071593f110f388a3f18fba441884387', text=\"Modularity is a measure of the structure of networks or graphs which measures the strength of division of a network into modules (also called groups, clusters or communities). Networks with high modularity have dense connections between the nodes within modules but sparse connections between nodes in different modules. Modularity is often used in optimization methods for detecting community structure in networks.  Biological networks, including animal brains, exhibit a high degree of modularity. However, modularity maximization is not statistically consistent, and finds communities in its own null model, i.e. fully random graphs, and therefore it cannot be used to find statistically significant community structures in empirical networks. Furthermore, it has been shown that modularity suffers a resolution limit and, therefore, it is unable to detect small communities.\\n\\n\\n== Motivation ==\\nMany scientifically important problems can be represented and empirically studied using networks. For example, biological and social patterns, the World Wide Web, metabolic networks, food webs, neural networks and pathological networks are real world problems that can be mathematically represented and topologically studied to reveal some unexpected structural features. Most of these networks possess a certain community structure that has substantial importance in building an understanding regarding the dynamics of the network. For instance, a closely connected social community will imply a faster rate of transmission of information or rumor among them than a loosely connected community. Thus, if a network is represented by a number of individual nodes connected by links which signify a certain degree of interaction between the nodes, communities are defined as groups of densely interconnected nodes that are only sparsely connected with the rest of the network. Hence, it may be imperative to identify the communities in networks since the communities may have quite different properties such as node degree, clustering coefficient, betweenness, centrality, etc., from that of the average network. Modularity is one such measure, which when maximized, leads to the appearance of communities in a given network.\\n\\n\\n== Definition ==\\nModularity is the fraction of the edges that fall within the given groups minus the expected fraction if edges were distributed at random. The value of the modularity for unweighted and undirected graphs lies in the range \\n  \\n    \\n      \\n        [\\n        −\\n        1\\n        \\n          /\\n        \\n        2\\n        ,\\n        1\\n        ]\\n      \\n    \\n    {\\\\displaystyle [-1/2,1]}\\n  .  It is positive if the number of edges within groups exceeds the number expected on the basis of chance. For a given division of the network's vertices into some modules, modularity reflects the concentration of edges within modules compared with random distribution of links between all nodes regardless of modules.\\nThere are different methods for calculating modularity. In the most common version of the concept, the randomization of the edges is done so as to preserve the degree of each vertex. Consider a graph with \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   nodes and \\n  \\n    \\n      \\n        m\\n      \\n    \\n    {\\\\displaystyle m}\\n   links (edges) such that the graph can be partitioned into two communities using a membership variable \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n  . If a node \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   belongs to community 1, \\n  \\n    \\n      \\n        \\n          s\\n          \\n            v\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle s_{v}=1}\\n  , or if \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   belongs to community 2, \\n  \\n    \\n      \\n        \\n          s\\n          \\n            v\\n          \\n        \\n        =\\n        −\\n        1\\n      \\n    \\n    {\\\\displaystyle s_{v}=-1}\\n  . Let the adjacency matrix for the network be represented by \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n  , where \\n  \\n    \\n      \\n        \\n          A\\n          \\n            v\\n            w\\n          \\n        \\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle A_{vw}=0}\\n   means there's no edge (no interaction) between nodes \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   and \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   and \\n  \\n    \\n      \\n        \\n          A\\n          \\n            v\\n            w\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle A_{vw}=1}\\n   means there is an edge between the two. Also for simplicity we consider an undirected network. Thus \\n  \\n    \\n      \\n        \\n          A\\n          \\n            v\\n            w\\n          \\n        \\n        =\\n        \\n          A\\n          \\n            w\\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle A_{vw}=A_{wv}}\\n  . (It is important to note that multiple edges may exist between two nodes, but here we assess the simplest case).\\nModularity \\n  \\n    \\n      \\n        Q\\n      \\n    \\n    {\\\\displaystyle Q}\\n   is then defined as the fraction of edges that fall within group 1 or 2, minus the expected number of edges within groups 1 and 2 for a random graph with the same node degree distribution as the given network.\\nThe expected number of edges shall be computed using the concept of a configuration model. The configuration model is a randomized realization of a particular network. Given a network with \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   nodes, where each node \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   has a node degree \\n  \\n    \\n      \\n        \\n          k\\n          \\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{v}}\\n  , the configuration model cuts each edge into two halves,  and then each half edge, called a stub, is rewired randomly with any other stub in the network, even allowing self-loops (which occur when a stub is rewired to another stub from the same node) and multiple-edges between the same two nodes. Thus, even though the node degree distribution of the graph remains intact, the configuration model results in a completely random network.\\n\\n\\n== Expected Number of Edges Between Nodes ==\\nNow consider two nodes \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   and \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n  , with node degrees \\n  \\n    \\n      \\n        \\n          k\\n          \\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{v}}\\n   and \\n  \\n    \\n      \\n        \\n          k\\n          \\n            w\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{w}}\\n   respectively, from a randomly rewired network as described above. We calculate the expected number of full edges between these nodes.\\nLet us consider each of the \\n  \\n    \\n      \\n        \\n          k\\n          \\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{v}}\\n   stubs of node \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   and create associated indicator variables \\n  \\n    \\n      \\n        \\n          I\\n          \\n            i\\n          \\n          \\n            (\\n            v\\n            ,\\n            w\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle I_{i}^{(v,w)}}\\n   for them, \\n  \\n    \\n      \\n        i\\n        =\\n        1\\n        ,\\n        …\\n        ,\\n        \\n          k\\n          \\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle i=1,\\\\ldots ,k_{v}}\\n  , with \\n  \\n    \\n      \\n        \\n          I\\n          \\n            i\\n          \\n          \\n            (\\n            v\\n            ,\\n            w\\n            )\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle I_{i}^{(v,w)}=1}\\n   if the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  -th stub happens to connect to one of the \\n  \\n    \\n      \\n        \\n          k\\n          \\n            w\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{w}}\\n   stubs of node \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   in this particular random graph. If it does not, then \\n  \\n    \\n      \\n        \\n          I\\n          \\n            i\\n          \\n          \\n            (\\n            v\\n            ,\\n            w\\n            )\\n          \\n        \\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle I_{i}^{(v,w)}=0}\\n  . Since the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  -th stub of node \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   can connect to any of the \\n  \\n    \\n      \\n        2\\n        m\\n        −\\n        1\\n      \\n    \\n    {\\\\displaystyle 2m-1}\\n   remaining stubs with equal probability, and since there are \\n  \\n    \\n      \\n        \\n          k\\n          \\n            w\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k_{w}}\\n   stubs it can connect to associated with node \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n  , evidently\\n\\n  \\n    \\n      \\n        p\\n        (\\n        \\n          I\\n          \\n            i\\n          \\n          \\n            (\\n            v\\n            ,\\n            w\\n            )\\n          \\n        \\n        =\\n        1\\n        )\\n        =\\n        E\\n        [\\n        \\n          I\\n          \\n            i\\n          \\n          \\n            (\\n            v\\n            ,\\n            w\\n            )\\n          \\n        \\n        ]\\n        =\\n        \\n          \\n            \\n              k\\n              \\n                w\\n              \\n            \\n            \\n              2\\n              m\\n              −\\n              1\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p(I_{i}^{(v,w)}=1)=E[I_{i}^{(v,w)}]={\\\\frac {k_{w}}{2m-1}}}\\n  The total number of full edges \\n  \\n    \\n      \\n        \\n          J\\n          \\n            v\\n            w\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle J_{vw}}\\n   between \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   and \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   is just \\n  \\n    \\n      \\n        \\n          J\\n          \\n            v\\n            w\\n          \\n        \\n        =\\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            \\n              k\\n              \\n                v\\n              \\n            \\n          \\n        \\n        \\n          I\\n          \\n            i\\n          \\n          \\n            (\\n            v\\n            ,\\n            w\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle J_{vw}=\\\\sum _{i=1}^{k_{v}}I_{i}^{(v,w)}}\\n  , so the expected value of this quantity is\\n\\n  \\n    \\n      \\n        E\\n        [\\n        \\n          J\\n          \\n            v\\n            w\\n          \\n        \\n        ]\\n        =\\n        E\\n        \\n          [\\n          \\n            \\n              ∑\\n              \\n                i\\n                =\\n                1\\n              \\n              \\n                \\n                  k\\n                  \\n                    v\\n                  \\n                \\n              \\n            \\n            \\n              I\\n              \\n                i\\n              \\n              \\n                (\\n                v\\n                ,\\n                w\\n                )\\n              \\n            \\n          \\n          ]\\n        \\n        =\\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            \\n              k\\n              \\n                v\\n              \\n            \\n          \\n        \\n        E\\n        [\\n        \\n          I\\n          \\n            i\\n          \\n          \\n            (\\n            v\\n            ,\\n            w\\n            )\\n          \\n        \\n        ]\\n        =\\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            \\n              k\\n              \\n                v\\n              \\n            \\n          \\n        \\n        \\n          \\n            \\n              k\\n              \\n                w\\n              \\n            \\n            \\n              2\\n              m\\n              −\\n              1\\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                k\\n                \\n                  v\\n                \\n              \\n              \\n                k\\n                \\n                  w\\n                \\n              \\n            \\n            \\n              2\\n              m\\n              −\\n              1\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle E[J_{vw}]=E\\\\left[\\\\sum _{i=1}^{k_{v}}I_{i}^{(v,w)}\\\\right]=\\\\sum _{i=1}^{k_{v}}E[I_{i}^{(v,w)}]=\\\\sum _{i=1}^{k_{v}}{\\\\frac {k_{w}}{2m-1}}={\\\\frac {k_{v}k_{w}}{2m-1}}}\\n  Many texts then make the following approximations, for random networks with a large number of edges. When \\n  \\n    \\n      \\n        m\\n      \\n    \\n    {\\\\displaystyle m}\\n   is large, they drop the subtraction of \\n  \\n    \\n      \\n        1\\n      \\n    \\n    {\\\\displaystyle 1}\\n   in the denominator above and simply use the approximate expression \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                k\\n                \\n                  v\\n                \\n              \\n              \\n                k\\n                \\n                  w\\n                \\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {k_{v}k_{w}}{2m}}}\\n   for the expected number of edges between two nodes. Additionally, in a large random network, the number of self-loops and multi-edges is vanishingly small. Ignoring self-loops and multi-edges allows one to assume that there is at most one edge between any two nodes. In that case, \\n  \\n    \\n      \\n        \\n          J\\n          \\n            v\\n            w\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle J_{vw}}\\n   becomes a binary indicator variable, so its expected value is also the probability that it equals \\n  \\n    \\n      \\n        1\\n      \\n    \\n    {\\\\displaystyle 1}\\n  , which means one can approximate the probability of an edge existing between nodes \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   and \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   as \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                k\\n                \\n                  v\\n                \\n              \\n              \\n                k\\n                \\n                  w\\n                \\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {k_{v}k_{w}}{2m}}}\\n  .\\n\\n\\n== Modularity ==\\nHence, the difference between the actual number of edges between node \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   and \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   and the expected number of edges between them is\\n\\n  \\n    \\n      \\n        \\n          A\\n          \\n            v\\n            w\\n          \\n        \\n        −\\n        \\n          \\n            \\n              \\n                k\\n                \\n                  v\\n                \\n              \\n              \\n                k\\n                \\n                  w\\n                \\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle A_{vw}-{\\\\frac {k_{v}k_{w}}{2m}}}\\n  \\nSumming over all node pairs gives the equation for modularity, \\n  \\n    \\n      \\n        Q\\n      \\n    \\n    {\\\\displaystyle Q}\\n  .\\n\\nIt is important to note that Eq. 3 holds good for partitioning into two communities only. Hierarchical partitioning (i.e. partitioning into two communities, then the two sub-communities further partitioned into two smaller sub communities only to maximize Q) is a possible approach to identify multiple communities in a network. Additionally, (3) can be generalized for partitioning a network into c communities.\\n\\nwhere eij is the fraction of edges with one end vertices in community i and the other in community j:\\n\\n  \\n    \\n      \\n        \\n          e\\n          \\n            i\\n            j\\n          \\n        \\n        =\\n        \\n          ∑\\n          \\n            v\\n            w\\n          \\n        \\n        \\n          \\n            \\n              A\\n              \\n                v\\n                w\\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          1\\n          \\n            v\\n            ∈\\n            \\n              c\\n              \\n                i\\n              \\n            \\n          \\n        \\n        \\n          1\\n          \\n            w\\n            ∈\\n            \\n              c\\n              \\n                j\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle e_{ij}=\\\\sum _{vw}{\\\\frac {A_{vw}}{2m}}1_{v\\\\in c_{i}}1_{w\\\\in c_{j}}}\\n  and ai is the fraction of ends of edges that are attached to vertices in community i:\\n\\n  \\n    \\n      \\n        \\n          a\\n          \\n            i\\n          \\n        \\n        =\\n        \\n          \\n            \\n              k\\n              \\n                i\\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        =\\n        \\n          ∑\\n          \\n            j\\n          \\n        \\n        \\n          e\\n          \\n            i\\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{i}={\\\\frac {k_{i}}{2m}}=\\\\sum _{j}e_{ij}}\\n  \\n\\n\\n== Example of multiple community detection ==\\nWe consider an undirected network with 10 nodes and 12 edges and the following adjacency matrix.\\n\\nThe communities in the graph are represented by the red, green and blue node clusters in Fig 1. The optimal community partitions are depicted in Fig 2.\\n\\n\\n== Matrix formulation ==\\nAn alternative formulation of the modularity, useful particularly in spectral optimization algorithms, is as follows.  Define \\n  \\n    \\n      \\n        \\n          S\\n          \\n            v\\n            r\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{vr}}\\n   to be \\n  \\n    \\n      \\n        1\\n      \\n    \\n    {\\\\displaystyle 1}\\n   if vertex \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   belongs to group \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n   and \\n  \\n    \\n      \\n        0\\n      \\n    \\n    {\\\\displaystyle 0}\\n   otherwise.  Then\\n\\n  \\n    \\n      \\n        δ\\n        (\\n        \\n          c\\n          \\n            v\\n          \\n        \\n        ,\\n        \\n          c\\n          \\n            w\\n          \\n        \\n        )\\n        =\\n        \\n          ∑\\n          \\n            r\\n          \\n        \\n        \\n          S\\n          \\n            v\\n            r\\n          \\n        \\n        \\n          S\\n          \\n            w\\n            r\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\delta (c_{v},c_{w})=\\\\sum _{r}S_{vr}S_{wr}}\\n  and hence\\n\\n  \\n    \\n      \\n        Q\\n        =\\n        \\n          \\n            1\\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            v\\n            w\\n          \\n        \\n        \\n          ∑\\n          \\n            r\\n          \\n        \\n        \\n          [\\n          \\n            \\n              A\\n              \\n                v\\n                w\\n              \\n            \\n            −\\n            \\n              \\n                \\n                  \\n                    k\\n                    \\n                      v\\n                    \\n                  \\n                  \\n                    k\\n                    \\n                      w\\n                    \\n                  \\n                \\n                \\n                  2\\n                  m\\n                \\n              \\n            \\n          \\n          ]\\n        \\n        \\n          S\\n          \\n            v\\n            r\\n          \\n        \\n        \\n          S\\n          \\n            w\\n            r\\n          \\n        \\n        =\\n        \\n          \\n            1\\n            \\n              2\\n              m\\n            \\n          \\n        \\n        \\n          T\\n          r\\n        \\n        (\\n        \\n          \\n            S\\n          \\n          \\n            \\n              T\\n            \\n          \\n        \\n        \\n          B\\n          S\\n        \\n        )\\n        ,\\n      \\n    \\n    {\\\\displaystyle Q={\\\\frac {1}{2m}}\\\\sum _{vw}\\\\sum _{r}\\\\left[A_{vw}-{\\\\frac {k_{v}k_{w}}{2m}}\\\\right]S_{vr}S_{wr}={\\\\frac {1}{2m}}\\\\mathrm {Tr} (\\\\mathbf {S} ^{\\\\mathrm {T} }\\\\mathbf {BS} ),}\\n  where \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   is the (non-square) matrix having elements \\n  \\n    \\n      \\n        \\n          S\\n          \\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{v}}\\n   and \\n  \\n    \\n      \\n        B\\n      \\n    \\n    {\\\\displaystyle B}\\n   is the so-called modularity matrix, which has elements\\n\\n  \\n    \\n      \\n        \\n          B\\n          \\n            v\\n            w\\n          \\n        \\n        =\\n        \\n          A\\n          \\n            v\\n            w\\n          \\n        \\n        −\\n        \\n          \\n            \\n              \\n                k\\n                \\n                  v\\n                \\n              \\n              \\n                k\\n                \\n                  w\\n                \\n              \\n            \\n            \\n              2\\n              m\\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle B_{vw}=A_{vw}-{\\\\frac {k_{v}k_{w}}{2m}}.}\\n  All rows and columns of the modularity matrix sum to zero, which means that the modularity of an undivided network is also always \\n  \\n    \\n      \\n        0\\n      \\n    \\n    {\\\\displaystyle 0}\\n  .\\nFor networks divided into just two communities, one can alternatively define \\n  \\n    \\n      \\n        \\n          s\\n          \\n            v\\n          \\n        \\n        =\\n        ±\\n        1\\n      \\n    \\n    {\\\\displaystyle s_{v}=\\\\pm 1}\\n   to indicate the community to which node \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   belongs, which then leads to\\n\\n  \\n    \\n      \\n        Q\\n        =\\n        \\n          \\n            1\\n            \\n              4\\n              m\\n            \\n          \\n        \\n        \\n          ∑\\n          \\n            v\\n            w\\n          \\n        \\n        \\n          B\\n          \\n            v\\n            w\\n          \\n        \\n        \\n          s\\n          \\n            v\\n          \\n        \\n        \\n          s\\n          \\n            w\\n          \\n        \\n        =\\n        \\n          \\n            1\\n            \\n              4\\n              m\\n            \\n          \\n        \\n        \\n          \\n            s\\n          \\n          \\n            \\n              T\\n            \\n          \\n        \\n        \\n          B\\n          s\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle Q={1 \\\\over 4m}\\\\sum _{vw}B_{vw}s_{v}s_{w}={1 \\\\over 4m}\\\\mathbf {s} ^{\\\\mathrm {T} }\\\\mathbf {Bs} ,}\\n  where \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n   is the column vector with elements \\n  \\n    \\n      \\n        \\n          s\\n          \\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle s_{v}}\\n  .This function has the same form as the Hamiltonian of an Ising spin glass, a connection that has been exploited to create simple computer algorithms, for instance using simulated annealing, to maximize the modularity.  The general form of the modularity for arbitrary numbers of communities is equivalent to a Potts spin glass and similar algorithms can be developed for this case also.\\n\\n\\n== Overfitting ==\\nAlthough the method of modularity maximization is motivated by computing a deviation from a null model, this deviation is not computed in a statistically consistent manner. Because of this, the method notoriously finds high-scoring communities in its own null model\\n\\n(the configuration model), which by definition cannot be statistically significant. Because of this, the method cannot be used to reliably obtain statistically significant community structure in empirical networks.\\n\\n\\n== Resolution limit ==\\nModularity compares the number of edges inside a cluster with the expected number of edges that\\none would find in the cluster if the network were a random network with the same number of nodes and where\\neach node keeps its degree, but edges are otherwise randomly attached. This random null model implicitly assumes that each node can get attached to any other node of the network. This assumption is however unreasonable if the network is very large, as the horizon of a node includes a small part of the network, ignoring most of it.\\nMoreover, this implies that the expected number of edges between two groups of nodes decreases if the size of the network increases. So, if a network is large enough, the expected number of edges between two groups of nodes in modularity's null model may be smaller than one. If this happens, a single edge between the two clusters would be interpreted by modularity as a sign of a strong correlation between the two clusters, and optimizing modularity would lead to the merging of the two clusters, independently of the clusters' features. So, even weakly interconnected complete graphs, which have the highest possible density of internal edges, and represent the best identifiable communities, would be merged by modularity optimization if the network were sufficiently large.\\nFor this reason, optimizing modularity in large networks would fail to resolve small communities, even when they are well defined. This bias\\nis inevitable for methods like modularity optimization, which rely on a global null model.\\n\\n\\n== Multiresolution methods ==\\nThere are two main approaches which try to solve the resolution limit within the modularity context: the addition of a resistance r to every node, in the form of a self-loop, which increases (r>0) or decreases (r<0) the aversion of nodes to form communities; or the addition of a parameter γ>0 in front of the null-case term in the definition of modularity, which controls the relative importance between internal links of the communities and the null model. Optimizing modularity for values of these parameters in their respective appropriate ranges, it is possible to recover the whole mesoscale of the network, from the macroscale in which all nodes belong to the same community, to the microscale in which every node forms its own community, hence the name multiresolution methods. However, it has been shown that these methods have limitations when communities are very heterogeneous in size.\\n\\n\\n== Software Tools ==\\nThere are a couple of software tools available that are able to compute clusterings in graphs with good modularity.\\nOriginal implementation of the multi-level Louvain method.The Leiden algorithm which additionally avoids unconnected communities.The Vienna Graph Clustering (VieClus) algorithm, a parallel memetic algorithm.\\n\\n\\n== See also ==\\nComplex network\\nCommunity structure\\nNull model\\nPercolation theory\\n\\n\\n== References ==\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10ce2e27-9666-471f-8d66-5b4af242079c', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='f4605aac6375f764ce1d16f4e8bfc20dbe7891701bb380555f0a8e33de75b1c1', text='In the study of complex networks, a network is said to have community structure if the nodes of the network can be easily grouped into (potentially overlapping) sets of nodes such that each set of nodes is densely connected internally. In the particular case of non-overlapping community finding, this implies that the network divides naturally into groups of nodes with dense connections internally and sparser connections between groups.  But overlapping communities are also allowed. The more general definition is based on the principle that pairs of nodes are more likely to be connected if they are both members of the same community(ies), and less likely to be connected if they do not share communities. A related but different problem is community search, where the goal is to find a community that a certain vertex belongs to.\\n\\n\\n== Properties ==\\nIn the study of networks, such as computer and information networks, social networks and biological networks, a number of different characteristics have been found to occur commonly, including the small-world property, heavy-tailed degree distributions, and clustering, among others. Another common characteristic is community structure.\\nIn the context of networks, community structure refers to the occurrence of groups of nodes in a network that are more densely connected internally than with the rest of the network, as shown in the example image to the right.  This inhomogeneity of connections suggests that the network has certain natural divisions within it.\\nCommunities are often defined in terms of the partition of the set of vertices, that is each node is put into one and only one community, just as in the figure. This is a useful simplification and most community detection methods find this type of community structure. However, in some cases a better representation could be one where vertices are in more than one community.  This might happen in a social network where each vertex represents a person, and the communities represent the different groups of friends: one community for family, another community for co-workers, one for friends in the same sports club, and so on. The use of cliques for community detection discussed below is just one example of how such overlapping community structure can be found.\\nSome networks may not have any meaningful community structure.  Many basic network models, for example, such as the random graph and the Barabási–Albert model, do not display community structure.\\n\\n\\n== Importance ==\\nCommunity structures are quite common in real networks. Social networks include community groups (the origin of the term, in fact) based on common location, interests, occupation, etc.Finding an underlying community structure in a network, if it exists, is important for a number of reasons. Communities allow us to create a large scale map of a network since individual communities act like meta-nodes in the network which makes its study easier.Individual communities also shed light on the function of the system represented by the network since communities often correspond to functional units of the system. In metabolic networks, such functional groups correspond to cycles or pathways whereas in the protein interaction network, communities correspond to proteins with similar functionality inside a biological cell. Similarly, citation networks form communities by research topic. Being able to identify these sub-structures within a network can provide insight into how network function and topology affect each other. Such insight can be useful in improving some algorithms on graphs such as spectral clustering.Importantly, communities often have very different properties than the average properties of the networks. Thus, only concentrating on the average properties usually misses many important and interesting features inside the networks. For example, in a given social network, both gregarious and reticent groups might exists simultaneously.Existence of communities also generally affects various processes like rumour spreading or epidemic spreading happening on a network. Hence to properly understand such processes, it is important to detect communities and also to study how they affect the spreading processes in various settings.\\nFinally, an important application that community detection has found in network science is the prediction of missing links and the identification of false links in the network. During the measurement process, some links may not get observed for a number of reasons. Similarly, some links could falsely enter into the data because of the errors in the measurement. Both these cases are well handled by community detection algorithm since it allows one to assign the probability of existence of an edge between a given pair of nodes.\\n\\n\\n== Algorithms for finding communities ==\\nFinding communities within an arbitrary network can be a computationally difficult task.  The number of communities, if any, within the network is typically unknown and the communities are often of unequal size and/or density.  Despite these difficulties, however, several methods for community finding have been developed and employed with varying levels of success.\\n\\n\\n=== Minimum-cut method ===\\nOne of the oldest algorithms for dividing networks into parts is the minimum cut method (and variants such as ratio cut and normalized cut).  This method sees use, for example, in load balancing for parallel computing in order to minimize communication between processor nodes.\\nIn the minimum-cut method, the network is divided into a predetermined number of parts, usually of approximately the same size, chosen such that the number of edges between groups is minimized.  The method works well in many of the applications for which it was originally intended but is less than ideal for finding community structure in general networks since it will find communities regardless of whether they are implicit in the structure, and it will find only a fixed number of them.\\n\\n\\n=== Hierarchical clustering ===\\nAnother method for finding community structures in networks is hierarchical clustering.  In this method one defines a similarity measure quantifying some (usually topological) type of similarity between node pairs.  Commonly used measures include the cosine similarity, the Jaccard index, and the Hamming distance between rows of the adjacency matrix.  Then one groups similar nodes into communities according to this measure.  There are several common schemes for performing the grouping, the two simplest being single-linkage clustering, in which two groups are considered separate communities if and only if all pairs of nodes in different groups have similarity lower than a given threshold, and complete linkage clustering, in which all nodes within every group have similarity greater than a threshold. An important step is how to determine the threshold to stop the agglomerative clustering, indicating a near-to-optimal community structure. A common strategy consist to build one or several metrics monitoring global properties of the network, which peak at given step of the clustering. An interesting approach in this direction is the use of various similarity or dissimilarity measures, combined through convex sums,. Another approximation is the computation of a quantity monitoring the density of edges within clusters with respect to the density between clusters, such as the partition density, which has been proposed when the similarity metric is defined between edges (which permits the definition of overlapping communities), and extended when the similarity is defined between nodes, which allows to consider alternative definitions of communities such as guilds (i.e. groups of nodes sharing a similar number of links with respect to the same neighbours but not necessarily connected themselves). These methods can be extended to consider multidimensional networks, for instance when we are dealing with networks having nodes with different types of links.\\n\\n\\n=== Girvan–Newman algorithm ===\\nAnother commonly used algorithm for finding communities is the Girvan–Newman algorithm.  This algorithm identifies edges in a network that lie between communities and then removes them, leaving behind just the communities themselves.  The identification is performed by employing the graph-theoretic measure betweenness centrality, which assigns a number to each edge which is large if the edge lies \"between\" many pairs of nodes.\\nThe Girvan–Newman algorithm returns results of reasonable quality and is popular because it has been implemented in a number of standard software packages.  But it also runs slowly, taking time O(m2n) on a network of n vertices and m edges, making it impractical for networks of more than a few thousand nodes.\\n\\n\\n=== Modularity maximization ===\\nIn spite of its known drawbacks, one of the most widely used methods for community detection is modularity maximization.  Modularity is a benefit function that measures the quality of a particular division of a network into communities.  The modularity maximization method detects communities by searching over possible divisions of a network for one or more that have particularly high modularity.  Since exhaustive search over all possible divisions is usually intractable, practical algorithms are based on approximate optimization methods such as greedy algorithms, simulated annealing, or spectral optimization, with different approaches offering different balances between speed and accuracy.\\nA popular modularity maximization approach is the Louvain method, which iteratively optimizes local communities until global modularity can no longer be improved given perturbations to the current community state.\\nAn algorithm that utilizes the RenEEL scheme, which is an example of the Extremal Ensemble Learning (EEL) paradigm, is currently the best modularity maximizing algorithm.The usefulness of modularity optimization is questionable, as it has been shown that modularity optimization often fails to detect clusters smaller than some scale, depending on the size of the network (resolution limit); on the other hand the landscape of modularity values is characterized by a huge degeneracy of partitions with high modularity, close to the absolute maximum, which may be very different from each other.\\n\\n\\n=== Statistical inference ===\\nMethods based on statistical inference attempt to fit a generative model to the network data, which encodes the community structure. The overall advantage of this approach compared to the alternatives is its more principled nature, and the capacity to inherently address issues of statistical significance. Most methods in the literature are based on the stochastic block model as well as variants including mixed membership,\\ndegree-correction, and hierarchical structures.Model selection can be performed using principled approaches such as minimum description length (or equivalently, Bayesian model selection) and likelihood-ratio test. Currently many algorithms exist to perform efficient inference of stochastic block models, including belief propagation\\nand agglomerative Monte Carlo.In contrast to approaches that attempt to cluster a network given an objective function, this class of methods is based on generative models, which not only serve as a description of the large-scale structure of the network, but also can be used to generalize the data and predict the occurrence of missing or spurious links in the network.\\n\\n\\n=== Clique-based methods ===\\nCliques are subgraphs in which every node is connected to every other node in the clique.  As nodes can not be more tightly connected than this, it is not surprising that there are many approaches to community detection in networks based on the detection of cliques in a graph and the analysis of how these overlap.  Note that as a node can be a member of more than one clique, a node can be a member of more than one community in these methods giving an \"overlapping community structure\".\\nOne approach is to find the \"maximal cliques\". That is to find the cliques which are not the subgraph of any other clique.  The classic algorithm to find these is the Bron–Kerbosch algorithm. The overlap of these can be used to define communities in several ways.  The simplest is to consider only  maximal cliques bigger than a minimum size (number of nodes).  The union of these cliques then defines a subgraph whose components (disconnected parts) then define communities. Such approaches are often implemented in social network analysis software such as UCInet.\\nThe alternative approach is to use cliques of fixed size \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  . The overlap of these can be used to define a type of \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -regular hypergraph or a structure which is a generalisation of the line graph (the case when \\n  \\n    \\n      \\n        k\\n        =\\n        2\\n      \\n    \\n    {\\\\displaystyle k=2}\\n  ) known as a \"Clique graph\".  The clique graphs have vertices which represent the cliques in the original graph while the edges of the clique graph record the overlap of the clique in the original graph.  Applying any of the previous community detection methods (which assign each node to a community) to the clique graph then assigns each clique to a community.  This can then be used to determine community membership of nodes in the cliques.  Again as a node may be in several cliques, it can be a member of several communities.\\nFor instance the  clique percolation method defines communities as percolation clusters of \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -cliques. To do this it\\nfinds all \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -cliques in a network, that is all the complete sub-graphs of \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -nodes.\\nIt then defines two \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -cliques to be adjacent if they share \\n  \\n    \\n      \\n        k\\n        −\\n        1\\n      \\n    \\n    {\\\\displaystyle k-1}\\n   nodes, that is this is used to define edges in a clique graph. A community is then defined to be the maximal union of \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -cliques in which we can reach any \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -clique from any other \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -clique through series of \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -clique adjacencies. That is communities are just the connected components in the clique graph. Since a node can belong to several different \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -clique percolation clusters at the same time, the communities can overlap with each other.\\n\\n\\n=== Community detection in latent feature spaces ===\\nA network can be represented or projected onto a latent space via representation learning methods to efficiently represent a system. Then, various clustering methods can be employed to detect community structures. For Euclidean spaces, methods like embedding-based Silhouette community detection can be utilized. For Hypergeometric latent spaces, critical gap method or modified density-based, hierarchical, or partitioning-based clustering methods can be utilized.\\n\\n\\n== Testing methods of finding communities algorithms ==\\nThe evaluation of algorithms, to detect which are better at detecting community structure, is still an open question. It must be based on analyses of networks of known structure. A typical example is the \"four groups\" test, in which a network is divided into four equally-sized groups (usually of 32 nodes each) and the probabilities of connection within and between groups varied to create more or less challenging structures for the detection algorithm. Such benchmark graphs are a special case of the planted l-partition model\\nof Condon and Karp, or more generally of \"stochastic block models\", a general class of random network models containing community structure. Other more flexible benchmarks have been proposed that allow for varying group sizes and nontrivial degree distributions, such as LFR benchmark\\nwhich is an extension of the four groups benchmark that includes heterogeneous distributions of node degree and community size, making it a more severe test of community detection methods.Commonly used computer-generated benchmarks start with a network of well-defined communities. Then, this structure is degraded by rewiring or removing links and it gets harder and harder for the algorithms to detect the original partition. At the end, the network reaches a point where it is essentially random. This kind of benchmark may be called \"open\". The performance on these benchmarks is evaluated by measures such as normalized mutual information or variation of information. They compare the solution obtained by an algorithm  with the original community structure, evaluating the similarity of both partitions.\\n\\n\\n== Detectability ==\\nDuring recent years, a rather surprising result has been obtained by various groups which shows that a phase transition exists in the community detection problem, showing that as the density of connections inside communities and between communities become more and more equal or both become smaller (equivalently, as the community structure becomes too weak or the network becomes too sparse), suddenly the communities become undetectable. In a sense, the communities themselves still exist, since the presence and absence of edges is still correlated with the community memberships of their endpoints; but it becomes information-theoretically impossible to label the nodes better than chance, or even distinguish the graph from one generated by a null model such as the Erdos–Renyi model without community structure. This transition is independent of the type of algorithm being used to detect communities, implying that there exists a fundamental limit on our ability to detect communities in networks, even with optimal Bayesian inference (i.e., regardless of our computational resources).Consider a stochastic block model with total \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   nodes, \\n  \\n    \\n      \\n        q\\n        =\\n        2\\n      \\n    \\n    {\\\\displaystyle q=2}\\n   groups of equal size, and let \\n  \\n    \\n      \\n        \\n          p\\n          \\n            in\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{in}}}\\n   and \\n  \\n    \\n      \\n        \\n          p\\n          \\n            out\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{out}}}\\n   be the connection probabilities inside and between the groups respectively. If \\n  \\n    \\n      \\n        \\n          p\\n          \\n            in\\n          \\n        \\n        >\\n        \\n          p\\n          \\n            out\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{in}}>p_{\\\\text{out}}}\\n  , the network would possess community structure since the link density inside the groups would be more than the density of links between the groups.  In the sparse case, \\n  \\n    \\n      \\n        \\n          p\\n          \\n            in\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{in}}}\\n   and \\n  \\n    \\n      \\n        \\n          p\\n          \\n            out\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{out}}}\\n   scale as \\n  \\n    \\n      \\n        O\\n        (\\n        1\\n        \\n          /\\n        \\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(1/n)}\\n   so that the average degree is constant:\\n\\n  \\n    \\n      \\n        \\n          p\\n          \\n            in\\n          \\n        \\n        =\\n        \\n          c\\n          \\n            in\\n          \\n        \\n        \\n          /\\n        \\n        n\\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{in}}=c_{\\\\text{in}}/n}\\n   and \\n  \\n    \\n      \\n        \\n          p\\n          \\n            out\\n          \\n        \\n        =\\n        \\n          c\\n          \\n            out\\n          \\n        \\n        \\n          /\\n        \\n        n\\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{out}}=c_{\\\\text{out}}/n}\\n  Then it becomes impossible to detect the communities when:\\n\\n  \\n    \\n      \\n        \\n          c\\n          \\n            in\\n          \\n        \\n        −\\n        \\n          c\\n          \\n            out\\n          \\n        \\n        =\\n        \\n          \\n            2\\n            (\\n            \\n              c\\n              \\n                in\\n              \\n            \\n            +\\n            \\n              c\\n              \\n                out\\n              \\n            \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle c_{\\\\text{in}}-c_{\\\\text{out}}={\\\\sqrt {2(c_{\\\\text{in}}+c_{\\\\text{out}})}}}\\n  \\n\\n\\n== See also ==\\nComplex network\\nHierarchy\\nNetwork theory\\nPercolation theory\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nCommunity detection in graphs – an introduction\\nAre there implementations of algorithms for community detection in graphs? – Stack Overflow\\nWhat are the differences between community detection algorithms in igraph? – Stack Overflow', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of summaries:\n",
    "\n",
    "1) with clustering:\n",
    "\n",
    "President Biden has congratulated the members of the 118th Congress, including the new Speaker of the House, Kevin McCarthy, and the new House Minority Leader, Hakeem Jeffries. In two years, the US economy has recovered from a recession, 12 million jobs have been created, and President Obama has passed over 300 bipartisan laws to help veterans, protect the right to marry, and restore the soul of the nation. The Bipartisan Infrastructure Law is the largest investment in infrastructure since Eisenhower's Interstate Highway System, and the Inflation Reduction Act is a law proposed to reduce health care costs. The Junk Fee Prevention Act is proposed to protect American consumers from unfair fees and restore the dignity of work. President Biden's plan to reduce child poverty and increase economic growth includes restoring the Child Tax Credit, providing access to affordable housing and education, and fighting fraud. He is also taking a stand against Vladimir Putin's war against Ukraine and is rallying the world to meet global challenges. Lastly, the speaker of the House of Representatives urges Americans to come together and condemn political violence, uphold the rule of law, and embrace light over darkness.\n",
    "\n",
    "2) llamaindex tree summarize with a ListIndex\n",
    "\n",
    "The speaker addresses various individuals and groups, congratulating the new Speaker of the House and acknowledging political leaders. They emphasize the progress and resilience of the United States, highlighting economic growth, job creation, and the handling of the COVID-19 pandemic. The speaker emphasizes bipartisan cooperation and discusses efforts to rebuild the middle class, create manufacturing jobs, invest in infrastructure, and address healthcare costs and the climate crisis. They stress fair taxation and urge the completion of their economic plan. The context information also mentions topics such as rebuilding after storms and wildfires, investing in clean energy, addressing the climate crisis, implementing tax reforms, improving public safety, and investing in education and affordable housing. The speaker emphasizes the desire for safe neighborhoods, law enforcement accountability, and the safety of children. They address issues such as gun violence, immigration, reproductive rights, LGBTQ rights, and the need for bipartisan cooperation. The speaker discusses progress made in areas such as addiction, mental health, cancer research, and veterans' support, but acknowledges more needs to be done. They stress the importance of democracy, protecting the right to vote, and upholding the rule of law. The speaker calls for unity, rejects hate and extremism, and believes the current moment is crucial for the future of the nation. They express optimism about the future of America and conclude by invoking the nation's identity as a beacon of hope.\n",
    "\n",
    "3) llamaindex tree summarize Summary index\n",
    "\n",
    "The speaker's address covers a wide range of topics and issues. They congratulate political leaders and emphasize the progress and resilience of the United States. The speaker discusses the state of the economy, the decline of COVID-19's impact, and the creation of new jobs. They highlight the threats faced by democracy and the importance of working together across party lines. The speaker outlines their vision for restoring the nation, rebuilding the middle class, and uniting the country. They emphasize the need to invest in infrastructure, manufacturing, and small businesses, as well as addressing healthcare costs. The speaker also addresses issues such as climate change, fair taxation, education, public safety, mental health, and substance abuse. They advocate for trust in law enforcement, accountability, and protecting reproductive and LGBTQ rights. The speaker discusses the need to defend democracy, compete with China, and invest in American innovation. They highlight progress made in areas such as addiction treatment, cancer research, and veterans' support. The speech concludes with a call for unity, upholding the rule of law, and protecting the right to vote.\n",
    "\n",
    "4) llamaindex tree summarize Summmary Index with CUSTOM_TREE_SUMMARIZE_PROMPT\n",
    "\n",
    "The speaker addresses various individuals and groups, congratulating the new Speaker of the House and acknowledging political leaders. They emphasize the progress and resilience of America, highlighting the ability to emerge stronger from crises. The speaker discusses economic challenges faced in the past but claims to have created a record number of new jobs. They also discuss the impact of COVID-19 and the threat to democracy. The importance of unity and bipartisanship is emphasized, along with achievements in defense, infrastructure, and veterans' rights. The speaker expresses their vision for restoring the nation, rebuilding the middle class, and uniting the country through job creation, manufacturing, and investing in infrastructure. Specific projects are mentioned, such as rebuilding after storms and wildfires, investing in clean energy and electric vehicle infrastructure, addressing the climate crisis, implementing tax reforms, cracking down on fraud and white-collar crime, improving public safety and police reform, and investing in education and affordable housing. The speaker also addresses the need to protect democracy, denounces political violence, and emphasizes the importance of upholding the rule of law and restoring trust in democratic institutions. They call for unity and express optimism about the nation's future. The speech concludes with a blessing for the audience and a wish for the protection of the troops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying summary of 2 articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fastapi_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
