{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('/workspaces/ml-learning/.env', override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def pickleSave(object, name, folder=\".\", silent=False):\n",
    "    filename = folder + \"/\" + name + \".pkl\"\n",
    "    if silent == False:\n",
    "        print(\"Saving object {} to pickle file {}\".format(name, filename))\n",
    "    with open(filename, mode=\"wb\") as fipkl:\n",
    "        pickle.dump(object, fipkl)\n",
    "\n",
    "\n",
    "def pickleLoad(name, folder: str, silent=False):\n",
    "    filename = folder + \"/\" + name + \".pkl\"\n",
    "    if silent == False:\n",
    "        print(\"Loading object {} from pickle file {}\".format(name, filename))\n",
    "\n",
    "    try:\n",
    "        with open(filename, mode=\"rb\") as fipkl:\n",
    "            myObject = pickle.load(fipkl)\n",
    "        return myObject\n",
    "    except IOError:\n",
    "        print(\"Pickle file {} not found, returning None object\".format(filename))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read Pubmed Papers.\"\"\"\n",
    "from typing import List, Optional\n",
    "\n",
    "from llama_index.readers.base import BaseReader\n",
    "from llama_index.readers.schema.base import Document\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "class PubmedReader(BaseReader):\n",
    "    \"\"\"Pubmed Reader.\n",
    "\n",
    "    Gets a search query, return a list of Documents of the top corresponding scientific papers on Pubmed.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_data_bioc(\n",
    "        self,\n",
    "        search_query: str,\n",
    "        max_results: Optional[int] = 10,\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Search for a topic on Pubmed, fetch the text of the most relevant full-length papers.\n",
    "        Uses the BoiC API, which has been down a lot.\n",
    "\n",
    "        Args:\n",
    "            search_query (str): A topic to search for (e.g. \"Alzheimers\").\n",
    "            max_results (Optional[int]): Maximum number of papers to fetch.\n",
    "\n",
    "        Returns:\n",
    "            List[Document]: A list of Document objects.\n",
    "        \"\"\"\n",
    "        import xml.etree.ElementTree as xml\n",
    "        from datetime import datetime\n",
    "\n",
    "        import requests\n",
    "\n",
    "        pubmed_search = []\n",
    "        parameters = {\"tool\": \"tool\", \"email\": \"email\", \"db\": \"pmc\"}\n",
    "        parameters[\"term\"] = search_query\n",
    "        parameters[\"retmax\"] = max_results\n",
    "        resp = requests.get(\n",
    "            \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\",\n",
    "            params=parameters,\n",
    "        )\n",
    "        root = xml.fromstring(resp.content)\n",
    "        display(Markdown(f'{root}'))\n",
    "\n",
    "        for elem in root.iter():\n",
    "            if elem.tag == \"Id\":\n",
    "                _id = elem.text\n",
    "                try:\n",
    "                    resp = requests.get(\n",
    "                        f\"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_json/PMC{_id}/ascii\"\n",
    "                    )\n",
    "                    info = resp.json()\n",
    "                    title = \"Pubmed Paper\"\n",
    "                    try:\n",
    "                        title = [\n",
    "                            p[\"text\"]\n",
    "                            for p in info[\"documents\"][0][\"passages\"]\n",
    "                            if p[\"infons\"][\"section_type\"] == \"TITLE\"\n",
    "                        ][0]\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "                    pubmed_search.append(\n",
    "                        {\n",
    "                            \"title\": title,\n",
    "                            \"url\": (\n",
    "                                f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{_id}/\"\n",
    "                            ),\n",
    "                            \"date\": info[\"date\"],\n",
    "                            \"documents\": info[\"documents\"],\n",
    "                        }\n",
    "                    )\n",
    "                except Exception:\n",
    "                    print(f\"Unable to parse PMC{_id} or it does not exist\")\n",
    "                    pass\n",
    "\n",
    "        # Then get documents from Pubmed text, which includes abstracts\n",
    "        pubmed_documents = []\n",
    "        for paper in pubmed_search:\n",
    "            for d in paper[\"documents\"]:\n",
    "                text = \"\\n\".join([p[\"text\"] for p in d[\"passages\"]])\n",
    "                pubmed_documents.append(\n",
    "                    Document(\n",
    "                        text=text,\n",
    "                        extra_info={\n",
    "                            \"Title of this paper\": paper[\"title\"],\n",
    "                            \"URL\": paper[\"url\"],\n",
    "                            \"Date published\": datetime.strptime(\n",
    "                                paper[\"date\"], \"%Y%m%d\"\n",
    "                            ).strftime(\"%m/%d/%Y\"),\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return pubmed_documents\n",
    "\n",
    "    def load_data(\n",
    "        self,\n",
    "        search_query: str,\n",
    "        max_results: Optional[int] = 10,\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Search for a topic on Pubmed, fetch the text of the most relevant full-length papers.\n",
    "        Args:\n",
    "            search_query (str): A topic to search for (e.g. \"Alzheimers\").\n",
    "            max_results (Optional[int]): Maximum number of papers to fetch.\n",
    "        Returns:\n",
    "            List[Document]: A list of Document objects.\n",
    "        \"\"\"\n",
    "        import time\n",
    "        import xml.etree.ElementTree as xml\n",
    "\n",
    "        import requests\n",
    "\n",
    "        #https://pubmed.ncbi.nlm.nih.gov/help/#proximity-searching\n",
    "\n",
    "        pubmed_search = []\n",
    "        parameters = {\"tool\": \"tool\", \"email\": \"email\", \"db\": \"pmc\"}\n",
    "        parameters[\"term\"] = search_query\n",
    "        parameters[\"retmax\"] = max_results\n",
    "        resp = requests.get(\n",
    "            \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\",\n",
    "            params=parameters,\n",
    "        )\n",
    "        root = xml.fromstring(resp.content)\n",
    "        display(Markdown(f'{root}'))\n",
    "        \n",
    "\n",
    "        for elem in root.iter():\n",
    "            if elem.tag == \"Id\":\n",
    "                _id = elem.text\n",
    "                url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?id={_id}&db=pmc\"\n",
    "                print(url)\n",
    "                try:\n",
    "                    resp = requests.get(url)\n",
    "                    info = xml.fromstring(resp.content)\n",
    "                    display(Markdown(f'{info}'))\n",
    "\n",
    "                    raw_text = \"\"\n",
    "                    title = \"\"\n",
    "                    journal = \"\"\n",
    "                    for element in info.iter():\n",
    "                        display(Markdown(f'Element:{element}'))\n",
    "                        if element.tag == \"article-title\":\n",
    "                            title = element.text\n",
    "                        elif element.tag == \"journal-title\":\n",
    "                            journal = element.text\n",
    "\n",
    "                        if element.text:\n",
    "                            raw_text += element.text.strip() + \" \"\n",
    "\n",
    "                    pubmed_search.append(\n",
    "                        {\n",
    "                            \"title\": title,\n",
    "                            \"journal\": journal,\n",
    "                            \"url\": (\n",
    "                                f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{_id}/\"\n",
    "                            ),\n",
    "                            \"text\": raw_text,\n",
    "                        }\n",
    "                    )\n",
    "                    time.sleep(1)  # API rate limits\n",
    "                except Exception as e:\n",
    "                    print(f\"Unable to parse PMC{_id} or it does not exist:\", e)\n",
    "                    pass\n",
    "\n",
    "        # Then get documents from Pubmed text, which includes abstracts\n",
    "        pubmed_documents = []\n",
    "        for paper in pubmed_search:\n",
    "            pubmed_documents.append(\n",
    "                Document(\n",
    "                    text=paper[\"text\"],\n",
    "                    extra_info={\n",
    "                        \"Title of this paper\": paper[\"title\"],\n",
    "                        \"Journal it was published in:\": paper[\"journal\"],\n",
    "                        \"URL\": paper[\"url\"],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return pubmed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PubmedReader()\n",
    "documents = loader.load_data(search_query='Phi X 174 Phage')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
