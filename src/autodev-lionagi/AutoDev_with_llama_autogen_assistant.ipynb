{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG assisted Auto Developer \n",
    "-- with LionAGI, LlamaIndex, Autogen and OAI code interpreter\n",
    "\n",
    "\n",
    "Let us develop a dev bot that can \n",
    "- read and understand lionagi's existing codebase\n",
    "- QA with the codebase to clarify tasks\n",
    "- produce and tests pure python codes with code interpreter with automatic followup if quality is less than expected\n",
    "- output final runnable python codes \n",
    "\n",
    "This tutorial shows you how you can automatically produce high quality prototype and drafts codes customized for your own codebase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lionagi\n",
      "  Downloading lionagi-0.0.112-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: llama_index in ./.venv/lib/python3.11/site-packages (0.9.24)\n",
      "Collecting pyautogen\n",
      "  Downloading pyautogen-0.2.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: aiohttp>=3.9.0 in ./.venv/lib/python3.11/site-packages (from lionagi) (3.9.1)\n",
      "Requirement already satisfied: python-dotenv==1.0.0 in ./.venv/lib/python3.11/site-packages (from lionagi) (1.0.0)\n",
      "Collecting tiktoken==0.5.1 (from lionagi)\n",
      "  Downloading tiktoken-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting httpx==0.25.1 (from lionagi)\n",
      "  Downloading httpx-0.25.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx==0.25.1->lionagi) (4.2.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx==0.25.1->lionagi) (2023.11.17)\n",
      "Requirement already satisfied: httpcore in ./.venv/lib/python3.11/site-packages (from httpx==0.25.1->lionagi) (1.0.2)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.11/site-packages (from httpx==0.25.1->lionagi) (3.6)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from httpx==0.25.1->lionagi) (1.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.11/site-packages (from tiktoken==0.5.1->lionagi) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.venv/lib/python3.11/site-packages (from tiktoken==0.5.1->lionagi) (2.31.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in ./.venv/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index) (2.0.24)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in ./.venv/lib/python3.11/site-packages (from llama_index) (4.12.2)\n",
      "Requirement already satisfied: dataclasses-json in ./.venv/lib/python3.11/site-packages (from llama_index) (0.6.3)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.11/site-packages (from llama_index) (1.2.14)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from llama_index) (2023.12.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in ./.venv/lib/python3.11/site-packages (from llama_index) (1.5.8)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in ./.venv/lib/python3.11/site-packages (from llama_index) (3.8.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from llama_index) (1.26.2)\n",
      "Requirement already satisfied: openai>=1.1.0 in ./.venv/lib/python3.11/site-packages (from llama_index) (1.6.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from llama_index) (2.1.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in ./.venv/lib/python3.11/site-packages (from llama_index) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.11/site-packages (from llama_index) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.11/site-packages (from llama_index) (0.9.0)\n",
      "Collecting diskcache (from pyautogen)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting flaml (from pyautogen)\n",
      "  Downloading FLAML-2.1.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting termcolor (from pyautogen)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp>=3.9.0->lionagi) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp>=3.9.0->lionagi) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp>=3.9.0->lionagi) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp>=3.9.0->lionagi) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp>=3.9.0->lionagi) (1.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama_index) (2.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in ./.venv/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama_index) (1.16.0)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (1.3.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama_index) (4.66.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai>=1.1.0->llama_index) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.11/site-packages (from openai>=1.1.0->llama_index) (2.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken==0.5.1->lionagi) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken==0.5.1->lionagi) (2.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama_index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.11/site-packages (from dataclasses-json->llama_index) (3.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->llama_index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->llama_index) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.11/site-packages (from pandas->llama_index) (2023.4)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama_index) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama_index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama_index) (2.14.6)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama_index) (1.16.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore->httpx==0.25.1->lionagi) (0.14.0)\n",
      "Downloading lionagi-0.0.112-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyautogen-0.2.2-py3-none-any.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.0/124.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading FLAML-2.1.1-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.2/295.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: termcolor, flaml, diskcache, tiktoken, httpx, lionagi, pyautogen\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.5.2\n",
      "    Uninstalling tiktoken-0.5.2:\n",
      "      Successfully uninstalled tiktoken-0.5.2\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.26.0\n",
      "    Uninstalling httpx-0.26.0:\n",
      "      Successfully uninstalled httpx-0.26.0\n",
      "Successfully installed diskcache-5.6.3 flaml-2.1.1 httpx-0.25.1 lionagi-0.0.112 pyautogen-0.2.2 termcolor-2.4.0 tiktoken-0.5.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lionagi llama_index pyautogen networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.11/site-packages (23.3.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (3.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import lionagi as li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_FOLDER = '/workspaces/ml-learning'\n",
    "\n",
    "ext=\".py\"                               # extension of files of interest, can be str or list[str]\n",
    "data_dir = f'/workspaces/ml-learning/paper-qa'     # directory of source data - lionagi codebase\n",
    "project_name = \"autodev_paper-qa\"           # give a project name\n",
    "output_dir = f'{WORKSPACE_FOLDER}/src/autodev-lionagi/data/log/coder/'        # output dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lst 1:  ['.py']\n",
      "exception l_call: _dir_to_path() got multiple values for argument 'dir'\n",
      "exception dir_to_path: Given function cannot be applied to the input. Error: _dir_to_path() got multiple values for argument 'dir'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid directory or extension, please check the path",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/lionagi/utils/sys_utils.py:492\u001b[0m, in \u001b[0;36ml_call\u001b[0;34m(input, func, flatten_dict, flat, dropna, **kwags)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlst 1: \u001b[39m\u001b[38;5;124m'\u001b[39m, lst)\n\u001b[0;32m--> 492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlst\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/lionagi/utils/sys_utils.py:492\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlst 1: \u001b[39m\u001b[38;5;124m'\u001b[39m, lst)\n\u001b[0;32m--> 492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwags\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m lst]\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: _dir_to_path() got multiple values for argument 'dir'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/lionagi/loader/load_utils.py:21\u001b[0m, in \u001b[0;36mdir_to_path\u001b[0;34m(dir, ext, recursive, flat)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m to_list(\u001b[43ml_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dir_to_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecursive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mext\u001b[49m\u001b[43m)\u001b[49m, \n\u001b[1;32m     23\u001b[0m                    flat\u001b[38;5;241m=\u001b[39mflat)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/lionagi/utils/sys_utils.py:495\u001b[0m, in \u001b[0;36ml_call\u001b[0;34m(input, func, flatten_dict, flat, dropna, **kwags)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexception l_call:\u001b[39m\u001b[38;5;124m'\u001b[39m, e)\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiven function cannot be applied to the input. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Given function cannot be applied to the input. Error: _dir_to_path() got multiple values for argument 'dir'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mli\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdir_to_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m chunks \u001b[38;5;241m=\u001b[39m li\u001b[38;5;241m.\u001b[39mfile_to_chunks(files, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,  overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \n\u001b[1;32m      5\u001b[0m                            threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, to_csv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, project\u001b[38;5;241m=\u001b[39mproject_name, \n\u001b[1;32m      6\u001b[0m                            filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_chunks.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, timestamp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/lionagi/loader/load_utils.py:51\u001b[0m, in \u001b[0;36mdir_to_files\u001b[0;34m(dir, ext, recursive, reader, clean, to_csv, project, output_dir, filename, verbose, timestamp, logger)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdir_to_files\u001b[39m(\u001b[38;5;28mdir\u001b[39m: \u001b[38;5;28mstr\u001b[39m, \n\u001b[1;32m     39\u001b[0m                  ext: \u001b[38;5;28mstr\u001b[39m, \n\u001b[1;32m     40\u001b[0m                  recursive: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m                  timestamp: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     49\u001b[0m                  logger: Optional[DataLogger] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 51\u001b[0m     sources \u001b[38;5;241m=\u001b[39m \u001b[43mdir_to_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_dict\u001b[39m(path_: Path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, Path]]:\n\u001b[1;32m     54\u001b[0m         folder, file \u001b[38;5;241m=\u001b[39m _split_path(path_)\n",
      "File \u001b[0;32m/workspaces/ml-learning/.venv/lib/python3.11/site-packages/lionagi/loader/load_utils.py:26\u001b[0m, in \u001b[0;36mdir_to_path\u001b[0;34m(dir, ext, recursive, flat)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexception dir_to_path:\u001b[39m\u001b[38;5;124m'\u001b[39m, e)\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid directory or extension, please check the path\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid directory or extension, please check the path"
     ]
    }
   ],
   "source": [
    "files = li.dir_to_files(dir=data_dir, ext=ext, clean=True, recursive=True,\n",
    "                        project=project_name, to_csv=True, timestamp=False)\n",
    "\n",
    "chunks = li.file_to_chunks(files, chunk_size=512,  overlap=0.1, \n",
    "                           threshold=100, to_csv=True, project=project_name, \n",
    "                           filename=f\"{project_name}_chunks.csv\", timestamp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      There are in total 107,716 \n",
      "      chracters in 19 non-empty files\n",
      "      \n",
      "Minimum length of files is 24 in characters\n",
      "Maximum length of files is 25,891 in characters\n",
      "Average length of files is 5,669 in characters\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "      There are in total {sum(li.l_call(files, lambda x: x['file_size'])):,} \n",
    "      chracters in {len(files)} non-empty files\n",
    "      \"\"\")\n",
    "\n",
    "lens = li.l_call(files, lambda x: len(x['content']))\n",
    "min_, max_, avg_ = min(lens), max(lens), sum(lens)/len(lens)\n",
    "\n",
    "print(f\"Minimum length of files is {min_} in characters\")\n",
    "print(f\"Maximum length of files is {max_:,} in characters\")\n",
    "print(f\"Average length of files is {int(avg_):,} in characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the files seem to be fairly uneven in terms of length\n",
    "which could bring problems in our subsequent analysis, we can stardardize them into chunks \n",
    "one convinient way to do this is via file_to_chunks function, it breaks the files into organized chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total 218 chunks\n",
      "Minimum length of content in chunk is 24 characters\n",
      "Maximum length of content in chunk is 609 characters\n",
      "Average length of content in chunk is 539 characters\n",
      "There are in total 117,666 chracters\n"
     ]
    }
   ],
   "source": [
    "lens = li.l_call(li.to_list(chunks, flat=True), lambda x: len(x[\"chunk_content\"]))\n",
    "min_, max_, avg_ = min(lens), max(lens), sum(lens)/len(lens)\n",
    "\n",
    "print(f\"There are in total {len(li.to_list(chunks,flat=True)):,} chunks\")\n",
    "print(f\"Minimum length of content in chunk is {min_} characters\")\n",
    "print(f\"Maximum length of content in chunk is {max_:,} characters\")\n",
    "print(f\"Average length of content in chunk is {int(avg_):,} characters\")\n",
    "print(f\"There are in total {sum(li.l_call(chunks, lambda x: x['chunk_size'])):,} chracters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'project': 'autodev_lion',\n",
       " 'folder': 'lionagi',\n",
       " 'file': 'version.py',\n",
       " 'file_size': 24,\n",
       " 'chunk_overlap': 0.1,\n",
       " 'chunk_threshold': 100,\n",
       " 'file_chunks': 1,\n",
       " 'chunk_id': 1,\n",
       " 'chunk_size': 24,\n",
       " 'chunk_content': '__version__ = \"0.0.106\" '}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup llamaIndex Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, VectorStoreIndex\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.schema import TextNode\n",
    "\n",
    "# build nodes from our existing chunks\n",
    "f = lambda content: TextNode(text=content)\n",
    "nodes = li.l_call(chunks, lambda x: f(x[\"chunk_content\"]))\n",
    "\n",
    "# set up vector index\n",
    "llm = OpenAI(temperature=0.1, model=\"gpt-4-1106-preview\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "index1 = VectorStoreIndex(nodes, include_embeddings=True, service_context=service_context)\n",
    "\n",
    "# set up query engine\n",
    "query_engine = index1.as_query_engine(include_text=False, response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `Session` object is made of a class that represents a conversation session with a conversational AI system. This class manages the interactions within the session.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"what is session object made of?\")\n",
    "\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using oai assistant Code Interpreter with Autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-3.5-turbo\", \"gpt-35-turbo\", \"gpt-4\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-turbo\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coder_instruction = f\"\"\"\n",
    "        You are an expert at writing python codes. Write pure python codes, and run it to validate the \n",
    "        codes, then return with the full implementation + the word TERMINATE when the task is solved \n",
    "        and there is no problem. Reply FAILED if you cannot solve the problem.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent\n",
    "from autogen.agentchat import UserProxyAgent\n",
    "\n",
    "# Initiate an agent equipped with code interpreter\n",
    "gpt_assistant = GPTAssistantAgent(\n",
    "    name=\"Coder Assistant\",\n",
    "    llm_config={\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"type\": \"code_interpreter\"\n",
    "            }\n",
    "        ],\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    "    instructions=coder_instruction,\n",
    ")\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda msg: \"TERMINATE\" in msg[\"content\"],\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,  # set to True or image name like \"python:3\" to use docker\n",
    "    },\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "async def code_pure_python(instruction):\n",
    "    user_proxy.initiate_chat(gpt_assistant, message=instruction)\n",
    "    return gpt_assistant.last_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Make query engine and oai assistant into tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool1 = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"query_lionagi_codebase\",\n",
    "            \"description\": \"Perform a query to a QA bot with access to a vector index built with package lionagi codebase\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"str_or_query_bundle\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"a question to ask the QA bot\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"str_or_query_bundle\"],\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]\n",
    "tool2=[{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"code_pure_python\",\n",
    "            \"description\": \"Give an instruction to a coding assistant to write pure python codes\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"instruction\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"coding instruction to give to the coding assistant\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"instruction\"],\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "tools = [tool1[0], tool2[0]]\n",
    "funcs = [query_engine.query, code_pure_python]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Write Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = {\n",
    "    \"persona\": \"a helpful software engineer\",\n",
    "    \"requirements\": \"think step by step before returning a thoughtful answer that follows the instruction with clearly, precisely worded answer with a humble yet confident tone\",\n",
    "    \"responsibilities\": f\"you are asked to help with coding on the python package of lionagi\",\n",
    "    \"tools\": \"provided with a QA bot for grounding responses, and a coding assistant to write pure python codes\"\n",
    "}\n",
    "\n",
    "function_call1 = {\n",
    "    \"notice\":\"\"\"\n",
    "        At each task step, identified by step number, you must use the tool \n",
    "        at least five times. Notice you are provided with a QA bot as your tool, \n",
    "        the bot has access to the source codes via a queriable index that takes \n",
    "        natural language query and return a natural language answer. You can \n",
    "        decide whether to invoke the function call, you will need to ask the bot \n",
    "        when there are things need clarification or further information. you \n",
    "        provide the query by asking a question, please use the tool extensively \n",
    "        as you can (up to ten times)\n",
    "        \"\"\",}\n",
    "\n",
    "function_call2 = {\n",
    "    \"notice\":\"\"\"\n",
    "        At each task step, identified by step number, you must use the tool \n",
    "        at least once, and you must use the tool at least once more if the previous \n",
    "        run failed. Notice you are provided with a coding assistant as your tool, the \n",
    "        bot can write and run python codes in a sandbox environment, it takes natural \n",
    "        language instruction, and return with 'success'/'failed'. For the instruction \n",
    "        you give, it needs to be very clear and detailed such that an AI coding assistant \n",
    "        can produce excellent output.  \n",
    "        \"\"\",}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct1 = {\n",
    "    \"task step\": \"1\", \n",
    "    \"task name\": \"understand user requirements\", \n",
    "    \"task objective\": \"get a comprehensive understanding of the task given\", \n",
    "    \"task description\": \"user provided you with a task, please understand the task, propose plans on delivering it\"\n",
    "}\n",
    "\n",
    "instruct2 = {\n",
    "    \"task step\": \"2\", \n",
    "    \"task name\": \"propose a pure python solution\", \n",
    "    \"task objective\": \"give detailed instruction on how to achieve above task with pure python as if to a coding bot\", \n",
    "    \"task description\": \"you are responsible for further customizing the coding task into our lionagi package requirements, you are provided with a QA bot, please keep on asking questions if there are anything unclear, your instruction should focus on functionalities and coding logic\",\n",
    "    \"function_call\": function_call1\n",
    "}\n",
    "\n",
    "instruct3 = {\n",
    "    \"task step\": \"3\", \n",
    "    \"task name\": \"write pure python codes\", \n",
    "    \"task objective\": \"write runnable python codes\", \n",
    "    \"task description\": \"from your improved understanding of the task, please instruct the coding assistant on wiriting pure python codes. you will reply with the full implementation if the coding assistant succeed, which you need to return the full implementation in a well structured py format, run it once more if report back'failed', and return 'Task failed' with most recent effort, after the second failed attempt \",\n",
    "    \"function_call\": function_call2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve a coding task in pure python\n",
    "async def solve_in_python(context, num=10):\n",
    "    \n",
    "    # set up session and register both tools to session \n",
    "    coder = li.Session(system, dir=output_dir)\n",
    "    coder.register_tools(tools=tools, funcs=funcs)\n",
    "    \n",
    "    # initiate should not use tools\n",
    "    await coder.initiate(instruct1, context=context, temperature=0.7)\n",
    "    \n",
    "    # auto_followup with QA bot tool\n",
    "    await coder.auto_followup(instruct2, num=num, temperature=0.6, tools=tool1,\n",
    "                                   tool_parser=lambda x: x.response)\n",
    "    \n",
    "    # auto_followup with code interpreter tool\n",
    "    await coder.auto_followup(instruct3, num=2, temperature=0.5, tools=tool2)\n",
    "    \n",
    "    # save to csv\n",
    "    coder.messages_to_csv()\n",
    "    coder.log_to_csv()\n",
    "    \n",
    "    # return codes\n",
    "    return coder.conversation.messages[-1]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue = {\n",
    "    \"raise files and chunks into objects\": \"\"\"\n",
    "        files and chunks are currently in dict format, please design classes for them, include all \n",
    "        members, methods, staticmethods, class methods... if needed. please make sure your work \n",
    "        has sufficiednt content, make sure to include typing and docstrings\n",
    "        \"\"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to Coder Assistant):\n",
      "\n",
      "Please define a Python class named 'File' with the following specifications:\n",
      "\n",
      "- Attributes:\n",
      "  - 'name': A string representing the name of the file.\n",
      "  - 'size': An integer representing the size of the file in bytes.\n",
      "  - 'file_type': A string representing the type of the file (e.g., 'txt', 'jpg').\n",
      "\n",
      "- Methods:\n",
      "  - '__init__': Constructor that takes 'name', 'size', and 'file_type' as parameters and initializes the respective attributes.\n",
      "  - 'read': A method that simulates reading the file content. For now, it can simply return a string 'File content of {name}.'\n",
      "  - 'write': A method that takes a string 'content' as a parameter and simulates writing to the file. It can print 'Writing to {name}: {content}'.\n",
      "  - 'delete': A method that simulates deleting the file. It can print '{name} deleted.'\n",
      "\n",
      "Please ensure to include type annotations for all attributes and method parameters, and add docstrings to the class and each method explaining their purpose.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder Assistant\u001b[0m (to user_proxy):\n",
      "\n",
      "The File class has been created as specified, including type annotations and docstrings. Here's the full implementation and example usage results:\n",
      "\n",
      "```python\n",
      "class File:\n",
      "    \"\"\"\n",
      "    A File class representing a file with basic operations like read, write, and delete.\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, name: str, size: int, file_type: str):\n",
      "        \"\"\"\n",
      "        Initializes a new File object with the provided name, size, and file type.\n",
      "        \n",
      "        :param name: String representing the name of the file.\n",
      "        :param size: Integer representing the size of the file in bytes.\n",
      "        :param file_type: String representing the type of the file (e.g., 'txt', 'jpg').\n",
      "        \"\"\"\n",
      "        self.name = name\n",
      "        self.size = size\n",
      "        self.file_type = file_type\n",
      "    \n",
      "    def read(self) -> str:\n",
      "        \"\"\"\n",
      "        Simulates reading the file content by returning a string representation of the content.\n",
      "        \n",
      "        :return: String representing the file content.\n",
      "        \"\"\"\n",
      "        return f\"File content of {self.name}.\"\n",
      "    \n",
      "    def write(self, content: str) -> None:\n",
      "        \"\"\"\n",
      "        Simulates writing the specified content to the file by printing the content.\n",
      "        \n",
      "        :param content: String representing the content to write to the file.\n",
      "        \"\"\"\n",
      "        print(f\"Writing to {self.name}: {content}\")\n",
      "    \n",
      "    def delete(self) -> None:\n",
      "        \"\"\"\n",
      "        Simulates deleting the file by printing a confirmation message.\n",
      "        \"\"\"\n",
      "        print(f\"{self.name} deleted.\")\n",
      "\n",
      "# Example usage:\n",
      "file_example = File(\"example.txt\", 1024, \"txt\")\n",
      "print(file_example.read())\n",
      "file_example.write(\"Hello World!\")\n",
      "file_example.delete()\n",
      "```\n",
      "\n",
      "Example output:\n",
      "\n",
      "```\n",
      "File content of example.txt.\n",
      "Writing to example.txt: Hello World!\n",
      "example.txt deleted.\n",
      "```\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to Coder Assistant):\n",
      "\n",
      "Please define two Python classes: 'File' and 'Chunk' with the following specifications:\n",
      "\n",
      "Class 'File':\n",
      "- Attributes:\n",
      "  - 'name': str, the name of the file.\n",
      "  - 'size': int, the size of the file in bytes.\n",
      "  - 'file_type': str, the type of the file (e.g., 'txt', 'jpg').\n",
      "- Methods:\n",
      "  - '__init__': Initializes the File instance with the given name, size, and file_type.\n",
      "  - 'read': Simulates reading the file by returning a string indicating the file content.\n",
      "  - 'write': Takes a string 'content' and simulates writing it to the file, printing a message.\n",
      "  - 'delete': Simulates deleting the file, printing a confirmation message.\n",
      "\n",
      "Class 'Chunk':\n",
      "- Attributes:\n",
      "  - 'index': int, the index of the chunk within the file.\n",
      "  - 'size': int, the size of the chunk in bytes.\n",
      "  - 'data': str, the content of the chunk.\n",
      "- Methods:\n",
      "  - '__init__': Initializes the Chunk instance with the given index, size, and data.\n",
      "  - 'get_data': Returns the data of the chunk.\n",
      "  - 'set_data': Takes a string 'new_data' and updates the chunk's data.\n",
      "\n",
      "Please ensure all attributes and methods have proper type annotations and docstrings explaining their purpose. After defining the classes, create instances of each class and demonstrate the usage of their methods with simple print statements.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder Assistant\u001b[0m (to user_proxy):\n",
      "\n",
      "The classes have been defined as per your specification and the methods have been demonstrated with print statements. Below is the full implementation:\n",
      "\n",
      "```python\n",
      "class File:\n",
      "    \"\"\"Represents a file with a name, size, and type.\"\"\"\n",
      "    \n",
      "    def __init__(self, name: str, size: int, file_type: str) -> None:\n",
      "        \"\"\"Initializes the file with a name, size, and type.\"\"\"\n",
      "        self.name = name\n",
      "        self.size = size\n",
      "        self.file_type = file_type\n",
      "\n",
      "    def read(self) -> str:\n",
      "        \"\"\"Simulates reading of file and returns the content as a string.\"\"\"\n",
      "        return f\"Reading content of file: {self.name}\"\n",
      "\n",
      "    def write(self, content: str) -> None:\n",
      "        \"\"\"Simulates writing content to the file, printing the operation.\"\"\"\n",
      "        print(f\"Writing to file: {self.name}. Content: {content}\")\n",
      "\n",
      "    def delete(self) -> None:\n",
      "        \"\"\"Simulates deleting the file and prints a confirmation message.\"\"\"\n",
      "        print(f\"File {self.name} deleted.\")\n",
      "\n",
      "\n",
      "class Chunk:\n",
      "    \"\"\"Represents a chunk of a file with an index, size, and data.\"\"\"\n",
      "    \n",
      "    def __init__(self, index: int, size: int, data: str) -> None:\n",
      "        \"\"\"Initializes the chunk with an index, size, and data.\"\"\"\n",
      "        self.index = index\n",
      "        self.size = size\n",
      "        self.data = data\n",
      "\n",
      "    def get_data(self) -> str:\n",
      "        \"\"\"Returns the data contained in the chunk.\"\"\"\n",
      "        return self.data\n",
      "\n",
      "    def set_data(self, new_data: str) -> None:\n",
      "        \"\"\"Updates the chunk's data with new data.\"\"\"\n",
      "        self.data = new_data\n",
      "\n",
      "# Creating instances of each class and demonstrating their usage\n",
      "file_example = File(name=\"example.txt\", size=1024, file_type=\"txt\")\n",
      "chunk_example = Chunk(index=1, size=512, data=\"This is a piece of data.\")\n",
      "\n",
      "# Demonstrating File methods\n",
      "print(file_example.read())\n",
      "file_example.write(\"Hello World\")\n",
      "file_example.delete()\n",
      "\n",
      "# Demonstrating Chunk methods\n",
      "print(chunk_example.get_data())\n",
      "chunk_example.set_data(\"New chunk data\")\n",
      "print(chunk_example.get_data())\n",
      "```\n",
      "\n",
      "Everything worked as expected.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "13 logs saved to data/log/coder/_messages_2023-12-18T21_55_03_205362.csv\n",
      "5 logs saved to data/log/coder/_llmlog_2023-12-18T21_55_03_206242.csv\n"
     ]
    }
   ],
   "source": [
    "response = await solve_in_python(issue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "import json\n",
    "\n",
    "response = json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The classes have been defined as per your specification and the methods have been demonstrated with print statements. Below is the full implementation:\n",
       "\n",
       "```python\n",
       "class File:\n",
       "    \"\"\"Represents a file with a name, size, and type.\"\"\"\n",
       "    \n",
       "    def __init__(self, name: str, size: int, file_type: str) -> None:\n",
       "        \"\"\"Initializes the file with a name, size, and type.\"\"\"\n",
       "        self.name = name\n",
       "        self.size = size\n",
       "        self.file_type = file_type\n",
       "\n",
       "    def read(self) -> str:\n",
       "        \"\"\"Simulates reading of file and returns the content as a string.\"\"\"\n",
       "        return f\"Reading content of file: {self.name}\"\n",
       "\n",
       "    def write(self, content: str) -> None:\n",
       "        \"\"\"Simulates writing content to the file, printing the operation.\"\"\"\n",
       "        print(f\"Writing to file: {self.name}. Content: {content}\")\n",
       "\n",
       "    def delete(self) -> None:\n",
       "        \"\"\"Simulates deleting the file and prints a confirmation message.\"\"\"\n",
       "        print(f\"File {self.name} deleted.\")\n",
       "\n",
       "\n",
       "class Chunk:\n",
       "    \"\"\"Represents a chunk of a file with an index, size, and data.\"\"\"\n",
       "    \n",
       "    def __init__(self, index: int, size: int, data: str) -> None:\n",
       "        \"\"\"Initializes the chunk with an index, size, and data.\"\"\"\n",
       "        self.index = index\n",
       "        self.size = size\n",
       "        self.data = data\n",
       "\n",
       "    def get_data(self) -> str:\n",
       "        \"\"\"Returns the data contained in the chunk.\"\"\"\n",
       "        return self.data\n",
       "\n",
       "    def set_data(self, new_data: str) -> None:\n",
       "        \"\"\"Updates the chunk's data with new data.\"\"\"\n",
       "        self.data = new_data\n",
       "\n",
       "# Creating instances of each class and demonstrating their usage\n",
       "file_example = File(name=\"example.txt\", size=1024, file_type=\"txt\")\n",
       "chunk_example = Chunk(index=1, size=512, data=\"This is a piece of data.\")\n",
       "\n",
       "# Demonstrating File methods\n",
       "print(file_example.read())\n",
       "file_example.write(\"Hello World\")\n",
       "file_example.delete()\n",
       "\n",
       "# Demonstrating Chunk methods\n",
       "print(chunk_example.get_data())\n",
       "chunk_example.set_data(\"New chunk data\")\n",
       "print(chunk_example.get_data())\n",
       "```\n",
       "\n",
       "Everything worked as expected.\n",
       "\n",
       "TERMINATE\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response['function call result']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lion_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
